<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>RPC on </title>
    <link>https://feilengcui008.github.io/tags/rpc/</link>
    <description>Recent content in RPC on </description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 24 Apr 2017 15:33:49 +0800</lastBuildDate>
    <atom:link href="https://feilengcui008.github.io/tags/rpc/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>gRPC-Go客户端源码分析</title>
      <link>https://feilengcui008.github.io/post/grpc-go%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</link>
      <pubDate>Mon, 24 Apr 2017 15:33:49 +0800</pubDate>
      <guid>https://feilengcui008.github.io/post/grpc-go%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</guid>
      <description>&lt;h2 id=&#34;基本设计&#34;&gt;基本设计&lt;/h2&gt;&#xA;&lt;p&gt;gRPC-Go客户端的逻辑相对比较简单，从前面服务端的逻辑我们知道，客户端会通过http2复用tcp连接，每一次请求的调用基本上就是在已经建立好的tcp连接(并用ClientTransport抽象)上发送http请求，通过帧和流与服务端交互数据。&lt;/p&gt;&#xA;&lt;p&gt;另外，一个服务对应的具体地址可能有多个，grpc在这里抽象了负载均衡的接口和部分实现。grpc提供两种负载均衡方式，一种是客户端内部自带的策略实现(目前只实现了轮询方式)，另一种方式是外部的load balancer。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;内部自带的策略实现: 这种方式主要针对一些简单的负载均衡策略比如轮询。轮询的实现逻辑是建立连接时通过定义的服务地址解析接口Resolver得到服务的地址列表，并单独用goroutine负责更新保持可用的连接，Watcher定义了具体更新实现的接口(比如多长时间解析更新一次)，最终在请求调用时会从可用连接列表中轮询选择其中一个连接发送请求。所以，grpc的负载均衡策略是请求级别的而不是连接级别的。&lt;/li&gt;&#xA;&lt;li&gt;外部load balancer：这种方式主要针对 较复杂的负载均衡策略。grpclb实现了grpc这边的逻辑，并用protobuf定义了与load balancer交互的接口。gRPC-Go客户端建立连接时，会先与load balancer建立连接，并使用和轮询方式类似的Resolver、Watcher接口来更新load balancer的可用连接列表，不同的是每次load balancer连接变化时，会像load balancer地址发送rpc请求得到服务的地址列表。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;客户端主要流程&#34;&gt;客户端主要流程&lt;/h2&gt;&#xA;&lt;p&gt;客户端的逻辑主要可分为下面两部分:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;建立连接&lt;/li&gt;&#xA;&lt;li&gt;请求调用、发送与响应&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;1-建立连接&#34;&gt;1. 建立连接&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;典型的步骤&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;func main() {&#xA;  // 建立连接&#xA;  conn, err := grpc.Dial(address, grpc.WithInsecure())&#xA;  c := pb.NewGreeterClient(conn)&#xA;  // 请求调用&#xA;  r, err := c.SayHello(context.Background(), &amp;amp;pb.HelloRequest{Name: name})&#xA;  // 处理返回r&#xA;  // 对于单次请求，grpc直接负责返回响应数据&#xA;  // 对于流式请求，grpc会返回一个流的封装，由开发者负责流中数据的读写&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&#xA;&lt;li&gt;建立tcp(http2)连接&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;func Dial(target string, opts ...DialOption) (*ClientConn, error) {&#xA;  return DialContext(context.Background(), target, opts...)&#xA;}&#xA;func DialContext(ctx context.Context, target string, opts ...DialOption) (conn *ClientConn, err error) {&#xA;  cc := &amp;amp;ClientConn{&#xA;    target: target,&#xA;    conns:  make(map[Address]*addrConn),&#xA;  }&#xA;  /* ... */&#xA;&#xA;  // 底层dialer，负责解析地址和建立tcp连接&#xA;  if cc.dopts.copts.Dialer == nil {&#xA;    cc.dopts.copts.Dialer = newProxyDialer(&#xA;      func(ctx context.Context, addr string) (net.Conn, error) {&#xA;        return dialContext(ctx, &amp;#34;tcp&amp;#34;, addr)&#xA;      },&#xA;    )&#xA;  }&#xA;  /* ... */&#xA;&#xA;  if cc.dopts.scChan != nil {&#xA;    // Wait for the initial service config.&#xA;    select {&#xA;    case sc, ok := &amp;lt;-cc.dopts.scChan:&#xA;      if ok {&#xA;        cc.sc = sc&#xA;      }&#xA;    case &amp;lt;-ctx.Done():&#xA;      return nil, ctx.Err()&#xA;    }&#xA;  }&#xA;  /* ... */&#xA;&#xA;  // 建立连接，如果设置了负载均衡，则通过负载均衡器建立连接&#xA;  // 否则直接连接&#xA;  waitC := make(chan error, 1)&#xA;  go func() {&#xA;    defer close(waitC)&#xA;    if cc.dopts.balancer == nil &amp;amp;&amp;amp; cc.sc.LB != nil {&#xA;      cc.dopts.balancer = cc.sc.LB&#xA;    }&#xA;    if cc.dopts.balancer != nil {&#xA;      var credsClone credentials.TransportCredentials&#xA;      if creds != nil {&#xA;        credsClone = creds.Clone()&#xA;      }&#xA;      config := BalancerConfig{&#xA;        DialCreds: credsClone,&#xA;      }&#xA;      // 负载均衡，可能是grcp-client内部的简单轮训负载均衡或者是外部的load balancer&#xA;      // 如果是外部的load balancer，这里的target是load balancer的服务名&#xA;      // grpclb会解析load balancer地址，建立rpc连接，得到服务地址列表，并通知Notify chan&#xA;      if err := cc.dopts.balancer.Start(target, config); err != nil {&#xA;        waitC &amp;lt;- err&#xA;        return&#xA;      }&#xA;      // 更新后地址的发送channel&#xA;      ch := cc.dopts.balancer.Notify()&#xA;      if ch != nil {&#xA;        if cc.dopts.block {&#xA;          doneChan := make(chan struct{})&#xA;          // lbWatcher负责接收负载均衡器的地址更新，从而更新连接&#xA;          go cc.lbWatcher(doneChan)&#xA;          &amp;lt;-doneChan&#xA;        } else {&#xA;          go cc.lbWatcher(nil)&#xA;        }&#xA;        return&#xA;      }&#xA;    }&#xA;    // 直接建立连接&#xA;    if err := cc.resetAddrConn(Address{Addr: target}, cc.dopts.block, nil); err != nil {&#xA;      waitC &amp;lt;- err&#xA;      return&#xA;    }&#xA;  }()&#xA;  /* ... */&#xA;  if cc.dopts.scChan != nil {&#xA;    go cc.scWatcher()&#xA;  }&#xA;&#xA;  return cc, nil&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&#xA;&lt;li&gt;内部负载均衡策略(轮询)，解析域名，并更新地址列表，写到Notify通知channel，由grpc的lbWatcher负责更新对应的服务连接列表&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;func (rr *roundRobin) Start(target string, config BalancerConfig) error {&#xA;  /* ... */&#xA;  // 服务名解析，具体实现可以DNS或者基于etcd的服务发现等，每次解析会返回一个watcher&#xA;  // watcher具体服务解析请求的周期等&#xA;  w, err := rr.r.Resolve(target)&#xA;  if err != nil {&#xA;    return err&#xA;  }&#xA;  rr.w = w&#xA;  rr.addrCh = make(chan []Address)&#xA;  go func() {&#xA;    // 循环，不断解析服务的地址，更新对应的地址列表&#xA;    for {&#xA;      if err := rr.watchAddrUpdates(); err != nil {&#xA;        return&#xA;      }&#xA;    }&#xA;  }()&#xA;  return nil&#xA;}&#xA;&#xA;func (rr *roundRobin) watchAddrUpdates() error {&#xA;  // 阻塞得到需要更新的地址列表，注意在naming里面的Resolver和Watcher&#xA;  // 定义了服务解析的接口，可以使用简单的dns解析实现、consul/etcd等服务发现&#xA;  // 以及其他形式，只要能返回对应的服务地址列表即可，Resolver里边缓存已经解析&#xA;  // 过的服务，并有单独的goroutine与后端服务通信更新，这样不用每次都解析地址&#xA;  updates, err := rr.w.Next()&#xA;  // 解析后，更新对应服务的地址列表，在内部做轮训负载均衡&#xA;  for _, update := range updates {&#xA;    addr := Address{&#xA;      Addr:     update.Addr,&#xA;      Metadata: update.Metadata,&#xA;    }&#xA;    switch update.Op {&#xA;    // 添加新地址&#xA;    case naming.Add:&#xA;      var exist bool&#xA;      for _, v := range rr.addrs {&#xA;        if addr == v.addr {&#xA;          exist = true&#xA;          grpclog.Println(&amp;#34;grpc: The name resolver wanted to add an existing address: &amp;#34;, addr)&#xA;          break&#xA;        }&#xA;      }&#xA;      if exist {&#xA;        continue&#xA;      }&#xA;      rr.addrs = append(rr.addrs, &amp;amp;addrInfo{addr: addr})&#xA;      // 删除&#xA;    case naming.Delete:&#xA;      for i, v := range rr.addrs {&#xA;        if addr == v.addr {&#xA;          copy(rr.addrs[i:], rr.addrs[i+1:])&#xA;          rr.addrs = rr.addrs[:len(rr.addrs)-1]&#xA;          break&#xA;        }&#xA;      }&#xA;  }&#xA;  open := make([]Address, len(rr.addrs))&#xA;  for i, v := range rr.addrs {&#xA;    open[i] = v.addr&#xA;  }&#xA;  // 通知lbWatcher&#xA;  rr.addrCh &amp;lt;- open&#xA;  return nil&#xA;}&#xA;&#xA;// 轮询得到一个可用连接&#xA;func (rr *roundRobin) Get(ctx context.Context, opts BalancerGetOptions) (addr Address, put func(), err error) {&#xA;  /* ... */&#xA;  if len(rr.addrs) &amp;gt; 0 {&#xA;    if rr.next &amp;gt;= len(rr.addrs) {&#xA;      rr.next = 0&#xA;    }&#xA;    next := rr.next&#xA;    for {&#xA;      // 找到下一个，赋予返回值&#xA;      a := rr.addrs[next]&#xA;      next = (next + 1) % len(rr.addrs)&#xA;      if a.connected {&#xA;        addr = a.addr&#xA;        rr.next = next&#xA;        rr.mu.Unlock()&#xA;        return&#xA;      }&#xA;      if next == rr.next {&#xA;        // Has iterated all the possible address but none is connected.&#xA;        break&#xA;      }&#xA;    }&#xA;  }&#xA;  /* ... */&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&#xA;&lt;li&gt;外部负载均衡&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;// 对于外部负载均衡，Start负责解析负载均衡器的地址列表&#xA;func (b *balancer) Start(target string, config grpc.BalancerConfig) error {&#xA;  /* ... */&#xA;  // 解析，返回watcher&#xA;  w, err := b.r.Resolve(target)&#xA;  b.w = w&#xA;  b.mu.Unlock()&#xA;  balancerAddrsCh := make(chan []remoteBalancerInfo, 1)&#xA;  go func() {&#xA;    for {&#xA;      // 一直循环解析load balancer的地址，一旦有更新则通知&#xA;      if err := b.watchAddrUpdates(w, balancerAddrsCh); err != nil {&#xA;        /* ... */&#xA;      }&#xA;    }&#xA;  }()&#xA;  go func() {&#xA;    var (&#xA;      cc *grpc.ClientConn&#xA;      // ccError is closed when there is an error in the current cc.&#xA;      // A new rb should be picked from rbs and connected.&#xA;      ccError chan struct{}&#xA;      rb      *remoteBalancerInfo&#xA;      rbs     []remoteBalancerInfo&#xA;      rbIdx   int&#xA;    )&#xA;    /* ... */&#xA;    for {&#xA;      var ok bool&#xA;      select {&#xA;      // 从channel中读取load balancer的列表&#xA;      case rbs, ok = &amp;lt;-balancerAddrsCh:&#xA;        /* ... */&#xA;      }&#xA;      /* ... */&#xA;      // 连接load balancer&#xA;      if creds == nil {&#xA;        cc, err = grpc.Dial(rb.addr, grpc.WithInsecure())&#xA;      } else {&#xA;        /* ... */&#xA;        cc, err = grpc.Dial(rb.addr, grpc.WithTransportCredentials(creds))&#xA;      }&#xA;      b.mu.Lock()&#xA;      b.seq++ // tick when getting a new balancer address&#xA;      seq := b.seq&#xA;      b.next = 0&#xA;      b.mu.Unlock()&#xA;      // 对于每个load balancer的地址变化，获取新的服务地址列表，并通知lbWatcher更新&#xA;      go func(cc *grpc.ClientConn, ccError chan struct{}) {&#xA;        // load balancer client&#xA;        lbc := lbpb.NewLoadBalancerClient(cc)&#xA;        // 得到server list，并写入addrChan这个Notify channel&#xA;        b.callRemoteBalancer(lbc, seq)&#xA;        cc.Close()&#xA;        select {&#xA;        case &amp;lt;-ccError:&#xA;        default:&#xA;          close(ccError)&#xA;        }&#xA;      }(cc, ccError)&#xA;    }&#xA;  }()&#xA;  return nil&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;2-请求调用发送与响应&#34;&gt;2. 请求调用、发送与响应&lt;/h3&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;// 单次请求，grpc负责invoke对应的服务方法，并直接返回数据&#xA;func (c *greeterClient) SayHello(ctx context.Context, in *HelloRequest, opts ...grpc.CallOption) (*HelloReply, error) {&#xA;  out := new(HelloReply)&#xA;  err := grpc.Invoke(ctx, &amp;#34;/helloworld.Greeter/SayHello&amp;#34;, in, out, c.cc, opts...)&#xA;  if err != nil {&#xA;    return nil, err&#xA;  }&#xA;  return out, nil&#xA;}&#xA;&#xA;// 流式请求，grpc返回对应的流&#xA;func (c *routeGuideClient) ListFeatures(ctx context.Context, in *Rectangle, opts ...grpc.CallOption) (RouteGuide_ListFeaturesClient, error) {&#xA;  stream, err := grpc.NewClientStream(ctx, &amp;amp;_RouteGuide_serviceDesc.Streams[0], c.cc, &amp;#34;/routeguide.RouteGuide/ListFeatures&amp;#34;, opts...)&#xA;  if err != nil {&#xA;    return nil, err&#xA;  }&#xA;  x := &amp;amp;routeGuideListFeaturesClient{stream}&#xA;  if err := x.ClientStream.SendMsg(in); err != nil {&#xA;    return nil, err&#xA;  }&#xA;  if err := x.ClientStream.CloseSend(); err != nil {&#xA;    return nil, err&#xA;  }&#xA;  return x, nil&#xA;}&#xA;&#xA;// 单次请求调用实现，响应返回时客户端会关闭流，而流式请求会直接将流封装后交给上层开发者，由开发者处理&#xA;func invoke(ctx context.Context, method string, args, reply interface{}, cc *ClientConn, opts ...CallOption) (e error) {&#xA;  /* ... */&#xA;  for {&#xA;    var (&#xA;      err    error&#xA;      t      transport.ClientTransport&#xA;      stream *transport.Stream&#xA;      // Record the put handler from Balancer.Get(...). It is called once the&#xA;      // RPC has completed or failed.&#xA;      put func()&#xA;    )&#xA;    /* ... */&#xA;    // 得到一个tcp连接(ClientTransport)&#xA;    t, put, err = cc.getTransport(ctx, gopts)&#xA;    /* ... */&#xA;    // 发送请求，打开新的流，序列化压缩请求数据，写入流&#xA;    stream, err = sendRequest(ctx, cc.dopts, cc.dopts.cp, callHdr, t, args, topts)&#xA;    /* ... */&#xA;    // 接收响应，解压反序列化响应，并写入reply&#xA;    err = recvResponse(ctx, cc.dopts, t, &amp;amp;c, stream, reply)&#xA;    /* ... */&#xA;    // 关闭流&#xA;    t.CloseStream(stream, nil)&#xA;    if put != nil {&#xA;      put()&#xA;      put = nil&#xA;    }&#xA;    return stream.Status().Err()&#xA;  }&#xA;}&#xA;&#xA;// 发送请求，打开一个新的流&#xA;func sendRequest(ctx context.Context, dopts dialOptions, compressor Compressor, callHdr *transport.CallHdr, t transport.ClientTransport, args interface{}, opts *transport.Options) (_ *transport.Stream, err error) {&#xA;  // 在此连接上打开新的流&#xA;  stream, err := t.NewStream(ctx, callHdr)&#xA;  if err != nil {&#xA;    return nil, err&#xA;  }&#xA;  /* ... */&#xA;  // 序列化压缩数据&#xA;  outBuf, err := encode(dopts.codec, args, compressor, cbuf, outPayload)&#xA;  // 写入流&#xA;  err = t.Write(stream, outBuf, opts)&#xA;  /* ... */&#xA;  // Sent successfully.&#xA;  return stream, nil&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;hr&gt;&#xA;&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;&#xA;&lt;p&gt;至此，gRPC-Go的客户端逻辑主体部分分析完了，其中比较重要的是:&lt;/p&gt;</description>
    </item>
    <item>
      <title>gRPC-Go服务端源码分析</title>
      <link>https://feilengcui008.github.io/post/grpc-go%E6%9C%8D%E5%8A%A1%E7%AB%AF%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</link>
      <pubDate>Sun, 23 Apr 2017 15:47:59 +0800</pubDate>
      <guid>https://feilengcui008.github.io/post/grpc-go%E6%9C%8D%E5%8A%A1%E7%AB%AF%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</guid>
      <description>&lt;h2 id=&#34;基本设计&#34;&gt;基本设计&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;服务抽象&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;一个Server可包含多个Service，每个Service包含多个业务逻辑方法，应用开发者需要:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;不使用protobuf&#xA;&lt;ul&gt;&#xA;&lt;li&gt;规定Service需要实现的接口&lt;/li&gt;&#xA;&lt;li&gt;实现此Service对应的ServiceDesc，ServiceDesc描述了服务名、处理此服务的接口类型、单次调用的方法数组、流式方法数组、其他元数据。&lt;/li&gt;&#xA;&lt;li&gt;实现Service接口具体业务逻辑的结构体&lt;/li&gt;&#xA;&lt;li&gt;实例化Server，并讲ServiceDesc和Service具体实现注册到Server&lt;/li&gt;&#xA;&lt;li&gt;监听并启动Server服务&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;使用protobuf&#xA;&lt;ul&gt;&#xA;&lt;li&gt;实现protobuf grpc插件生成的Service接口&lt;/li&gt;&#xA;&lt;li&gt;实例化Server，并注册Service接口的具体实现&lt;/li&gt;&#xA;&lt;li&gt;监听并启动Server&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;可见，protobuf的gRPC-Go插件帮助我们生成了Service的接口和ServiceDesc。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;底层传输协议&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;gRPC-Go使用http2作为应用层的传输协议，http2会复用底层tcp连接，以流和数据帧的形式处理上层协议，gRPC-Go使用http2的主要逻辑有下面几点，关于http2详细的细节可参考&lt;a href=&#34;http://http2.github.io/&#34;&gt;http2的规范&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;http2帧分为几大类，gRPC-Go使用中比较重要的是HEADERS和DATA帧类型。&#xA;&lt;ul&gt;&#xA;&lt;li&gt;HEADERS帧在打开一个新的流时使用，通常是客户端的一个http请求，gRPC-Go通过底层的go的http2实现帧的读写，并解析出客户端的请求头(大多是grpc内部自己定义的)，读取请求体的数据，grpc规定请求体的数据由两部分构成(5 byte + len(msg)), 其中第1字节表明是否压缩，第2-5个字节消息体的长度(最大2^32即4G)，msg为客户端请求序列化后的原始数据。&lt;/li&gt;&#xA;&lt;li&gt;数据帧从属于某个stream，按照stream id查找，并写入对应的stream中。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Server端接收到客户端建立的连接后，使用一个goroutine专门处理此客户端的连接(即一个tcp连接或者说一个http2连接)，所以同一个grpc客户端连接上服务端后，后续的请求都是通过同一个tcp连接。&lt;/li&gt;&#xA;&lt;li&gt;客户端和服务端的连接在应用层由Transport抽象(类似通常多路复用实现中的封装的channel)，在客户端是ClientTransport，在服务端是ServerTransport。Server端接收到一个客户端的http2请求后即打开一个新的流，ClientTransport和ServerTransport之间使用这个新打开的流以http2帧的形式交换数据。&lt;/li&gt;&#xA;&lt;li&gt;客户端的每个http2请求会打开一个新的流。流可以从两边关闭，对于单次请求来说，客户端会主动关闭流，对于流式请求客户端不会主动关闭(即使使用了CloseSend也只是发送了数据发送结束的标识，还是由服务端关闭)。&lt;/li&gt;&#xA;&lt;li&gt;gRPC-Go中的单次方法和流式方法&#xA;&lt;ul&gt;&#xA;&lt;li&gt;无论是单次方法还是流式方法，服务端在调用完用户的处理逻辑函数返回后，都会关闭流(这也是为什么ServerStream不需要实现CloseSend的原因)。区别只是对于服务端的流式方法来说，可循环多次读取这个流中的帧数据并处理，以此&amp;quot;复用&amp;quot;这个流。&lt;/li&gt;&#xA;&lt;li&gt;客户端如果是流式方法，需要显示调用CloseSend，表示数据发送的结束&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;服务端主要流程&#34;&gt;服务端主要流程&lt;/h2&gt;&#xA;&lt;p&gt;由于比较多，所以分以下几个部分解读主要逻辑:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;实例化Server&lt;/li&gt;&#xA;&lt;li&gt;注册Service&lt;/li&gt;&#xA;&lt;li&gt;监听并接收连接请求&lt;/li&gt;&#xA;&lt;li&gt;连接与请求处理&lt;/li&gt;&#xA;&lt;li&gt;连接的处理细节(http2连接的建立)&lt;/li&gt;&#xA;&lt;li&gt;新请求的处理细节(新流的打开和帧数据的处理)&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;实例化Server&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;// 工厂方法&#xA;func NewServer(opt ...ServerOption) *Server {&#xA;  var opts options&#xA;  // 默认最大消息长度: 4M&#xA;  opts.maxMsgSize = defaultMaxMsgSize&#xA;  // 设置定制的参数&#xA;  for _, o := range opt {&#xA;    o(&amp;amp;opts)&#xA;  }&#xA;  // 默认编解码方式为protobuf&#xA;  if opts.codec == nil {&#xA;    // Set the default codec.&#xA;    opts.codec = protoCodec{}&#xA;  }&#xA;  // 实例化Server&#xA;  s := &amp;amp;Server{&#xA;    lis:   make(map[net.Listener]bool),&#xA;    opts:  opts,&#xA;    conns: make(map[io.Closer]bool),&#xA;    m:     make(map[string]*service),&#xA;  }&#xA;  s.cv = sync.NewCond(&amp;amp;s.mu)&#xA;  s.ctx, s.cancel = context.WithCancel(context.Background())&#xA;  if EnableTracing {&#xA;    _, file, line, _ := runtime.Caller(1)&#xA;    s.events = trace.NewEventLog(&amp;#34;grpc.Server&amp;#34;, fmt.Sprintf(&amp;#34;%s:%d&amp;#34;, file, line))&#xA;  }&#xA;  return s&#xA;}&#xA;&#xA;// Server结构体&#xA;// 一个Server结构代表对外服务的单元，每个Server可以注册&#xA;// 多个Service，每个Service可以有多个方法，主程序需要&#xA;// 实例化Server，注册Service，然后调用s.Serve(l)&#xA;type Server struct {&#xA;  opts options&#xA;  mu sync.Mutex // guards following&#xA;  // 监听地址列表&#xA;  lis map[net.Listener]bool&#xA;  // 客户端的连接&#xA;  conns map[io.Closer]bool&#xA;  drain bool&#xA;  // 上下文&#xA;  ctx    context.Context&#xA;  cancel context.CancelFunc&#xA;  // A CondVar to let GracefulStop() blocks until all the pending RPCs are finished&#xA;  // and all the transport goes away.&#xA;  // 优雅退出时，会等待在此信号，直到所有的RPC都处理完了，并且所有&#xA;  // 的传输层断开&#xA;  cv *sync.Cond&#xA;  // 服务名: 服务&#xA;  m map[string]*service // service name -&amp;gt; service info&#xA;  // 事件追踪&#xA;  events trace.EventLog&#xA;}&#xA;&#xA;// Server配置项&#xA;// Server可设置的选项&#xA;type options struct {&#xA;  // 加密信息， 目前实现了TLS&#xA;  creds credentials.TransportCredentials&#xA;  // 数据编解码，目前实现了protobuf，并用缓存池sync.Pool优化&#xA;  codec Codec&#xA;  // 数据压缩，目前实现了gzip&#xA;  cp Compressor&#xA;  // 数据解压，目前实现了gzip&#xA;  dc Decompressor&#xA;  // 最大消息长度&#xA;  maxMsgSize int&#xA;  // 单次请求的拦截器&#xA;  unaryInt UnaryServerInterceptor&#xA;  // 流式请求的拦截器&#xA;  streamInt   StreamServerInterceptor&#xA;  inTapHandle tap.ServerInHandle&#xA;  // 统计&#xA;  statsHandler stats.Handler&#xA;  // 最大并发流数量，http2协议规范&#xA;  maxConcurrentStreams uint32&#xA;  useHandlerImpl       bool // use http.Handler-based server&#xA;  unknownStreamDesc    *StreamDesc&#xA;  // server端的keepalive参数，会由单独的gorotine负责探测客户端连接的活性&#xA;  keepaliveParams keepalive.ServerParameters&#xA;  keepalivePolicy keepalive.EnforcementPolicy&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&#xA;&lt;li&gt;注册Service&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;// 注册service: sd接口，ss实现&#xA;// 如果使用protobuf的gRPC-Go插件，则会生成sd接口&#xA;func (s *Server) RegisterService(sd *ServiceDesc, ss interface{}) {&#xA;  // 检查ss是否实现sd定义的服务方法接口&#xA;  ht := reflect.TypeOf(sd.HandlerType).Elem()&#xA;  st := reflect.TypeOf(ss)&#xA;  if !st.Implements(ht) {&#xA;    grpclog.Fatalf(&amp;#34;grpc: Server.RegisterService found the handler of type %v that does not satisfy %v&amp;#34;, st, ht)&#xA;  }&#xA;  s.register(sd, ss)&#xA;}&#xA;&#xA;func (s *Server) register(sd *ServiceDesc, ss interface{}) {&#xA;  /* ... */&#xA;  // 检查是否已注册&#xA;  if _, ok := s.m[sd.ServiceName]; ok {&#xA;    grpclog.Fatalf(&amp;#34;grpc: Server.RegisterService found duplicate service registration for %q&amp;#34;, sd.ServiceName)&#xA;  }&#xA;  // 实例化一个服务&#xA;  srv := &amp;amp;service{&#xA;    // 具体实现&#xA;    server: ss,&#xA;    // 单次方法信息&#xA;    md:    make(map[string]*MethodDesc),&#xA;    // 流式方法信息&#xA;    sd:    make(map[string]*StreamDesc),&#xA;    mdata: sd.Metadata,&#xA;  }&#xA;  for i := range sd.Methods {&#xA;    d := &amp;amp;sd.Methods[i]&#xA;    srv.md[d.MethodName] = d&#xA;  }&#xA;  for i := range sd.Streams {&#xA;    d := &amp;amp;sd.Streams[i]&#xA;    srv.sd[d.StreamName] = d&#xA;  }&#xA;  // 注册服务到server&#xA;  s.m[sd.ServiceName] = srv&#xA;}&#xA;&#xA;// 一个由protobuf grcp-go插件生成的sd例子&#xA;var _Greeter_serviceDesc = grpc.ServiceDesc{&#xA;  // 服务名&#xA;  ServiceName: &amp;#34;app.Greeter&amp;#34;,&#xA;  // 此服务的处理类型(通常为实现某服务接口的具体实现结构体)&#xA;  HandlerType: (*GreeterServer)(nil),&#xA;  // 单次方法&#xA;  Methods: []grpc.MethodDesc{&#xA;    {&#xA;      // 方法名&#xA;      MethodName: &amp;#34;SayHello&amp;#34;,&#xA;      // 最终调用的对应/service/method的方法&#xA;      Handler:    _Greeter_SayHello_Handler,&#xA;    },&#xA;  },&#xA;  Streams:  []grpc.StreamDesc{},&#xA;  Metadata: &amp;#34;app.proto&amp;#34;,&#xA;}&#xA;&#xA;// 要注意的是protobuf的gRPC-Go插件为我们生成的MethodDesc中的Handler&#xA;// 对于单次方法和流式方法区别较大，单次方法的参数传入和返回的是单一的请求&#xA;// 和返回对象，而流式方法传入的是底层流的封装ClientStream、ServerStream&#xA;// 因此流式方法可多次读写流。&#xA;// 单次方法的一个例子&#xA;func _Greeter_SayHello_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {&#xA;  in := new(HelloRequest)&#xA;  // 注意这个dec方法参数，负责反序列化，解压&#xA;  if err := dec(in); err != nil {&#xA;    return nil, err&#xA;  }&#xA;  if interceptor == nil {&#xA;    return srv.(GreeterServer).SayHello(ctx, in)&#xA;  }&#xA;  /* ... */&#xA;}&#xA;// 流式方法的一个例子(假设是客户端可流式发送)&#xA;func _Greeter_SayHello_Handler(srv interface{}, stream grpc.ServerStream) error {&#xA;  // 这里应该由业务逻辑实现的SayHello处理流式读取处理的逻辑&#xA;  return srv.(GreeterServer).SayHello(&amp;amp;greeterSayHelloServer{stream})&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&#xA;&lt;li&gt;监听并接收连接请求&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;func (s *Server) Serve(lis net.Listener) error {&#xA;  /* ... */&#xA;  var tempDelay time.Duration // how long to sleep on accept failure&#xA;  // 循环处理连接，每个连接使用一个goroutine处理&#xA;  // accept如果失败，则下次accept之前睡眠一段时间&#xA;  for {&#xA;    rawConn, err := lis.Accept()&#xA;    if err != nil {&#xA;      if ne, ok := err.(interface {&#xA;        Temporary() bool&#xA;      }); ok &amp;amp;&amp;amp; ne.Temporary() {&#xA;        if tempDelay == 0 {&#xA;          // 初始5ms&#xA;          tempDelay = 5 * time.Millisecond&#xA;        } else {&#xA;          // 否则翻倍&#xA;          tempDelay *= 2&#xA;        }&#xA;        // 不超过1s&#xA;        if max := 1 * time.Second; tempDelay &amp;gt; max {&#xA;          tempDelay = max&#xA;        }&#xA;  d     /* ... */&#xA;        // 等待超时重试，或者context事件的发生&#xA;        select {&#xA;        case &amp;lt;-time.After(tempDelay):&#xA;        case &amp;lt;-s.ctx.Done():&#xA;        }&#xA;        continue&#xA;      }&#xA;      /* ... */&#xA;    }&#xA;    // 重置延时&#xA;    tempDelay = 0&#xA;    // Start a new goroutine to deal with rawConn&#xA;    // so we don&amp;#39;t stall this Accept loop goroutine.&#xA;    // 每个新的tcp连接使用单独的goroutine处理&#xA;    go s.handleRawConn(rawConn)&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&#xA;&lt;li&gt;连接与请求处理&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;func (s *Server) handleRawConn(rawConn net.Conn) {&#xA;  // 是否加密&#xA;  conn, authInfo, err := s.useTransportAuthenticator(rawConn)&#xA;  /* ... */&#xA;  s.mu.Lock()&#xA;  // 如果此goroutine处于处理连接中时，server被关闭，则直接关闭连接返回&#xA;  if s.conns == nil {&#xA;    s.mu.Unlock()&#xA;    conn.Close()&#xA;    return&#xA;  }&#xA;  s.mu.Unlock()&#xA;&#xA;  if s.opts.useHandlerImpl {&#xA;    // 测试时使用&#xA;    s.serveUsingHandler(conn)&#xA;  } else {&#xA;    // 处理http2连接的建立，http2连接的建立也需要客户端和&#xA;    // 服务端交换，即http2 Connection Preface，所以后面&#xA;    // 的宏观逻辑是，先处理http2连接建立过程中的帧数据信息，&#xA;    // 然后一直循环处理新的流的建立(即新的http2请求的到达)&#xA;    // 和帧的数据收发。&#xA;    s.serveHTTP2Transport(conn, authInfo)&#xA;  }&#xA;}&#xA;&#xA;// 每个http2连接在服务端会生成一个ServerTransport，这里是 htt2server&#xA;func (s *Server) serveHTTP2Transport(c net.Conn, authInfo credentials.AuthInfo) {&#xA;  config := &amp;amp;transport.ServerConfig{&#xA;    MaxStreams:      s.opts.maxConcurrentStreams,&#xA;    AuthInfo:        authInfo,&#xA;    InTapHandle:     s.opts.inTapHandle,&#xA;    StatsHandler:    s.opts.statsHandler,&#xA;    KeepaliveParams: s.opts.keepaliveParams,&#xA;    KeepalivePolicy: s.opts.keepalivePolicy,&#xA;  }&#xA;  // 返回实现了ServerTransport接口的http2server&#xA;  // 接口规定了HandleStream, Write等方法&#xA;  st, err := transport.NewServerTransport(&amp;#34;http2&amp;#34;, c, config)&#xA;  /* ... */&#xA;  // 加入每个连接的ServerTransport&#xA;  if !s.addConn(st) {&#xA;    // 出错关闭Transport，即关闭客户端的net.Conn&#xA;    st.Close()&#xA;    return&#xA;  }&#xA;  // 开始处理连接Transport，处理新的帧数据和流的打开&#xA;  s.serveStreams(st)&#xA;}&#xA;&#xA;// 新建ServerTransport&#xA;func newHTTP2Server(conn net.Conn, config *ServerConfig) (_ ServerTransport, err error) {&#xA;  // 封装帧的读取，底层使用的是http2.frame&#xA;  framer := newFramer(conn)&#xA;  // 初始的配置帧&#xA;  // Send initial settings as connection preface to client.&#xA;  var settings []http2.Setting&#xA;  // TODO(zhaoq): Have a better way to signal &amp;#34;no limit&amp;#34; because 0 is&#xA;  // permitted in the HTTP2 spec.&#xA;  // 流的最大数量&#xA;  maxStreams := config.MaxStreams&#xA;  if maxStreams == 0 {&#xA;    maxStreams = math.MaxUint32&#xA;  } else {&#xA;    settings = append(settings, http2.Setting{&#xA;      ID:  http2.SettingMaxConcurrentStreams,&#xA;      Val: maxStreams,&#xA;    })&#xA;  }&#xA;  // 流窗口大小，默认16K&#xA;  if initialWindowSize != defaultWindowSize {&#xA;    settings = append(settings, http2.Setting{&#xA;      ID:  http2.SettingInitialWindowSize,&#xA;      Val: uint32(initialWindowSize)})&#xA;  }&#xA;  if err := framer.writeSettings(true, settings...); err != nil {&#xA;    return nil, connectionErrorf(true, err, &amp;#34;transport: %v&amp;#34;, err)&#xA;  }&#xA;  // Adjust the connection flow control window if needed.&#xA;  if delta := uint32(initialConnWindowSize - defaultWindowSize); delta &amp;gt; 0 {&#xA;    if err := framer.writeWindowUpdate(true, 0, delta); err != nil {&#xA;      return nil, connectionErrorf(true, err, &amp;#34;transport: %v&amp;#34;, err)&#xA;    }&#xA;  }&#xA;  // tcp连接的KeepAlive相关参数&#xA;  kp := config.KeepaliveParams&#xA;  // 最大idle时间，超过此客户端连接将被关闭，默认无穷&#xA;  if kp.MaxConnectionIdle == 0 {&#xA;    kp.MaxConnectionIdle = defaultMaxConnectionIdle&#xA;  }&#xA;  if kp.MaxConnectionAge == 0 {&#xA;    kp.MaxConnectionAge = defaultMaxConnectionAge&#xA;  }&#xA;  // Add a jitter to MaxConnectionAge.&#xA;  kp.MaxConnectionAge += getJitter(kp.MaxConnectionAge)&#xA;  if kp.MaxConnectionAgeGrace == 0 {&#xA;    kp.MaxConnectionAgeGrace = defaultMaxConnectionAgeGrace&#xA;  }&#xA;  if kp.Time == 0 {&#xA;    kp.Time = defaultServerKeepaliveTime&#xA;  }&#xA;  if kp.Timeout == 0 {&#xA;    kp.Timeout = defaultServerKeepaliveTimeout&#xA;  }&#xA;  kep := config.KeepalivePolicy&#xA;  if kep.MinTime == 0 {&#xA;    kep.MinTime = defaultKeepalivePolicyMinTime&#xA;  }&#xA;  var buf bytes.Buffer&#xA;  t := &amp;amp;http2Server{&#xA;    ctx:             context.Background(),&#xA;    conn:            conn,&#xA;    remoteAddr:      conn.RemoteAddr(),&#xA;    localAddr:       conn.LocalAddr(),&#xA;    authInfo:        config.AuthInfo,&#xA;    framer:          framer,&#xA;    hBuf:            &amp;amp;buf,&#xA;    hEnc:            hpack.NewEncoder(&amp;amp;buf),&#xA;    maxStreams:      maxStreams,&#xA;    inTapHandle:     config.InTapHandle,&#xA;    controlBuf:      newRecvBuffer(),&#xA;    fc:              &amp;amp;inFlow{limit: initialConnWindowSize},&#xA;    sendQuotaPool:   newQuotaPool(defaultWindowSize),&#xA;    state:           reachable,&#xA;    writableChan:    make(chan int, 1),&#xA;    shutdownChan:    make(chan struct{}),&#xA;    activeStreams:   make(map[uint32]*Stream),&#xA;    streamSendQuota: defaultWindowSize,&#xA;    stats:           config.StatsHandler,&#xA;    kp:              kp,&#xA;    idle:            time.Now(),&#xA;    kep:             kep,&#xA;  }&#xA;  /* ... */&#xA;  // 专门处理控制信息&#xA;  go t.controller()&#xA;  // 专门处理tcp连接的保火逻辑&#xA;  go t.keepalive()&#xA;  // 解锁&#xA;  t.writableChan &amp;lt;- 0&#xA;  return t, nil&#xA;}&#xA;&#xA;&#xA;func (s *Server) serveStreams(st transport.ServerTransport) {&#xA;  // 处理完移除&#xA;  defer s.removeConn(st)&#xA;  // 处理完关闭Transport&#xA;  defer st.Close()&#xA;  var wg sync.WaitGroup&#xA;  // ServerTransport定义的HandleStream, 传入handler和trace callback方法&#xA;  // 这里ServerTransport的HandleStream实现会使用包装的http2.frame，循环不断读取帧&#xA;  // 直到客户端的net.Conn返回错误或者关闭为止，handler只用来处理HEADER类型的帧(即新的http&#xA;  // 请求，新的流的打开)，其他帧比如数据帧会分发到对应的stream, 这里的HEADER帧数据包含&#xA;  // 了grpc定义的http请求头等信息。HandleStream会一直循环读取新到达的帧，知道出现错误&#xA;  // 实在需要关闭客户端的连接，流读写相关的错误一般不会导致连接的关闭。&#xA;  st.HandleStreams(func(stream *transport.Stream) {&#xA;    wg.Add(1)&#xA;    go func() {&#xA;      defer wg.Done()&#xA;      // 处理stream，只有HEADER类型的帧才调用这个处理请求头等信息&#xA;      s.handleStream(st, stream, s.traceInfo(st, stream))&#xA;    }()&#xA;  }, func(ctx context.Context, method string) context.Context {&#xA;    if !EnableTracing {&#xA;      return ctx&#xA;    }&#xA;    tr := trace.New(&amp;#34;grpc.Recv.&amp;#34;+methodFamily(method), method)&#xA;    return trace.NewContext(ctx, tr)&#xA;  })&#xA;  // 等待HandleStream结束，除非客户端的连接由于错误发生需要关闭，一般不会到这&#xA;  wg.Wait()&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&#xA;&lt;li&gt;连接的处理细节(http2连接的建立)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;// 实现的ServerTransport的HandleStreams接口&#xA;func (t *http2Server) HandleStreams(handle func(*Stream), traceCtx func(context.Context, string) context.Context) {&#xA;  // Check the validity of client preface.&#xA;  // 检查是否是http2&#xA;  // 建立一个http2连接之后，之后的所有stream复用此连接&#xA;  preface := make([]byte, len(clientPreface))&#xA;  if _, err := io.ReadFull(t.conn, preface); err != nil {&#xA;    grpclog.Printf(&amp;#34;transport: http2Server.HandleStreams failed to receive the preface from client: %v&amp;#34;, err)&#xA;    t.Close()&#xA;    return&#xA;  }&#xA;  if !bytes.Equal(preface, clientPreface) {&#xA;    grpclog.Printf(&amp;#34;transport: http2Server.HandleStreams received bogus greeting from client: %q&amp;#34;, preface)&#xA;    t.Close()&#xA;    return&#xA;  }&#xA;  // 读取一帧配置信息，参考http2的规范&#xA;  frame, err := t.framer.readFrame()&#xA;  /* ... */&#xA;  sf, ok := frame.(*http2.SettingsFrame)&#xA;  /* ... */&#xA;  t.handleSettings(sf)&#xA;&#xA;  // 一直循环读取并处理帧, 注意什么时候底层的tcp连接会关闭，通常大多数情况下不会导致连接的关闭&#xA;  // 从这里开始就是处理流和数据帧的逻辑了，连接复用在这里真正被体现&#xA;  for {&#xA;    frame, err := t.framer.readFrame()&#xA;    atomic.StoreUint32(&amp;amp;t.activity, 1)&#xA;    if err != nil {&#xA;      // StreamError，不退出，&#xA;      if se, ok := err.(http2.StreamError); ok {&#xA;        t.mu.Lock()&#xA;        s := t.activeStreams[se.StreamID]&#xA;        t.mu.Unlock()&#xA;        // 关闭Stream&#xA;        if s != nil {&#xA;          t.closeStream(s)&#xA;        }&#xA;        // 控制输出错误信息&#xA;        t.controlBuf.put(&amp;amp;resetStream{se.StreamID, se.Code})&#xA;        continue&#xA;      }&#xA;      // io.EOF什么时候触发? 客户端关闭连接?&#xA;      if err == io.EOF || err == io.ErrUnexpectedEOF {&#xA;        t.Close()&#xA;        return&#xA;      }&#xA;      grpclog.Printf(&amp;#34;transport: http2Server.HandleStreams failed to read frame: %v&amp;#34;, err)&#xA;      t.Close()&#xA;      return&#xA;    }&#xA;    // HTTP2定义的帧类型&#xA;    switch frame := frame.(type) {&#xA;    // HEADER frame用来打开一个stream，表示一个新请求的到来和一个新的流的建立，这里需要使用Server定义的处理逻辑&#xA;    // 解析请求头，得到服务和方法的名称&#xA;    case *http2.MetaHeadersFrame:&#xA;      // 上层传递过来的handle处理stream&#xA;      if t.operateHeaders(frame, handle, traceCtx) {&#xA;        t.Close()&#xA;        break&#xA;      }&#xA;    // DataFrame, RSTStream, WindowUpdateFrame都属于特定stream id的Stream&#xA;    // 会被分派给对应的Stream&#xA;    case *http2.DataFrame:&#xA;      t.handleData(frame)&#xA;    case *http2.RSTStreamFrame:&#xA;      t.handleRSTStream(frame)&#xA;    case *http2.SettingsFrame:&#xA;      t.handleSettings(frame)&#xA;    case *http2.PingFrame:&#xA;      t.handlePing(frame)&#xA;    case *http2.WindowUpdateFrame:&#xA;      t.handleWindowUpdate(frame)&#xA;    case *http2.GoAwayFrame:&#xA;      // TODO: Handle GoAway from the client appropriately.&#xA;    default:&#xA;      grpclog.Printf(&amp;#34;transport: http2Server.HandleStreams found unhandled frame type %v.&amp;#34;, frame)&#xA;    }&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&#xA;&lt;li&gt;新请求的处理细节(新流的打开和帧数据的处理)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;// 解析流，提取服务名，方法名等信息，handleStream实现的是stream的业务逻辑处理&#xA;func (s *Server) handleStream(t transport.ServerTransport, stream *transport.Stream, trInfo *traceInfo) {&#xA;  sm := stream.Method()&#xA;  if sm != &amp;#34;&amp;#34; &amp;amp;&amp;amp; sm[0] == &amp;#39;/&amp;#39; {&#xA;    sm = sm[1:]&#xA;  }&#xA;  pos := strings.LastIndex(sm, &amp;#34;/&amp;#34;)&#xA;  /* ... */&#xA;  // 服务名&#xA;  service := sm[:pos]&#xA;  // 方法名&#xA;  method := sm[pos+1:]&#xA;  // 服务&#xA;  srv, ok := s.m[service]&#xA;  // 未注册的服务&#xA;  if !ok {&#xA;    if unknownDesc := s.opts.unknownStreamDesc; unknownDesc != nil {&#xA;      s.processStreamingRPC(t, stream, nil, unknownDesc, trInfo)&#xA;      return&#xA;    }&#xA;    /* ... */&#xA;    return&#xA;  }&#xA;  // Unary RPC or Streaming RPC?&#xA;  // 处理单次请求&#xA;  if md, ok := srv.md[method]; ok {&#xA;    s.processUnaryRPC(t, stream, srv, md, trInfo)&#xA;    return&#xA;  }&#xA;  // 处理流式请求&#xA;  if sd, ok := srv.sd[method]; ok {&#xA;    s.processStreamingRPC(t, stream, srv, sd, trInfo)&#xA;    return&#xA;  }&#xA;  &#xA;  // 没找到对应方法&#xA;  if unknownDesc := s.opts.unknownStreamDesc; unknownDesc != nil {&#xA;    s.processStreamingRPC(t, stream, nil, unknownDesc, trInfo)&#xA;    return&#xA;  }&#xA;  /* ... */&#xA;}&#xA;&#xA;// 处理单次请求&#xA;func (s *Server) processUnaryRPC(t transport.ServerTransport, stream *transport.Stream, srv *service, md *MethodDesc, trInfo *traceInfo) (err error) {&#xA;  /* ... */&#xA;  // 发送数据的压缩格式&#xA;  if s.opts.cp != nil {&#xA;    // NOTE: this needs to be ahead of all handling, https://github.com/grpc/gRPC-Go/issues/686.&#xA;    stream.SetSendCompress(s.opts.cp.Type())&#xA;  }&#xA;  // 解析消息&#xA;  p := &amp;amp;parser{r: stream}&#xA;  for { // TODO: delete&#xA;    // 第一个HEADER帧过后，后面的数据帧包含消息数据&#xA;    // 头5个字节：第一个字节代表是否压缩，2-5个字节消息体的长度，后面的数据全部读取给req&#xA;    pf, req, err := p.recvMsg(s.opts.maxMsgSize)&#xA;    /* ... */&#xA;    // 检查压缩类型是否正确&#xA;    if err := checkRecvPayload(pf, stream.RecvCompress(), s.opts.dc); err != nil {&#xA;      /* ... */&#xA;    }&#xA;    // 解压解码等操作，最终数据放到v中，而这个v则指向服务接口实现对应方法的请求参数req&#xA;    df := func(v interface{}) error {&#xA;      if inPayload != nil {&#xA;        inPayload.WireLength = len(req)&#xA;      }&#xA;      if pf == compressionMade {&#xA;        var err error&#xA;        // 解压&#xA;        req, err = s.opts.dc.Do(bytes.NewReader(req))&#xA;        if err != nil {&#xA;          return Errorf(codes.Internal, err.Error())&#xA;        }&#xA;      }&#xA;      // 解压之后超过最大消息长度&#xA;      if len(req) &amp;gt; s.opts.maxMsgSize {&#xA;        // TODO: Revisit the error code. Currently keep it consistent with&#xA;        // java implementation.&#xA;        return status.Errorf(codes.Internal, &amp;#34;grpc: server received a message of %d bytes exceeding %d limit&amp;#34;, len(req), s.opts.maxMsgSize)&#xA;      }&#xA;      // 解码&#xA;      if err := s.opts.codec.Unmarshal(req, v); err != nil {&#xA;        return status.Errorf(codes.Internal, &amp;#34;grpc: error unmarshalling request: %v&amp;#34;, err)&#xA;      }&#xA;      /* ... */&#xA;    }&#xA;&#xA;    // 处理原始消息数据，调用服务方法，这个Handler即上面protobuf的gRPC-Go插件为我们生成的处理函数&#xA;    reply, appErr := md.Handler(srv.server, stream.Context(), df, s.opts.unaryInt)&#xA;    /* ... */&#xA;    // 发送响应，输出会在Transport和Stream两层做流控&#xA;    if err := s.sendResponse(t, stream, reply, s.opts.cp, opts); err != nil {&#xA;      // 单次请求处理完毕，直接返回&#xA;      if err == io.EOF {&#xA;        // The entire stream is done (for unary RPC only).&#xA;        return err&#xA;      }&#xA;      /* ... */&#xA;    }&#xA;    &#xA;    // TODO: Should we be logging if writing status failed here, like above?&#xA;    // Should the logging be in WriteStatus?  Should we ignore the WriteStatus&#xA;    // error or allow the stats handler to see it?&#xA;    // 发送http响应头，关闭stream&#xA;    return t.WriteStatus(stream, status.New(codes.OK, &amp;#34;&amp;#34;))&#xA;  }&#xA;}&#xA;&#xA;// 处理流式方法&#xA;func (s *Server) processStreamingRPC(t transport.ServerTransport, stream *transport.Stream, srv *service, sd *StreamDesc, trInfo *traceInfo) (err error) {&#xA;  /* ... */&#xA;  ss := &amp;amp;serverStream{&#xA;    t:            t,&#xA;    s:            stream,&#xA;    p:            &amp;amp;parser{r: stream},&#xA;    codec:        s.opts.codec,&#xA;    cp:           s.opts.cp,&#xA;    dc:           s.opts.dc,&#xA;    maxMsgSize:   s.opts.maxMsgSize,&#xA;    trInfo:       trInfo,&#xA;    statsHandler: sh,&#xA;  }&#xA;  if ss.cp != nil {&#xA;    ss.cbuf = new(bytes.Buffer)&#xA;  }&#xA;  /* ... */&#xA;  var appErr error&#xA;  var server interface{}&#xA;  if srv != nil {&#xA;    server = srv.server&#xA;  }&#xA;  if s.opts.streamInt == nil {&#xA;    // 调用protobuf gRPC-Go插件生成的ServiceDesc中的Handler&#xA;    appErr = sd.Handler(server, ss)&#xA;  } else {&#xA;    info := &amp;amp;StreamServerInfo{&#xA;      FullMethod:     stream.Method(),&#xA;      IsClientStream: sd.ClientStreams,&#xA;      IsServerStream: sd.ServerStreams,&#xA;    }&#xA;    appErr = s.opts.streamInt(server, ss, info, sd.Handler)&#xA;  }&#xA;  /* ... */&#xA;  // 注意，业务逻辑的实现函数返回后，最终还是会由服务端关闭流&#xA;  return t.WriteStatus(ss.s, status.New(codes.OK, &amp;#34;&amp;#34;))&#xA;}&#xA;&#xA;// 发送响应数据，输出写数据时做了流量的控制&#xA;func (s *Server) sendResponse(t transport.ServerTransport, stream *transport.Stream, msg interface{}, cp Compressor, opts *transport.Options) error {&#xA;  // 编码并压缩&#xA;  p, err := encode(s.opts.codec, msg, cp, cbuf, outPayload)&#xA;  // ok, 写响应，加了出带宽的流控&#xA;  err = t.Write(stream, p, opts)&#xA;  /* ... */&#xA;  return err&#xA;}&#xA;func (t *http2Server) Write(s *Stream, data []byte, opts *Options) (err error) {&#xA;  // TODO(zhaoq): Support multi-writers for a single stream.&#xA;  var writeHeaderFrame bool&#xA;  s.mu.Lock()&#xA;  // stream已经关闭了&#xA;  if s.state == streamDone {&#xA;    s.mu.Unlock()&#xA;    return streamErrorf(codes.Unknown, &amp;#34;the stream has been done&amp;#34;)&#xA;  }&#xA;  // 需要写header&#xA;  if !s.headerOk {&#xA;    writeHeaderFrame = true&#xA;  }&#xA;  s.mu.Unlock()&#xA;  // 写响应头&#xA;  if writeHeaderFrame {&#xA;    t.WriteHeader(s, nil)&#xA;  }&#xA;&#xA;  // 缓冲&#xA;  r := bytes.NewBuffer(data)&#xA;  for {&#xA;    if r.Len() == 0 {&#xA;      return nil&#xA;    }&#xA;    // 每个frame最多16k&#xA;    size := http2MaxFrameLen&#xA;    // ServerTransport的quota默认等于Stream的quota，为默认窗口大小65535字节&#xA;    // 流层限流&#xA;    sq, err := wait(s.ctx, nil, nil, t.shutdownChan, s.sendQuotaPool.acquire())&#xA;    // 传输层限流&#xA;    tq, err := wait(s.ctx, nil, nil, t.shutdownChan, t.sendQuotaPool.acquire())&#xA;    if sq &amp;lt; size {&#xA;      size = sq&#xA;    }&#xA;    if tq &amp;lt; size {&#xA;      size = tq&#xA;    }&#xA;    // 实际需要发送的数据, 返回buf的size长度的slice&#xA;    p := r.Next(size)&#xA;    ps := len(p)&#xA;    // 小于本次的quota，则归还多的部分&#xA;    if ps &amp;lt; sq {&#xA;      // Overbooked stream quota. Return it back.&#xA;      // add会重置channel中的可用quota&#xA;      s.sendQuotaPool.add(sq - ps)&#xA;    }&#xA;    if ps &amp;lt; tq {&#xA;      // Overbooked transport quota. Return it back.&#xA;      t.sendQuotaPool.add(tq - ps)&#xA;    }&#xA;    t.framer.adjustNumWriters(1)&#xA;    // 等待拿到此transport的锁，通过t.writableChan实现，由于可能有多个stream等待写transport，所以需要&#xA;    // 用chan序列化&#xA;    if _, err := wait(s.ctx, nil, nil, t.shutdownChan, t.writableChan); err != nil {&#xA;      /* ... */&#xA;    }&#xA;    select {&#xA;    case &amp;lt;-s.ctx.Done():&#xA;      t.sendQuotaPool.add(ps)&#xA;      if t.framer.adjustNumWriters(-1) == 0 {&#xA;        t.controlBuf.put(&amp;amp;flushIO{})&#xA;      }&#xA;      // 需要释放锁&#xA;      t.writableChan &amp;lt;- 0&#xA;      return ContextErr(s.ctx.Err())&#xA;    default:&#xA;    }&#xA;    var forceFlush bool&#xA;    // 没有剩下的数据可写了，直接flush，注意http2.frame写的时候是写到framer的Buffer writer&#xA;    // 中，需要flush buffer writer，让数据完全写到客户端的net.Conn里去&#xA;    // 注意这里的opts.Last，客户端发送完数据后需要显示调用CloseSend标识opts.Last为true&#xA;    // 只有在不是显示由客户端发送结束标识，并且是最后一个使用这个stream，且没有可再读取&#xA;    // 的数据时才强制flush&#xA;    if r.Len() == 0 &amp;amp;&amp;amp; t.framer.adjustNumWriters(0) == 1 &amp;amp;&amp;amp; !opts.Last {&#xA;      forceFlush = true&#xA;    }&#xA;    // 写到buffer reader中&#xA;    if err := t.framer.writeData(forceFlush, s.id, false, p); err != nil {&#xA;      t.Close()&#xA;      return connectionErrorf(true, err, &amp;#34;transport: %v&amp;#34;, err)&#xA;    }&#xA;    // flush&#xA;    if t.framer.adjustNumWriters(-1) == 0 {&#xA;      t.framer.flushWrite()&#xA;    }&#xA;    // 需要释放锁，让其他stream写&#xA;    t.writableChan &amp;lt;- 0&#xA;  }&#xA;}&#xA;&#xA;// Data帧的处理，直接写到对应流的buf&#xA;func (t *http2Server) handleData(f *http2.DataFrame) {&#xA;  // 根据stream id找到stream&#xA;  s, ok := t.getStream(f)&#xA;&#xA;  if size &amp;gt; 0 {&#xA;    if f.Header().Flags.Has(http2.FlagDataPadded) {&#xA;      if w := t.fc.onRead(uint32(size) - uint32(len(f.Data()))); w &amp;gt; 0 {&#xA;        t.controlBuf.put(&amp;amp;windowUpdate{0, w})&#xA;      }&#xA;    }&#xA;    /* ... */&#xA;    s.mu.Unlock()&#xA;    // TODO(bradfitz, zhaoq): A copy is required here because there is no&#xA;    // guarantee f.Data() is consumed before the arrival of next frame.&#xA;    // Can this copy be eliminated?&#xA;    if len(f.Data()) &amp;gt; 0 {&#xA;      data := make([]byte, len(f.Data()))&#xA;      copy(data, f.Data())&#xA;      // 写入stream的buf&#xA;      s.write(recvMsg{data: data})&#xA;    }&#xA;  }&#xA;  if f.Header().Flags.Has(http2.FlagDataEndStream) {&#xA;    // Received the end of stream from the client.&#xA;    s.mu.Lock()&#xA;    if s.state != streamDone {&#xA;      s.state = streamReadDone&#xA;    }&#xA;    s.mu.Unlock()&#xA;    // 写入stream的buf&#xA;    s.write(recvMsg{err: io.EOF})&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;hr&gt;&#xA;&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;&#xA;&lt;p&gt;至此，服务端的主要流程就基本走完了，整个处理流程还有很多加密、授权、http2连接的控制信息(比如窗口大小的设置等)、KeepAlive逻辑以及穿插在各个地方的统计、追踪、日志处理等细节，这些细节对理解gRPC-Go的实现影响不大，所以不再细说。整个流程下来，多少可以看到Go的很多特性极大地方便了grpc的实现，用goroutine代替多路复用的回调，io的抽象与缓冲。同时，http2整个的模型其实和基于多路复用实现的grpc框架底层数据传输协议有些类似，http2的一个帧类似于某个格式化和序列化后的请求数据或响应数据，但是传统的rpc协议并没有流对应的概念，要实现&amp;quot;流的复用&amp;quot;也不是太容易，请求的下层直接是tcp连接，另外http2是通用的标准化协议，而且复用连接之后其性能也不差。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
