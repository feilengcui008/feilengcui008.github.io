<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on </title>
    <link>https://feilengcui008.github.io/post/</link>
    <description>Recent content in Posts on </description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 28 Jun 2023 19:52:51 +0800</lastBuildDate>
    <atom:link href="https://feilengcui008.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Rust实践</title>
      <link>https://feilengcui008.github.io/post/rust%E5%AE%9E%E8%B7%B5/</link>
      <pubDate>Wed, 28 Jun 2023 19:52:51 +0800</pubDate>
      <guid>https://feilengcui008.github.io/post/rust%E5%AE%9E%E8%B7%B5/</guid>
      <description>&lt;h1 id=&#34;依赖关系引用计数arc与锁mutex&#34;&gt;依赖关系、引用、计数（Arc）与锁（Mutex）&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;功能模块的设计清晰，最终一定是DAG，即使出现循环依赖也可以通过抽取公共部分为第三模块来解除循环依&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Rust里类型初始化每个成员必须强制显示初始化，所以这种循环依赖其实写不出来，虽然也可以通过Option来绕过，但是不推荐这种做法&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;引用是表达依赖关系的方式&#xA;&lt;ul&gt;&#xA;&lt;li&gt;当一个模块只被某个模块唯一引用那么其实就是own的关系，这时在模块初始化时用move语义即可（即拥有所有权）&lt;/li&gt;&#xA;&lt;li&gt;当一个模块被多个模块引用时，会出现生命周期问题，即：被引用模块的生命周期需要大于等于引用它的所有模块的生命周期，否则容易出现use after free问题&lt;/li&gt;&#xA;&lt;li&gt;依赖关系不只是模块（struct）间，也可能是此struct方法成员启动的协程/线程间，每个协程/线程都有自己的生命周期，通过struct方法启动则说明对其有依赖，也就是此struct会被多方引用（即依赖）&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;原子计数（Arc）是解决多引用生命周期问题很自然的工具，类似C++智能指针。Arc对象可以被clone，这样每个引用者就可以own此Arc的拷贝（注意own的是Arc拷贝的所有权，而不是持有引用），而通过此Arc的拷贝，间接持有了目标对象的引用，而Arc的原子计数可以保证目标对象的生命周期至少比其所有引用持有者长。&lt;/li&gt;&#xA;&lt;li&gt;当目标对象通过Arc被多方持有后，生命周期问题得以解决（不会出现use after free问题），但是多个引用者需要并发访问（读写）目标对象。显然，只有Arc原子引用计数是不够的（并且Rust的借用规则并不允许多个&amp;amp;mut），这里需要做到共享与互斥，也就需要Atomic/Mutex/RwLock来控制对通过Arc引用的目标对象的并发读写。（不要去管Send/Sync，定义很难理解&amp;quot;准确&amp;quot;）&#xA;以上，是Arc&amp;lt;Mutex&lt;!-- raw HTML omitted --&gt;&amp;gt;、Arc&amp;lt;RwLock&lt;!-- raw HTML omitted --&gt;&amp;gt;常见用法的由来。常见模式：&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Handler -&amp;gt; TableManager -&amp;gt; ElectionInfo&#xA;Handler -&amp;gt; TabletManager -&amp;gt; ElectionInfo&#xA;&#xA;pub struct Handler {&#xA;    pub table_manager: Arc&amp;lt;TableManager&amp;gt;,  // 持有Arc副本&#xA;    pub tablet_manager: Arc&amp;lt;TabletManager&amp;gt;,&#xA;}&#xA;&#xA;pub struct TableManager {&#xA;    election_info: Arc&amp;lt;ElectionInfo&amp;gt;,&#xA;}&#xA;&#xA;pub struct TabletManager {&#xA;    election_info: Arc&amp;lt;ElectionInfo&amp;gt;,&#xA;    tablets: RwLock&amp;lt;TabletRouter&amp;gt;, // 并发读写&#xA;    id_allocator: AtomicU64, // 并发读写&#xA;}&#xA;&#xA;// 怎么用&#xA;let election_info = Arc::new(ElectionInfo::new());&#xA;let table_manager = Arc::new(TableManager::new(election_info.clone()))&#xA;let tablet_manager = Arc::new(TableManager::new(election_info.clone()))&#xA;let handler = Handler::new(table_manager.clone(), tablet_manager.clone())&#xA;tokio::spawn({&#xA;    election_info.xxx&#xA;})&#xA;tokio::spawn({&#xA;    table_manager.yyy&#xA;})&#xA;tokio::spawn({&#xA;    tablet_manager.zzz&#xA;})&#xA;tokio::spawn({&#xA;    handler.table_manager.yyy&#xA;})&#xA;tokio::spawn({&#xA;    handler.tablet_manager.id_allocator.load()&#xA;    let router = handler.tablet_manager.tablets.write().unwrap();&#xA;    router.ggg&#xA;    // router unlocked&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;错误处理&#34;&gt;错误处理&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;实现自定义Error的From并善用?;能极大降低不同层次错误转换的冗余代码，当然有时需要更丰富的错误信息时，可以手动构造错误。此外，出现错误时希望打印日志或者metrics等信息然后通过?;快速返回，可以使用map_err&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;pub async fn init_table(&amp;amp;self, table: &amp;amp;Table) -&amp;gt; Result&amp;lt;(), TabletManagerError&amp;gt; {&#xA;    if self.partitions.read().unwrap().get(&amp;amp;table_name).is_some() {&#xA;       // 手动构造TabletManagerError，errmsg中注入更多信息&#xA;       return Err(TabletManagerError::new(&amp;amp;format!(&#xA;           &amp;#34;{}:table {table_name} partitions already exist, can not init again&amp;#34;,&#xA;           ManagerErrorKind::TableAlreadyExist,&#xA;       )));&#xA;    }&#xA;    ....&#xA;    // map_err打印错误信息&#xA;    let rw_replica_val = serde_json::to_string(&amp;amp;rw_replica_route_info).map_err(|err| {&#xA;        tracing::error!(&#xA;            &amp;#34;init tablet serialize rw replica data failed {err}, {table_name} {tablet_id}&amp;#34;&#xA;        );&#xA;        err&#xA;    })?; // ?;错误快速返回，并由于TabletManagerError实现了From，这里会自动转换json错误为TabletManagerError&#xA;    ....&#xA;}&#xA;&#xA;impl From&amp;lt;serde_json::Error&amp;gt; for TabletManagerError {&#xA;    fn from(e: serde_json::Error) -&amp;gt; Self {&#xA;        TabletManagerError::new(&amp;#34;SystemError:serde json error&amp;#34;)&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&#xA;&lt;li&gt;在不希望明确具体的错误类型时，可以使用anyhow::Result/anyhow::format_error!&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;// 返回Result&amp;lt;&amp;gt;类型&#xA;pub async fn on_leader_start(&amp;amp;self) -&amp;gt; Result&amp;lt;()&amp;gt; {&#xA;    self.tablet_task_manager&#xA;        .on_leader_start()&#xA;        .await&#xA;        .map_err(|err| {&#xA;            // 构造anyhow::Error&#xA;            anyhow::format_err!(&amp;#34;call tablet task manager on leader start failed {err}&amp;#34;,)&#xA;    })&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;stdsync--tokiosync&#34;&gt;std::sync &amp;amp; tokio::sync&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;垮await或者会导致线程长时间阻塞（例如睡眠挂起），用tokio::sync::Mutex/RwLock/Channel&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;pub struct SystemInfoManager&amp;lt;T: KvStore&amp;gt; {&#xA;    replica_epoch_allocator: tokio::sync::Mutex&amp;lt;IdAllocator&amp;lt;T&amp;gt;&amp;gt;,&#xA;}&#xA;&#xA;impl&amp;lt;T: KvStore&amp;gt; SystemInfoManager&amp;lt;T&amp;gt; {&#xA;    pub async fn alloc_replica_epoch(&amp;amp;self) -&amp;gt; Result&amp;lt;u64, SystemInfoManagerError&amp;gt; {&#xA;        // replica_epoch_allocator跨await，需要使用异步版本的Mutex，否则可能导致线程hang&#xA;        let mut id_allocator = self.replica_epoch_allocator.lock().await;&#xA;        let new_id = id_allocator.alloc_id().await.map_err(|err| {&#xA;            let err_msg = format!(&amp;#34;alloc replica epoch failed {err}&amp;#34;);&#xA;            tracing::error!(err_msg);&#xA;            SystemInfoManagerError::new(&amp;amp;err_msg)&#xA;        })?;&#xA;        Ok(new_id)&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&#xA;&lt;li&gt;不垮await且临界区较短，使用std::sync::Mutex/RwLock，性能更好&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;结构体方法中启动协程定时任务&#34;&gt;结构体方法中启动协程定时任务&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;参数Arc&lt;!-- raw HTML omitted --&gt;，原因在于tokio::spawn的协程需要共享结构体的所有权，并对其进行读写访问&lt;/li&gt;&#xA;&lt;li&gt;tokio::select!，类似go中的select {}&lt;/li&gt;&#xA;&lt;li&gt;tokio_util::sync::CancellationToken&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;impl TabletReplicaController {&#xA;    pub fn run(self: Arc&amp;lt;Self&amp;gt;) {&#xA;        lust::spawn({&#xA;            async move {&#xA;                let cloned_token = self.base.cancel_token.clone();&#xA;                let mut tick_count = 0;&#xA;                loop {&#xA;                    let interval =&#xA;                        Duration::from_secs(get_config().tablet_replica_reconcile_interval_sec);&#xA;                    let enabled = get_config().enable_tablet_replica_reconcile_controller;&#xA;                    tokio::select! {&#xA;                        _ = cloned_token.cancelled() =&amp;gt; {&#xA;                            tracing::info!(&amp;#34;tablet replica reconcile controller exit on stop signal&amp;#34;);&#xA;                            return;&#xA;                        }&#xA;                        _ = tokio::time::sleep(interval) =&amp;gt; {&#xA;                            tick_count += 1;&#xA;                            if !self.base.election_info.is_leader() {&#xA;                                tracing::info!(&amp;#34;tablet replica reconcile controller not on leader, just skip&amp;#34;);&#xA;                                continue;&#xA;                            }&#xA;                            if !enabled {&#xA;                                tracing::info!(&amp;#34;tablet replica reconcile controller not enabled, just skip&amp;#34;);&#xA;                                continue;&#xA;                            }&#xA;                            if let Err(err) = self.do_tablet_replica_reconcile().await {&#xA;                                metrics(metric::defs::TABLET_REPLICA_CONTROLLER_RECONCILE_FAIL_COUNT).emit(tags![], 1);&#xA;                                tracing::error!(&amp;#34;do tablet replica reconcile failed {err}, tick {tick_count}&amp;#34;);&#xA;                                continue;&#xA;                            }&#xA;                            metrics(metric::defs::TABLET_REPLICA_CONTROLLER_RECONCILE_SUCCEED_COUNT).emit(tags![], 1);&#xA;                            tracing::debug!(&amp;#34;do tablet replica reconcile success, tick {tick_count}&amp;#34;);&#xA;                        }&#xA;                    }&#xA;                }&#xA;            }&#xA;        });&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;send--sync&#34;&gt;Send &amp;amp; Sync&lt;/h1&gt;&#xA;&lt;p&gt;按Rust的官方定义：&lt;/p&gt;</description>
    </item>
    <item>
      <title>有状态服务托管的两种模式</title>
      <link>https://feilengcui008.github.io/post/%E6%9C%89%E7%8A%B6%E6%80%81%E6%9C%8D%E5%8A%A1%E6%89%98%E7%AE%A1%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%A8%A1%E5%BC%8F/</link>
      <pubDate>Sat, 22 Feb 2020 15:44:41 +0800</pubDate>
      <guid>https://feilengcui008.github.io/post/%E6%9C%89%E7%8A%B6%E6%80%81%E6%9C%8D%E5%8A%A1%E6%89%98%E7%AE%A1%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%A8%A1%E5%BC%8F/</guid>
      <description>&lt;p&gt;通常情况下，PaaS托管的大部分负载都是无状态应用。但是对于一些依赖数据的特殊应用（比如分布式文件系统、zk、mysql），PaaS也应当提供相应的扩展能力，支持这些应用的托管。本质上讲，PaaS支持不同类型的工作负载托管的核心在策略，类似K8S的controller，这些策略适配了不同工作负载的工作方式。对于有状态应用托管，有两种方式。&lt;/p&gt;&#xA;&lt;h2 id=&#34;operator&#34;&gt;Operator&lt;/h2&gt;&#xA;&lt;p&gt;基于K8S的CRD，工作负载的策略以一个Operator的形式实现。这种方式策略的所有逻辑完全实现在PaaS内部，对于不同的有状态应用，PaaS可能需要实现不同的Operator。&lt;/p&gt;&#xA;&lt;h2 id=&#34;回调&#34;&gt;回调&lt;/h2&gt;&#xA;&lt;p&gt;策略由业务自身实现，以Server回调接口的形式暴露给PaaS，PaaS实现删除容器时的回调逻辑，负责通知业务当前容器的状态，由业务的策略Server负责自身业务状态的处理（比如数据搬迁），给PaaS返回成功后，PaaS才真正删除容器。这种方式的好处是，回调机制对于有状态应用是完全通用的，且对于大公司来说，内部大部分有状态应用（比如文件系统、数据库）都已经有了自己的管控系统，只需要实现一个旁路的策略Server即可，成本不高。&lt;/p&gt;&#xA;&lt;p&gt;当然，基于回调的业务逻辑Server如果由PaaS来实现，其实就很类似Operator了。&lt;/p&gt;</description>
    </item>
    <item>
      <title>PaaS、服务管理和资源管理</title>
      <link>https://feilengcui008.github.io/post/paas%E6%9C%8D%E5%8A%A1%E7%AE%A1%E7%90%86%E5%92%8C%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86/</link>
      <pubDate>Sun, 08 Dec 2019 13:36:55 +0800</pubDate>
      <guid>https://feilengcui008.github.io/post/paas%E6%9C%8D%E5%8A%A1%E7%AE%A1%E7%90%86%E5%92%8C%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86/</guid>
      <description>&lt;h2 id=&#34;paas&#34;&gt;PaaS&lt;/h2&gt;&#xA;&lt;p&gt;PaaS是什么？怎么做？我想现在也没有一个公司或者开源项目能够给出统一的回答。PaaS本身就应该是面向场景的，它需要上层模型（应用模型和workflow），即使是各公司内部的PaaS也都受限于各种原因（业务场景、研发流程、IDC等）在上层模型上有自己的特点，简单给k8s什么的加个ui，还是大量照搬k8s原生的概念（比如deployment）绝不是一个理想易用的PaaS，典型例子openshift。K8s这些概念抽象得很好，但是并不是研发和运维共同native，而是运维native的，对研发来说并不是最好的概念。对于研发来说，我可能要上线的是一个应用，而不是一个deployment，概念的区别和定位一下就出来了。&lt;/p&gt;&#xA;&lt;p&gt;公司搞一个”通用“的统一PaaS就够了吗？显然不是。业务场景的多样性和复杂性，以及它们对研发运维流程的特殊要求，决定了垂类PaaS必定会存在的，比如机器学习平台。我可能底层还是基于公司的统一的PaaS，但是上层模型一定是不一样的，搞机器学习的算法工程师一定是需要一套顺手的workflow的，这一套workflow跟通常的研发相比，一定是有自己的特定要求的。其他垂类PaaS，比如针对web类型应用的，针对异构计算的，针对金融场景的&amp;hellip;&amp;hellip;&lt;/p&gt;&#xA;&lt;h2 id=&#34;服务管理&#34;&gt;服务管理&lt;/h2&gt;&#xA;&lt;p&gt;服务这个词概念很模糊，这里定义为一组容器，类似k8s的deployment或者service的概念，这里重点在管理。一组容器在那了，它可能由于机器节点宕机、网络故障、程序bug变得异常，我们需要修复；另外，容器资源随着业务流量的增减也需要自适应地变更，这些本质上都是在做一组容器（即服务）的管理，如何自动化地做这些事情？策略化。K8s的各种controller以及很多概念本质上就是各种服务管理策略，是面向自动化运维的，而不是面向研发。&lt;/p&gt;&#xA;&lt;h2 id=&#34;资源管理&#34;&gt;资源管理&lt;/h2&gt;&#xA;&lt;p&gt;容器创建、调度、状态，机器节点的管理，网络，quota配额，混部资源QOS，单机隔离&amp;hellip;&amp;hellip;这些才是资源管理的事情。资源管理把最基本的管好了，服务管理在策略层面去做更多的事情，分工明确，皆大欢喜。&lt;/p&gt;&#xA;&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;&#xA;&lt;p&gt;给k8s加个ui一定不是个理想易用的PaaS（可能小公司无所谓），无论统一的PaaS还是垂类的PaaS一定需要有上层模型。另外，实际上k8s把资源管理和服务管理都做了，另一种思路是服务管理其实可以到更上层一些或者外围来做。广义PaaS摊子可以很大，中间这一层不好做，用户客服问题会非常之多，垂类PaaS或者更往下的资源管理相对好些。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Raft读请求</title>
      <link>https://feilengcui008.github.io/post/raft%E8%AF%BB%E8%AF%B7%E6%B1%82/</link>
      <pubDate>Thu, 08 Mar 2018 19:52:51 +0800</pubDate>
      <guid>https://feilengcui008.github.io/post/raft%E8%AF%BB%E8%AF%B7%E6%B1%82/</guid>
      <description>&lt;h2 id=&#34;raft保证读请求linearizability的方法&#34;&gt;Raft保证读请求Linearizability的方法&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;1.Leader把每次读请求作为一条日志记录，以日志复制的形式提交，并应用到状态机后，读取状态机中的数据返回。（一次RTT、一次磁盘写）&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;2.使用Leader Lease，保证整个集群只有一个Leader，Leader接收到都请求后，记录下当前的commitIndex为readIndex，当applyIndex大于等于readIndex 后，则可以读取状态机中的数据返回。（0次RTT、0次磁盘写）&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;3.不使用Leader Lease，而是当Leader通过以下两点来保证整个集群中只有其一个正常工作的Leader：（1）在每个Term开始时，由于新选出的Leader可能不知道上一个Term的commitIndex，所以需要先在当前新的Term提交一条空操作的日志；（2）Leader每次接到读请求后，向多数节点发送心跳确认自己的Leader身份。之后的读流程与Leader Lease的做法相同。（一次RTT、0次磁盘写）&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;4.从Follower节点读：Follower先向Leader询问readIndex，Leader收到Follower的请求后依然要通过2或3中的方法确认自己Leader的身份，然后返回当前的commitIndex作为readIndex，Follower拿到readIndex后，等待本地的applyIndex大于等于readIndex后，即可读取状态机中的数据返回。（2次或1次RTT、0次磁盘写）&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;raft保证读请求sequential-consistency的方法&#34;&gt;Raft保证读请求Sequential Consistency的方法&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Leader处理每次读写、Follower处理每次读请求时，都返回本节点的applyIndex，客户端在本地保存自己看到的最新的applyIndex。客户端每次请求时都带上这个applyIndex（假设为clientIndex），Leader或者Follower拿客户端请求中的clientIndex和自己本地的applyIndex比较，如果applyIndex大于等于clientIndex，则可以读取状态机数据返回，否则等待，直到applyIndex大于等于clientIndex。（0次RTT、0次写磁盘）&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;linearizability和sequential-consistency的区别&#34;&gt;Linearizability和Sequential Consistency的区别&lt;/h2&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Linearizability - All processes see all shared accesses in the same order. Accesses are furthurmore ordered according to a global timestamp&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Sequential - All processes see all shared accesses in the same order. Accesses are not ordered in time.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;举个例：在Raft中，Linearizability读保证任何一个客户端的读请求对其他所有客户端后续的请求都是可见的，而Sequential读保证某个客户端自己的读请求对后续自己的请求是可见的。比如其中一个客户端提交了写请求且Leader上更新了状态机，接着此客户端向Leader发起读请求，而由于Follower可能还没有apply这条日志，所以另一个客户端向Follower发起了读请求，这样两个客户端将读取到不同的数据。&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>读在2017</title>
      <link>https://feilengcui008.github.io/post/%E8%AF%BB%E5%9C%A82017/</link>
      <pubDate>Sun, 31 Dec 2017 23:20:45 +0800</pubDate>
      <guid>https://feilengcui008.github.io/post/%E8%AF%BB%E5%9C%A82017/</guid>
      <description>&lt;p&gt;由于一些原因，这一年比往年多读了不少书，而且基本是集中在岁末三个月。除了基本的知识摄取和休闲娱乐外，另一个目的是想从中寻找一些问题的答案，而这些问题让我很是恼火。虽最终未能如愿，但或多或少影响了认知，“不求甚解”的一个好处可能就在于潜移默化吧。&lt;/p&gt;&#xA;&lt;p&gt;总体来说，这一年读书比较杂。究其原因，一方面是由于自身所处困境的实际需求，另一方面是由于上大学以后阅读量急剧减少(惭愧&amp;hellip;)，欠的阅读债比较多，再者便是专业上的需求。按学科划分来讲，社会科学方面主要集中在心理学，人文方面主要是文学和文学史，自然科学方面则是本行计算机（就不放在这里了）。这里主要挑出一些对我有些影响的书。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;心理学&#34;&gt;心理学&lt;/h2&gt;&#xA;&lt;p&gt;心理学方面，主要是从入门读物、基础教材、细分流派和研究领域几个方面来阅读的。整理了一个豆列。这里挑了我看过的和比较感兴趣的方向。&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;少有人走的路&#xA;★★★★☆8.4&#xA;心智成熟的旅程 / [美] M·斯科特·派克 / 吉林文史出版社&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;第一本，非它莫属。开篇第一句便透露了生活的真相。后面自律、爱、成长和信仰等几部分更是颇有同感。&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;我们时代的神经症人格&#xA;★★★★★9.0&#xA;卡伦·霍尼 / 译林出版社&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;本身作为患者，霍尼关于神经症的分析淋漓尽致。&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;活出生命的意义&#xA;★★★★★8.7&#xA;[奥] 维克多·弗兰克 / 华夏出版社&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;可能，你有那么一段时间，“一切对我来说都毫无意义，突然之间”（《我是个年轻人，我心情不太好》），可以看看这本书。事业、爱与体验、苦难本身的意义。&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;改变心理学的40项研究&#xA;★★★★★8.8&#xA;探索心理学研究的历史 / 〔美〕Roger R Hock著 / 中国轻工业出版社&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;心理学的一些著名研究成果。&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;这才是心理学（第9版）&#xA;★★★★★9.0&#xA;看穿世界的批判性思维 / 基思·斯坦诺维奇 (Keith E.Stanovich) / 人民邮电出版社&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;用科学讲心理学。（其实不如说是用心理学讲科学&amp;hellip;&amp;hellip;）&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;心理学导论&#xA;★★★★★9.3&#xA;思想与行为的认识之路 / Dennis Coon / 中国轻工业出版社&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;不错的心理学入门教材，通俗易懂，内容丰富。还有《津巴多普通心理学》和《心理学与生活》这两本书也不错，可惜图书馆一直没借到，也就还没看。&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;人格心理学&#xA;★★★★★8.9&#xA;万千心理 / Jerry M. Burger / 中国轻工业出版社&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;各流派关于人格的理论。&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;发展心理学&#xA;★★★★★8.7&#xA;人的毕生发展 / [美] 罗伯特·费尔德曼 / 世界图书出版公司北京公司&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linux内核栈与thread_info</title>
      <link>https://feilengcui008.github.io/post/linux%E5%86%85%E6%A0%B8%E6%A0%88/</link>
      <pubDate>Mon, 09 Oct 2017 11:41:42 +0800</pubDate>
      <guid>https://feilengcui008.github.io/post/linux%E5%86%85%E6%A0%B8%E6%A0%88/</guid>
      <description>&lt;h2 id=&#34;内核栈与thread_info&#34;&gt;内核栈与thread_info&lt;/h2&gt;&#xA;&lt;p&gt;Linux内核在x86平台下，PAGE_SIZE为4KB(32位和64位相同)，THREAD_SIZE为8KB(32位)或者16KB(64位)。THREAD_SIZE表示了整个内核栈的大小，栈可以向下增长(栈低在高地址)或者向上增长(栈低在低地址)，后面的分析都是基于向下增长的方式。如图中所示，整个内核栈可分为四个部分，从低地址开始依次为:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;thread_info结构体&lt;/li&gt;&#xA;&lt;li&gt;溢出标志&lt;/li&gt;&#xA;&lt;li&gt;从溢出标志开始到kernel_stack之间的实际可用栈内存空间，kernel_stack为percpu变量，通过它可间接找到内核栈的起始地址&lt;/li&gt;&#xA;&lt;li&gt;从kernel_stack到栈底的长度为KERNEL_STACK_OFFSET的保留空间&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://feilengcui008.github.io/images/kernel_stack.jpg&#34; alt=&#34;kernel_stack&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;内核引入thread_info的一大原因是方便通过它直接找到进(线)程的task_struct指针，x86 平台的thread_info结构体定义在arch/x86/include/asm/thread_info.h。&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;// Linux 3.19.3 x86平台的thread_info&#xA;struct thread_info {&#xA;&#x9;struct task_struct&#x9;*task;&#x9;&#x9;/* main task structure */&#xA;&#x9;struct exec_domain&#x9;*exec_domain;&#x9;/* execution domain */&#xA;&#x9;__u32&#x9;&#x9;&#x9;flags;&#x9;&#x9;/* low level flags */&#xA;&#x9;__u32&#x9;&#x9;&#x9;status;&#x9;&#x9;/* thread synchronous flags */&#xA;&#x9;__u32&#x9;&#x9;&#x9;cpu;&#x9;&#x9;/* current CPU */&#xA;&#x9;int&#x9;&#x9;&#x9;saved_preempt_count;&#xA;&#x9;mm_segment_t&#x9;&#x9;addr_limit;&#xA;&#x9;void __user&#x9;&#x9;*sysenter_return;&#xA;&#x9;unsigned int&#x9;&#x9;sig_on_uaccess_error:1;&#xA;&#x9;unsigned int&#x9;&#x9;uaccess_err:1;&#x9;/* uaccess failed */&#xA;};&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;由于thread_info结构体恰好位于内核栈的低地址开始处，所以只要知道内核栈的起始地址，就可以通过其得到thread_info，进而得到task_struct，后面会分析这个过程的实现。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;current宏&#34;&gt;current宏&lt;/h2&gt;&#xA;&lt;p&gt;current宏在Linux 内核中负责获取当前cpu上的task_struct，通常是借助thread_info和内核栈实现，这种方式的主要逻辑是:&lt;/p&gt;</description>
    </item>
    <item>
      <title>ABI</title>
      <link>https://feilengcui008.github.io/post/abi/</link>
      <pubDate>Thu, 21 Sep 2017 16:32:02 +0800</pubDate>
      <guid>https://feilengcui008.github.io/post/abi/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Application_binary_interface&#34;&gt;ABI&lt;/a&gt;指应用二进制接口，规定了二进制程序两个模块之间或者二进制程序与操作系统之间的接口，这里主要关注&lt;a href=&#34;https://en.wikipedia.org/wiki/Calling_convention&#34;&gt;调用规范call convention&lt;/a&gt;。不同的体系结构、操作系统、编程语言、每种编程语言的不同编译器实现基本都有自己规定或者遵循的ABI和调用规范。另外，也可通过&lt;a href=&#34;https://en.wikipedia.org/wiki/Foreign_function_interface&#34;&gt;FFI&lt;/a&gt;规范实现跨编程语言的过程调用，比如Python/Java/Go等提供了C的FFI，这样通过C实现互相调用。&lt;/p&gt;&#xA;&lt;p&gt;Linux在x86_64和i386下的ABI:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/X86_calling_conventions&#34;&gt;x86下的调用规范&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://stackoverflow.com/questions/2535989/what-are-the-calling-conventions-for-unix-linux-system-calls-on-x86-64&#34;&gt;Linux i386 and x86_64 call convention&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://refspecs.linuxfoundation.org/elf/x86_64-abi-0.99.pdf&#34;&gt;x86_64下用户态程序和系统调用ABI&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://refspecs.linuxfoundation.org/elf/abi386-4.pdf&#34;&gt;i386下用户态和系统调用ABI&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;这里就不详细解释不同的ABI和调用规范了，可以通过简单的C/C++程序和内核代码分别验证用户态和系统调用的规范。另外，对于类似Go语言有自己的一套函数&lt;a href=&#34;https://github.com/golang/go/issues/16922&#34;&gt;调用规范&lt;/a&gt;的，也可以通过生成的汇编去验证。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Fuzz go struct using reflection</title>
      <link>https://feilengcui008.github.io/post/fuzz-go-struct-using-reflection/</link>
      <pubDate>Mon, 24 Jul 2017 09:48:05 +0800</pubDate>
      <guid>https://feilengcui008.github.io/post/fuzz-go-struct-using-reflection/</guid>
      <description>&lt;p&gt;有时为了测试接口，需要伪造一些随机的请求数据，所以基于Go反射写了一个&lt;a href=&#34;https://github.com/feilengcui008/pieces/blob/master/go/lib/fuzz/fuzz.go&#34;&gt;fuzz&lt;/a&gt;小工具来自动填充请求结构体，基本上支持大部分的Go类型：integer、float、bool、string、slice、map、struct、pointer，而且支持非导出类型(包括非导出的nil value)。实现的思路比较简单，只是有两个地方使用的小trick值得提一提，对深入理解Go的反射有些帮助。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;1.unexported字段&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Go的反射不允许对struct中未导出的字段设置值，即是unsettable的，所以无法直接使用Set或SetXXX的方法。这里的技巧在于reflect.NewAt，这个函数可以在当前reflect.Value指向的数据的同一内存地址重新构造相同类型的值，并返回指针的reflect.Value，而其Elem是settable的，所以可以通过这种方式绕过限制，具体可参考&lt;a href=&#34;https://github.com/feilengcui008/pieces/blob/master/go/lib/fuzz/fuzz.go#L77&#34;&gt;这行代码&lt;/a&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;2.unexported且nil的字段&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;除了需要unexported字段相同的处理方式，由于nil字段反射后是nil value，其Elem是zero value，而zero value是unsettable和unaddressable的，因此需要新建一个Elem类型的值(reflect.New)并赋给nil value，此后其Elem就可以使用Set/SetXXX正常赋值了，具体可参考&lt;a href=&#34;https://github.com/feilengcui008/pieces/blob/master/go/lib/fuzz/fuzz.go#L67&#34;&gt;这几行代码&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Tcplayer介绍</title>
      <link>https://feilengcui008.github.io/post/tcplayer%E4%BB%8B%E7%BB%8D/</link>
      <pubDate>Wed, 19 Jul 2017 20:06:51 +0800</pubDate>
      <guid>https://feilengcui008.github.io/post/tcplayer%E4%BB%8B%E7%BB%8D/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://github.com/feilengcui008/tcplayer&#34;&gt;Tcplayer&lt;/a&gt;是最近写的一个流量抓取、解析、放大、重放工具。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;背景&#34;&gt;背景&lt;/h3&gt;&#xA;&lt;p&gt;通常真实流量相比手工构造的请求来说，更有利于测试。真实流量的回放大体上有以下三种方式：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;应用层&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;这种方式通常是在服务中耦合拷贝请求的代码。由于直接工作在应用层，截下来的流量是一个个完整的请求，所以其支持场景最多，实现也相对简单，但是需要耦合其他代码，也会给服务程序带来资源消耗。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;网络层&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;这种方式通常是抓取原始网络包，解析出IP报文，修改报文的目的IP和端口，伪造与测试机的TCP会话，回放到测试机。其优点是不需要处理传输层的TCP包排序，也不需要理解上层应用层的请求格式，但其缺点是配置相对复杂，且难以支持长连接。如果在长连接已经建立的情况下抓包回放，由于测试机并未经过任何SYN-SYN/ACK-ACK的请求建立过程，所以所有请求的TCP PUSH包都会被测试机RST丢掉。通常后端RPC服务都是长连接，所以这是一个比较大的问题。这里有一个工作在这一层的开源工具&lt;a href=&#34;https://github.com/session-replay-tools/tcpcopy&#34;&gt;tcpcopy&lt;/a&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;传输层&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;为了解决外部代码依赖以及长连接的问题，tcplayer基于TCP传输层，按照应用层请求格式解析出请求并重放。这种方式可以抓取到已经建立的长连接的请求，并且服务不需要耦合其他代码，但同时引入了解析和匹配应用层协议的复杂性。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;tcplayer&#34;&gt;Tcplayer&lt;/h3&gt;&#xA;&lt;p&gt;Tcplayer主要包含以下几个步骤:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;libpcap抓取实时流量&lt;/li&gt;&#xA;&lt;li&gt;tcp包重排序&lt;/li&gt;&#xA;&lt;li&gt;对于每一个tcp会话(flow)，解析tcp包，尽量匹配应用层协议&lt;/li&gt;&#xA;&lt;li&gt;放大应用层请求，并与测试服务建立连接回放&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;目前支持的协议:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;HTTP 1.x 短连接&lt;/li&gt;&#xA;&lt;li&gt;Thrift strict mode binary protocol&lt;/li&gt;&#xA;&lt;li&gt;GRPC部分支持&#xA;&lt;ul&gt;&#xA;&lt;li&gt;这里由于GRPC基于HTTP2，而HTTP2的基本单元Frame没有固定的协议字段，所以无法匹配，导致无法解析到正确的HTTP2请求&amp;hellip;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;要给tcplayer添加其他自己定义的应用层协议很容易，可以参考代码&lt;a href=&#34;https://github.com/feilengcui008/tcplayer/blob/master/factory/thrift.go&#34;&gt;factory/thrift&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>迁移到Hugo</title>
      <link>https://feilengcui008.github.io/post/%E8%BF%81%E7%A7%BB%E5%88%B0hugo/</link>
      <pubDate>Tue, 06 Jun 2017 15:13:08 +0800</pubDate>
      <guid>https://feilengcui008.github.io/post/%E8%BF%81%E7%A7%BB%E5%88%B0hugo/</guid>
      <description>&lt;p&gt;之前一直使用&lt;a href=&#34;https://hexo.io/&#34;&gt;hexo&lt;/a&gt;作为markdown的静态页面生成器，各种主题插件支持也很完善，但是其缺点是一大堆nodejs的依赖，而且生成速度非常慢。最近发现一个Go写的静态页面生成器&lt;a href=&#34;https://gohugo.io/&#34;&gt;hugo&lt;/a&gt;，由于直接编译生成一个二进制文件，所以解决了包依赖的问题，实测其生成速度确实比hexo快太多。因此决定切换到hugo，顺便将之前基于&lt;a href=&#34;https://github.com/forsigner/fexo&#34;&gt;fexo&lt;/a&gt;改的hexo主题移植到hugo上了，代码仓库在&lt;a href=&#34;https://github.com/feilengcui008/toys/tree/master/simpost-theme&#34;&gt;simpost-theme&lt;/a&gt;。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Go调度详解</title>
      <link>https://feilengcui008.github.io/post/go%E8%B0%83%E5%BA%A6%E8%AF%A6%E8%A7%A3/</link>
      <pubDate>Tue, 09 May 2017 19:40:07 +0800</pubDate>
      <guid>https://feilengcui008.github.io/post/go%E8%B0%83%E5%BA%A6%E8%AF%A6%E8%A7%A3/</guid>
      <description>&lt;h2 id=&#34;1-基本单元&#34;&gt;1. 基本单元&lt;/h2&gt;&#xA;&lt;p&gt;Go调度相关的四个基本单元是g、m、p、schedt。g是协程任务信息单元，m实际执行体，p是本地资源池和g任务池，schedt是全局资源池和g任务池。这里的m对应一个os线程，所以整个执行逻辑简单来说就是&amp;quot;某个os线程m不断尝试拿资源p并找任务g执行，没有可执行g则睡眠，等待唤醒并重复此过程&amp;quot;，这个执行逻辑加上sysmon系统线程的定时抢占逻辑实际上就是整个宏观的调度逻辑了(其中穿插了很多唤醒m、system goroutine等等复杂的细节)，而找协程任务g的过程占据了其中大部分。g的主要来源有本地队列、全局队列、其他p的本地队列、poller(net和file)，以及一些system goroutine比如timerproc、bgsweeper、gcMarkWorker、runfinq、forcegchelper等。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;2-调度的整体流程&#34;&gt;2. 调度的整体流程&lt;/h2&gt;&#xA;&lt;p&gt;(1) 关于g0栈和g栈&lt;/p&gt;&#xA;&lt;p&gt;由于m是实际执行体，m的整个代码逻辑基本上就是整个调度逻辑。类似于Linux的内核栈和用户栈，Go的m也有两类栈：一类是系统栈(或者叫调度栈)，主要用于运行runtime的程序逻辑；另一类是g栈，用于运行g的程序逻辑。每个m在创建时会分配一个默认的g叫g0，g0不执行任何代码逻辑，只是用来存放m的调度栈等信息。当要执行Go runtime的一些逻辑比如创建g、新建m等，都会首先切换到g0栈然后执行，而执行g任务时，会切换到g的栈上。在调度栈和g栈上不断切换使整个调度过程复杂了不少。&lt;/p&gt;&#xA;&lt;p&gt;(2) 关于m的spinning自旋&lt;/p&gt;&#xA;&lt;p&gt;在Go的调度中，m一旦被创建则不会退出。在syscall、cgocall、lockOSThread时，为了防止阻塞其他g的执行，Go会新建或者唤醒m(os线程)执行其他的g，所以可能导致m的增加。如何保证m数量不会太多，同时有足够的线程使p(cpu)不会空闲？主要的手段是通过多路复用和m的spinning。多路复用解决网络和文件io时的阻塞(与net poll类似，Go1.8.1的代码中为os.File加了poll接口)，避免每次读写的系统调用消耗线程。而m的spinning的作用是尽量保证始终有m处于spinning寻找g(并不是执行g，充分利用多cpu)的同时，不会有太多m同时处于spinning(浪费cpu)。不同于一般意义的自旋，m处于自旋是指m的本地队列、全局队列、poller都没有g可运行时，m进入自旋并尝试从其他p偷取(steal)g，每当一个spinning的m获取到g后，会退出spinning并尝试唤醒新的m去spinning。所以，一旦总的spinning的m数量大于0时，就不用唤醒新的m了去spinning浪费cpu了。&lt;/p&gt;&#xA;&lt;p&gt;(3) 整个调度的流程图&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;schedule&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://feilengcui008.github.io/images/schedule.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;findrunnable&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://feilengcui008.github.io/images/findrunnable.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;3-m的视角看调度&#34;&gt;3. m的视角看调度&lt;/h2&gt;&#xA;&lt;p&gt;(1) Go中的m大概可分为以下几种&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;系统线程，比如sysmon，其运行不需要p&lt;/li&gt;&#xA;&lt;li&gt;lockedm，与某个g绑定，未拿到对应的lockedg时睡眠，等待被唤醒，无法被调度&lt;/li&gt;&#xA;&lt;li&gt;陷入syscall的m，执行系统调用中，返回时进入调度逻辑&lt;/li&gt;&#xA;&lt;li&gt;cgo的m，cgo的调用实际上使用了lockedm和syscall&lt;/li&gt;&#xA;&lt;li&gt;正在执行goroutine的m&lt;/li&gt;&#xA;&lt;li&gt;正在执行调度逻辑的m&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;(2) 什么时候可能需要新建或者唤醒m&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;有新的可运行g或者拿到可运行的g&#xA;&lt;ul&gt;&#xA;&lt;li&gt;goready，将g入队列&lt;/li&gt;&#xA;&lt;li&gt;newproc，新建g并入队列&lt;/li&gt;&#xA;&lt;li&gt;m从schedule拿到g，自身退出spinning&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;有p资源被释放handoff(p)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;(3) m何时交出资源p，并进入睡眠&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;lockedm主动交出p&lt;/li&gt;&#xA;&lt;li&gt;处于syscall中，并被sysmon抢占(超过10ms)交出p&lt;/li&gt;&#xA;&lt;li&gt;cgocall被sysmon抢占交出p，或由于lockedm主动交出p&lt;/li&gt;&#xA;&lt;li&gt;findrunnable没找到可运行的g，主动交出p，进入睡眠&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;4-g的视角看调度&#34;&gt;4. g的视角看调度&lt;/h2&gt;&#xA;&lt;p&gt;(1) 与goroutine相关的调度逻辑&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;go(runtime.newproc)产生新的g，放到本地队列或全局队列&lt;/li&gt;&#xA;&lt;li&gt;gopark，g置为waiting状态，等待显示goready唤醒，在poller中用得较多&lt;/li&gt;&#xA;&lt;li&gt;goready，g置为runnable状态，放入全局队列&lt;/li&gt;&#xA;&lt;li&gt;gosched，g显示调用runtime.Gosched或被抢占，置为runnable状态，放入全局队列&lt;/li&gt;&#xA;&lt;li&gt;goexit，g执行完退出，g所属m切换到g0栈，重新进入schedule&lt;/li&gt;&#xA;&lt;li&gt;g陷入syscall&#xA;&lt;ul&gt;&#xA;&lt;li&gt;net io和部分file io，没有事件则gopark&lt;/li&gt;&#xA;&lt;li&gt;普通的阻塞系统调用，返回时m重新进入schedule&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;g陷入cgocall&#xA;&lt;ul&gt;&#xA;&lt;li&gt;lockedm加上syscall的处理逻辑&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;g执行超过10ms被sysmon抢占&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Go接口详解</title>
      <link>https://feilengcui008.github.io/post/go%E6%8E%A5%E5%8F%A3%E8%AF%A6%E8%A7%A3/</link>
      <pubDate>Sun, 30 Apr 2017 14:45:23 +0800</pubDate>
      <guid>https://feilengcui008.github.io/post/go%E6%8E%A5%E5%8F%A3%E8%AF%A6%E8%A7%A3/</guid>
      <description>&lt;p&gt;Go接口的设计和实现是Go整个类型系统的一大特点。接口嵌入和组合、duck typing等实现了优雅的代码复用、解耦、模块化的特性，而且接口是方法动态分派、反射的实现基础(当然更基础的是编译为运行时提供的类型信息)。理解了接口的实现之后，就不难理解&amp;quot;著名&amp;quot;的nil返回值问题以及反射、type switch、type assertion等原理。本文主要基于Go1.8.1的源码介绍接口的内部实现及其使用相关的问题。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;1-接口的实现&#34;&gt;1. 接口的实现&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;下面是接口在runtime中的实现，注意其中包含了接口本身和实际数据类型的类型信息:&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;// src/runtime/runtime2.go&#xA;type iface struct {&#xA;  // 包含接口的静态类型信息、数据的动态类型信息、函数表&#xA;  tab  *itab&#xA;  // 指向具体数据的内存地址比如slice、map等，或者在接口&#xA;  // 转换时直接存放小数据(一个指针的长度)&#xA;  data unsafe.Pointer&#xA;}&#xA;type itab struct {&#xA;  // 接口的类型信息&#xA;  inter  *interfacetype&#xA;  // 具体数据的类型信息&#xA;  _type  *_type&#xA;  link   *itab&#xA;  hash   uint32&#xA;  bad    bool&#xA;  inhash bool&#xA;  unused [2]byte&#xA;  // 函数地址表，这里放置和接口方法对应的具体数据类型的方法地址&#xA;  // 实现接口调用方法的动态分派，一般在给接口赋值发生转换时候会&#xA;  // 更新此表，或者从直接拿缓存的itab&#xA;  fun    [1]uintptr // variable sized&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;另外，需要注意与接口相关的两点优化，会影响到反射等的实现:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;(1) 空接口(interface{})的itab优化。当将某个类型的值赋给空接口时，由于空接口没有方法，所以空接口的tab会直接指向数据的具体类型。在Go的reflect包中，&lt;code&gt;reflect.TypeOf&lt;/code&gt;和&lt;code&gt;reflect.ValueOf&lt;/code&gt;的参数都是空接口，因此所有参数都会先转换为空接口类型。这样，反射就实现了对所有参数类型获取实际数据类型的统一。这在后面反射的基本实现中会分析到。&lt;/li&gt;&#xA;&lt;li&gt;(2) 发生接口转换时data字段相关的优化。当被转换为接口的数据的类型长度不超过一个指针的长度时(比如pointer、map、func、chan、[1]int等类型)，接口转换时会将数据直接拷贝存放到接口的data字段中(DirectIface)，而不再额外分配内存并拷贝。另外，从go1.8+的源码来看除了DirectIface的优化以外，还对长度较小(不超过64字节，未初始化数据内存的array，空字符串等)的零值做了优化，也不会重新分配内存，而是直接指向一个包级全局数组变量zeroVal的首地址。注意第2点优化发生在接口转换时生成的临时接口上，而不是被赋值的接口左值上。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;再者，在Go中只有值传递，与具体的类型实现无关，但是某些类型具有引用的属性。典型的9种非基础类型中:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;array传递会拷贝整块数据内存，传递长度为len(arr) * Sizeof(elem)&lt;/li&gt;&#xA;&lt;li&gt;string、slice、interface传递的是其runtime的实现，所以长度是固定的，分别为16、24、16字节(amd64)&lt;/li&gt;&#xA;&lt;li&gt;map、func、chan、pointer传递的是指针，所以长度固定为8字节(amd64)&lt;/li&gt;&#xA;&lt;li&gt;struct传递的是所有字段的内存拷贝，所以长度是所有字段的长度和&lt;/li&gt;&#xA;&lt;li&gt;详细的测试可以参考&lt;a href=&#34;https://github.com/feilengcui008/pieces/blob/master/go/basics/pass_by_value_main.go&#34;&gt;这段程序&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;2-runtime中接口的转换操作&#34;&gt;2. runtime中接口的转换操作&lt;/h2&gt;&#xA;&lt;p&gt;接口相关的操作主要在于对其内部字段itab的操作，因为接口转换最重要的是类型信息。这里简单分析几个runtime中相关的函数。主要实现在&lt;code&gt;src/runtime/iface.go&lt;/code&gt;中。值得注意的是，接口的类型转换在编译期会生成一个函数调用的语法树节点(OCALL)，调用runtime提供的相应接口转换函数完成接口的类型设置，所以接口的转换是在运行时发生的，其具体类型的方法地址表也是在运行时填写的，这一点和C++的虚函数表不太一样。另外，由于在运行时转换会产生开销，所以对转换的itab做了缓存。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Go的自举</title>
      <link>https://feilengcui008.github.io/post/go%E7%9A%84%E8%87%AA%E4%B8%BE/</link>
      <pubDate>Thu, 27 Apr 2017 15:37:35 +0800</pubDate>
      <guid>https://feilengcui008.github.io/post/go%E7%9A%84%E8%87%AA%E4%B8%BE/</guid>
      <description>&lt;p&gt;Go从1.5开始就基本全部由.go和.s文件写成了，.c文件被全部重写。了解Go语言的自举是很有意思的事情，能帮助理解Go的编译链接流程、Go的标准库和二进制工具等。本文基于go1.8的源码分析了编译时的自举流程。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;1-基本流程&#34;&gt;1. 基本流程&lt;/h2&gt;&#xA;&lt;p&gt;Go的编译自举流程分为以下几步(假设这里老版本的Go为go_old):&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;go_old -&amp;gt; dist: 用老版本的Go编译出新代码的dist工具&lt;/li&gt;&#xA;&lt;li&gt;go_old + dist -&amp;gt; asm, compile, link: 用老版本的Go和dist工具编译出bootstrap工具，asm用于汇编源码中的.s文件，输出.o对象文件；compile用于编译源码中的.go文件，输出归档打包后的.a文件；link用于链接二进制文件。这里还要依赖外部的pack程序，负责归档打包编译的库。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;到这里，dist/asm/compile/link都是链接的老的runtime，所以其运行依赖于go_old。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;asm, compile, link -&amp;gt; go_bootstrap: 这里用新代码的asm/compile/link的逻辑编译出新的go二进制文件及其依赖的所有包，包括新的runtime。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;go_bootstrap install std cmd: 重新编译所有的标准库和二进制文件，替换之前编译的所有标准库和二进制工具(包括之前编译的dist,asm,link,compile等)，这样标准库和二进制工具依赖的都是新的代码编译生成的runtime，而且是用新的代码本身的编译链接逻辑。(这里go_bootstrap install会使用上一步的asm,compile,link工具实现编译链接，虽然其用的是go_old的runtime，但是这几个工具已经是新代码的编译链接逻辑)。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;一句话总结，借用老的runtime编译新的代码逻辑(编译器、链接器、新的runtime)生成新代码的编译、链接工具，并用这些工具重新编译新代码和工具本身。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;2-具体实现&#34;&gt;2. 具体实现&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;生成dist&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;// make.bash&#xA;# 编译cmd/dist，需要在host os和host arch下编译(dist需要在本地机器运行)，因此这里把环境变量清掉了&#xA;# 注意在bash中，单行的环境变量只影响后面的命令，不会覆盖外部环境变量!!!&#xA;GOROOT=&amp;#34;$GOROOT_BOOTSTRAP&amp;#34; GOOS=&amp;#34;&amp;#34; GOARCH=&amp;#34;&amp;#34; &amp;#34;$GOROOT_BOOTSTRAP/bin/go&amp;#34; build -o cmd/dist/dist ./cmd/dist&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&#xA;&lt;li&gt;生成bootstrap二进制文件和库&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;// make.bash&#xA;# 设置环境变量&#xA;eval $(./cmd/dist/dist env -p || echo FAIL=true)&#xA;&#xA;# 编译cmd/compile, cmd/asm, cmd/link, cmd/go bootstrap工具，注意外部传进来的GOOS和GOARCH目标平台的环境变量&#xA;# 这里可提供GOARCH和GOOS环境变量交叉编译&#xA;./cmd/dist/dist bootstrap $buildall $GO_DISTFLAGS -v # builds go_bootstrap&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&#xA;&lt;li&gt;重新生成当前平台的go&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;// make.bash&#xA;// std, cmd, all在go里有特殊的含义，这里重新编译了所有标准库和默认工具的二进制程序&#xA;if [ &amp;#34;$GOHOSTARCH&amp;#34; != &amp;#34;$GOARCH&amp;#34; -o &amp;#34;$GOHOSTOS&amp;#34; != &amp;#34;$GOOS&amp;#34; ]; then&#xA;  echo &amp;#34;##### Building packages and commands for host, $GOHOSTOS/$GOHOSTARCH.&amp;#34;&#xA;  # 重置GOOS和GOARCH环境变量，不会影响外层的环境变量&#xA;  CC=$CC GOOS=$GOHOSTOS GOARCH=$GOHOSTARCH \&#xA;    &amp;#34;$GOTOOLDIR&amp;#34;/go_bootstrap install -gcflags &amp;#34;$GO_GCFLAGS&amp;#34; -ldflags &amp;#34;$GO_LDFLAGS&amp;#34; -v std cmd&#xA;  echo&#xA;fi&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&#xA;&lt;li&gt;生成目标平台的Go&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;CC=$CC_FOR_TARGET &amp;#34;$GOTOOLDIR&amp;#34;/go_bootstrap install $GO_FLAGS -gcflags &amp;#34;$GO_GCFLAGS&amp;#34; -ldflags &amp;#34;$GO_LDFLAGS&amp;#34; -v std cmd&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&#xA;&lt;li&gt;dist bootstrap逻辑&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;// cmd/dist&#xA;dist的bootstrap逻辑不具体分析了，基本过程是先编译好asm, compile, link工具，然后用它们编译cmd/go及其依赖的runtime和标准库。中间主要是用compile编译.go文件、asm汇编.s文件和用link/pack链接归档打包目标文件的过程。&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;hr&gt;&#xA;&lt;h2 id=&#34;3-小问题&#34;&gt;3. 小问题&lt;/h2&gt;&#xA;&lt;p&gt;分析代码中遇到几个值得注意的小问题:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Go的context包实现分析</title>
      <link>https://feilengcui008.github.io/post/go%E7%9A%84context%E5%8C%85%E5%AE%9E%E7%8E%B0%E5%88%86%E6%9E%90/</link>
      <pubDate>Mon, 24 Apr 2017 21:07:23 +0800</pubDate>
      <guid>https://feilengcui008.github.io/post/go%E7%9A%84context%E5%8C%85%E5%AE%9E%E7%8E%B0%E5%88%86%E6%9E%90/</guid>
      <description>&lt;p&gt;Go1.7引入了context包，并在之后版本的标准库中广泛使用，尤其是net/http包。context包实现了一种优雅的并发安全的链式或树状通知机制，并且带取消、超时、值传递的特性，其底层还是基于channel、goroutine和time.Timer。通常一段应用程序会涉及多个树状的处理逻辑，树的节点之间存在一定依赖关系，比如子节点依赖父节点的完成，如果父节点退出，则子节点需要立即退出，所以这种模型可以比较优雅地处理程序的多个逻辑部分，而context很好地实现了这个模型。对于请求响应的形式(比如http)尤其适合这种模型。下面分析下context包的具体实现。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;1-基本设计&#34;&gt;1. 基本设计&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;context的类型主要有emptyCtx(用于默认Context)、cancelCtx(带cancel的Context)、timerCtx(计时并带cancel的Context)、valueCtx(携带kv键值对)，多种类型可以以父子节点形式相互组合其功能形成新的Context。&lt;/li&gt;&#xA;&lt;li&gt;cancelCtx是最核心的，是WithCancel的底层实现，且可包含多个cancelCtx子节点，从而构成一棵树。&lt;/li&gt;&#xA;&lt;li&gt;emptyCtx目前有两个实例化的ctx: background和TODO，background作为整个运行时的默认ctx，而TODO主要用来临时填充未确定具体Context类型的ctx参数&lt;/li&gt;&#xA;&lt;li&gt;timerCtx借助cancelCtx实现，只是其cancel的调用可由time.Timer的事件回调触发，WithDeadline和WithTimeout的底层实现。&lt;/li&gt;&#xA;&lt;li&gt;cancelCtx的cancel有几种方式&#xA;&lt;ul&gt;&#xA;&lt;li&gt;主动调用cancel&lt;/li&gt;&#xA;&lt;li&gt;其父ctx被cancel，触发子ctx的cancel&lt;/li&gt;&#xA;&lt;li&gt;time.Timer事件触发timerCtx的cancel回调&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;当一个ctx被cancel后，ctx内部的负责通知的channel被关闭，从而触发select此channel的goroutine获得通知，完成相应逻辑的处理&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;2-具体实现&#34;&gt;2. 具体实现&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Context接口&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;type Context interface {&#xA;  // 只用于timerCtx，即WithDeadline和WithTimeout&#xA;  Deadline() (deadline time.Time, ok bool)&#xA;  // 需要获取通知的goroutine可以select此chan，当此ctx被cancel时，会close此chan&#xA;  Done() &amp;lt;-chan struct{}&#xA;  // 错误信息&#xA;  Err() error&#xA;  // 只用于valueCtx&#xA;  Value(key interface{}) interface{}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&#xA;&lt;li&gt;几种主要Context的实现&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;// cancelCtx&#xA;type cancelCtx struct {&#xA;  Context&#xA;  mu       sync.Mutex            &#xA;  done     chan struct{}         &#xA;  // 主要用于存储子cancelCtx和timerCtx&#xA;  // 当此ctx被cancel时，会自动cancel其所有children中的ctx&#xA;  children map[canceler]struct{} &#xA;  err      error                 &#xA;}&#xA;// timeCtx&#xA;type timerCtx struct {&#xA;  cancelCtx&#xA;  // 借助计时器触发timeout事件&#xA;  timer *time.Timer&#xA;  deadline time.Time&#xA;}&#xA;// valueCtx &#xA;type valueCtx struct {&#xA;  Context&#xA;  key, val interface{}&#xA;}&#xA;&#xA;// cancel逻辑&#xA;func (c *cancelCtx) cancel(removeFromParent bool, err error) {&#xA;  /* ... */&#xA;  c.err = err&#xA;  // 如果在第一次调用Done之前就调用cancel，则done为nil&#xA;  if c.done == nil {&#xA;    c.done = closedchan&#xA;  } else {&#xA;    close(c.done)&#xA;  }&#xA;  for child := range c.children {&#xA;    // NOTE: acquiring the child&amp;#39;s lock while holding parent&amp;#39;s lock.&#xA;    // 不能将子ctx从当前移除，由于移除需要拿当前ctx的锁&#xA;    child.cancel(false, err)&#xA;  }&#xA;  // 直接置为nil让gc处理子ctx的回收?&#xA;  c.children = nil&#xA;  c.mu.Unlock()&#xA;&#xA;  // 把自己从parent里移除，注意这里需要拿parent的锁&#xA;  if removeFromParent {&#xA;    removeChild(c.Context, c)&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&#xA;&lt;li&gt;外部接口&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;// Background&#xA;func Background() Context {&#xA;  // 直接返回默认的顶层ctx&#xA;  return background&#xA;}&#xA;&#xA;// WithCancel&#xA;func WithCancel(parent Context) (ctx Context, cancel CancelFunc) {&#xA;  // 实例化cancelCtx&#xA;  c := newCancelCtx(parent)&#xA;  // 如果parent是cancelCtx类型，则注册到parent.children，否则启用&#xA;  // 新的goroutine专门负责此ctx的cancel，当parent被cancel后，自动&#xA;  // 回调child的cancel&#xA;  propagateCancel(parent, &amp;amp;c)&#xA;  return &amp;amp;c, func() { c.cancel(true, Canceled) }&#xA;}&#xA;&#xA;// WithDeadline&#xA;func WithDeadline(parent Context, deadline time.Time) (Context, CancelFunc) {&#xA;  // 如果parent是deadline，且比当前早，则直接返回cancelCtx&#xA;  if cur, ok := parent.Deadline(); ok &amp;amp;&amp;amp; cur.Before(deadline) {&#xA;    return WithCancel(parent)&#xA;  }&#xA;  c := &amp;amp;timerCtx{&#xA;    cancelCtx: newCancelCtx(parent),&#xA;    deadline:  deadline,&#xA;  }&#xA;  propagateCancel(parent, c)&#xA;  d := time.Until(deadline)&#xA;  // 已经过了&#xA;  if d &amp;lt;= 0 {&#xA;    c.cancel(true, DeadlineExceeded) // deadline has already passed&#xA;    return c, func() { c.cancel(true, Canceled) }&#xA;  }&#xA;  c.mu.Lock()&#xA;  defer c.mu.Unlock()&#xA;  if c.err == nil {&#xA;    // time.Timer到时则自动回调cancel&#xA;    c.timer = time.AfterFunc(d, func() {&#xA;      c.cancel(true, DeadlineExceeded)&#xA;    })&#xA;  }&#xA;  return c, func() { c.cancel(true, Canceled) }&#xA;}&#xA;&#xA;// WithTimeout&#xA;func WithTimeout(parent Context, timeout time.Duration) (Context, CancelFunc) {&#xA;  // 直接使用WithDeadline的实现即可&#xA;  return WithDeadline(parent, time.Now().Add(timeout))&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;hr&gt;&#xA;&lt;h2 id=&#34;3-简单例子&#34;&gt;3. 简单例子&lt;/h2&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;package main&#xA;&#xA;import (&#xA;  &amp;#34;context&amp;#34;&#xA;  &amp;#34;fmt&amp;#34;&#xA;  &amp;#34;time&amp;#34;&#xA;)&#xA;&#xA;func OuterLogicWithContext(ctx context.Context, fn func(ctx context.Context) error) error {&#xA;  go fn(ctx)&#xA;  for {&#xA;    select {&#xA;    case &amp;lt;-ctx.Done():&#xA;      fmt.Println(&amp;#34;OuterLogicWithContext ended&amp;#34;)&#xA;      return ctx.Err()&#xA;    }&#xA;  }&#xA;}&#xA;&#xA;func InnerLogicWithContext(ctx context.Context) error {&#xA;Loop:&#xA;  for {&#xA;    select {&#xA;    case &amp;lt;-ctx.Done():&#xA;      break Loop&#xA;    }&#xA;  }&#xA;  fmt.Println(&amp;#34;InnerLogicWithContext ended&amp;#34;)&#xA;  return ctx.Err()&#xA;}&#xA;&#xA;func main() {&#xA;  ctx := context.Background()&#xA;  var cancel context.CancelFunc&#xA;  ctx, cancel = context.WithCancel(ctx)&#xA;  ctx, cancel = context.WithTimeout(ctx, time.Second)&#xA;  go OuterLogicWithContext(ctx, InnerLogicWithContext)&#xA;  time.Sleep(time.Second * 3)&#xA;  // has been canceled by timer&#xA;  cancel()&#xA;  fmt.Println(&amp;#34;main ended&amp;#34;)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    <item>
      <title>gRPC-Go客户端源码分析</title>
      <link>https://feilengcui008.github.io/post/grpc-go%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</link>
      <pubDate>Mon, 24 Apr 2017 15:33:49 +0800</pubDate>
      <guid>https://feilengcui008.github.io/post/grpc-go%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</guid>
      <description>&lt;h2 id=&#34;基本设计&#34;&gt;基本设计&lt;/h2&gt;&#xA;&lt;p&gt;gRPC-Go客户端的逻辑相对比较简单，从前面服务端的逻辑我们知道，客户端会通过http2复用tcp连接，每一次请求的调用基本上就是在已经建立好的tcp连接(并用ClientTransport抽象)上发送http请求，通过帧和流与服务端交互数据。&lt;/p&gt;&#xA;&lt;p&gt;另外，一个服务对应的具体地址可能有多个，grpc在这里抽象了负载均衡的接口和部分实现。grpc提供两种负载均衡方式，一种是客户端内部自带的策略实现(目前只实现了轮询方式)，另一种方式是外部的load balancer。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;内部自带的策略实现: 这种方式主要针对一些简单的负载均衡策略比如轮询。轮询的实现逻辑是建立连接时通过定义的服务地址解析接口Resolver得到服务的地址列表，并单独用goroutine负责更新保持可用的连接，Watcher定义了具体更新实现的接口(比如多长时间解析更新一次)，最终在请求调用时会从可用连接列表中轮询选择其中一个连接发送请求。所以，grpc的负载均衡策略是请求级别的而不是连接级别的。&lt;/li&gt;&#xA;&lt;li&gt;外部load balancer：这种方式主要针对 较复杂的负载均衡策略。grpclb实现了grpc这边的逻辑，并用protobuf定义了与load balancer交互的接口。gRPC-Go客户端建立连接时，会先与load balancer建立连接，并使用和轮询方式类似的Resolver、Watcher接口来更新load balancer的可用连接列表，不同的是每次load balancer连接变化时，会像load balancer地址发送rpc请求得到服务的地址列表。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;客户端主要流程&#34;&gt;客户端主要流程&lt;/h2&gt;&#xA;&lt;p&gt;客户端的逻辑主要可分为下面两部分:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;建立连接&lt;/li&gt;&#xA;&lt;li&gt;请求调用、发送与响应&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;1-建立连接&#34;&gt;1. 建立连接&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;典型的步骤&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;func main() {&#xA;  // 建立连接&#xA;  conn, err := grpc.Dial(address, grpc.WithInsecure())&#xA;  c := pb.NewGreeterClient(conn)&#xA;  // 请求调用&#xA;  r, err := c.SayHello(context.Background(), &amp;amp;pb.HelloRequest{Name: name})&#xA;  // 处理返回r&#xA;  // 对于单次请求，grpc直接负责返回响应数据&#xA;  // 对于流式请求，grpc会返回一个流的封装，由开发者负责流中数据的读写&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&#xA;&lt;li&gt;建立tcp(http2)连接&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;func Dial(target string, opts ...DialOption) (*ClientConn, error) {&#xA;  return DialContext(context.Background(), target, opts...)&#xA;}&#xA;func DialContext(ctx context.Context, target string, opts ...DialOption) (conn *ClientConn, err error) {&#xA;  cc := &amp;amp;ClientConn{&#xA;    target: target,&#xA;    conns:  make(map[Address]*addrConn),&#xA;  }&#xA;  /* ... */&#xA;&#xA;  // 底层dialer，负责解析地址和建立tcp连接&#xA;  if cc.dopts.copts.Dialer == nil {&#xA;    cc.dopts.copts.Dialer = newProxyDialer(&#xA;      func(ctx context.Context, addr string) (net.Conn, error) {&#xA;        return dialContext(ctx, &amp;#34;tcp&amp;#34;, addr)&#xA;      },&#xA;    )&#xA;  }&#xA;  /* ... */&#xA;&#xA;  if cc.dopts.scChan != nil {&#xA;    // Wait for the initial service config.&#xA;    select {&#xA;    case sc, ok := &amp;lt;-cc.dopts.scChan:&#xA;      if ok {&#xA;        cc.sc = sc&#xA;      }&#xA;    case &amp;lt;-ctx.Done():&#xA;      return nil, ctx.Err()&#xA;    }&#xA;  }&#xA;  /* ... */&#xA;&#xA;  // 建立连接，如果设置了负载均衡，则通过负载均衡器建立连接&#xA;  // 否则直接连接&#xA;  waitC := make(chan error, 1)&#xA;  go func() {&#xA;    defer close(waitC)&#xA;    if cc.dopts.balancer == nil &amp;amp;&amp;amp; cc.sc.LB != nil {&#xA;      cc.dopts.balancer = cc.sc.LB&#xA;    }&#xA;    if cc.dopts.balancer != nil {&#xA;      var credsClone credentials.TransportCredentials&#xA;      if creds != nil {&#xA;        credsClone = creds.Clone()&#xA;      }&#xA;      config := BalancerConfig{&#xA;        DialCreds: credsClone,&#xA;      }&#xA;      // 负载均衡，可能是grcp-client内部的简单轮训负载均衡或者是外部的load balancer&#xA;      // 如果是外部的load balancer，这里的target是load balancer的服务名&#xA;      // grpclb会解析load balancer地址，建立rpc连接，得到服务地址列表，并通知Notify chan&#xA;      if err := cc.dopts.balancer.Start(target, config); err != nil {&#xA;        waitC &amp;lt;- err&#xA;        return&#xA;      }&#xA;      // 更新后地址的发送channel&#xA;      ch := cc.dopts.balancer.Notify()&#xA;      if ch != nil {&#xA;        if cc.dopts.block {&#xA;          doneChan := make(chan struct{})&#xA;          // lbWatcher负责接收负载均衡器的地址更新，从而更新连接&#xA;          go cc.lbWatcher(doneChan)&#xA;          &amp;lt;-doneChan&#xA;        } else {&#xA;          go cc.lbWatcher(nil)&#xA;        }&#xA;        return&#xA;      }&#xA;    }&#xA;    // 直接建立连接&#xA;    if err := cc.resetAddrConn(Address{Addr: target}, cc.dopts.block, nil); err != nil {&#xA;      waitC &amp;lt;- err&#xA;      return&#xA;    }&#xA;  }()&#xA;  /* ... */&#xA;  if cc.dopts.scChan != nil {&#xA;    go cc.scWatcher()&#xA;  }&#xA;&#xA;  return cc, nil&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&#xA;&lt;li&gt;内部负载均衡策略(轮询)，解析域名，并更新地址列表，写到Notify通知channel，由grpc的lbWatcher负责更新对应的服务连接列表&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;func (rr *roundRobin) Start(target string, config BalancerConfig) error {&#xA;  /* ... */&#xA;  // 服务名解析，具体实现可以DNS或者基于etcd的服务发现等，每次解析会返回一个watcher&#xA;  // watcher具体服务解析请求的周期等&#xA;  w, err := rr.r.Resolve(target)&#xA;  if err != nil {&#xA;    return err&#xA;  }&#xA;  rr.w = w&#xA;  rr.addrCh = make(chan []Address)&#xA;  go func() {&#xA;    // 循环，不断解析服务的地址，更新对应的地址列表&#xA;    for {&#xA;      if err := rr.watchAddrUpdates(); err != nil {&#xA;        return&#xA;      }&#xA;    }&#xA;  }()&#xA;  return nil&#xA;}&#xA;&#xA;func (rr *roundRobin) watchAddrUpdates() error {&#xA;  // 阻塞得到需要更新的地址列表，注意在naming里面的Resolver和Watcher&#xA;  // 定义了服务解析的接口，可以使用简单的dns解析实现、consul/etcd等服务发现&#xA;  // 以及其他形式，只要能返回对应的服务地址列表即可，Resolver里边缓存已经解析&#xA;  // 过的服务，并有单独的goroutine与后端服务通信更新，这样不用每次都解析地址&#xA;  updates, err := rr.w.Next()&#xA;  // 解析后，更新对应服务的地址列表，在内部做轮训负载均衡&#xA;  for _, update := range updates {&#xA;    addr := Address{&#xA;      Addr:     update.Addr,&#xA;      Metadata: update.Metadata,&#xA;    }&#xA;    switch update.Op {&#xA;    // 添加新地址&#xA;    case naming.Add:&#xA;      var exist bool&#xA;      for _, v := range rr.addrs {&#xA;        if addr == v.addr {&#xA;          exist = true&#xA;          grpclog.Println(&amp;#34;grpc: The name resolver wanted to add an existing address: &amp;#34;, addr)&#xA;          break&#xA;        }&#xA;      }&#xA;      if exist {&#xA;        continue&#xA;      }&#xA;      rr.addrs = append(rr.addrs, &amp;amp;addrInfo{addr: addr})&#xA;      // 删除&#xA;    case naming.Delete:&#xA;      for i, v := range rr.addrs {&#xA;        if addr == v.addr {&#xA;          copy(rr.addrs[i:], rr.addrs[i+1:])&#xA;          rr.addrs = rr.addrs[:len(rr.addrs)-1]&#xA;          break&#xA;        }&#xA;      }&#xA;  }&#xA;  open := make([]Address, len(rr.addrs))&#xA;  for i, v := range rr.addrs {&#xA;    open[i] = v.addr&#xA;  }&#xA;  // 通知lbWatcher&#xA;  rr.addrCh &amp;lt;- open&#xA;  return nil&#xA;}&#xA;&#xA;// 轮询得到一个可用连接&#xA;func (rr *roundRobin) Get(ctx context.Context, opts BalancerGetOptions) (addr Address, put func(), err error) {&#xA;  /* ... */&#xA;  if len(rr.addrs) &amp;gt; 0 {&#xA;    if rr.next &amp;gt;= len(rr.addrs) {&#xA;      rr.next = 0&#xA;    }&#xA;    next := rr.next&#xA;    for {&#xA;      // 找到下一个，赋予返回值&#xA;      a := rr.addrs[next]&#xA;      next = (next + 1) % len(rr.addrs)&#xA;      if a.connected {&#xA;        addr = a.addr&#xA;        rr.next = next&#xA;        rr.mu.Unlock()&#xA;        return&#xA;      }&#xA;      if next == rr.next {&#xA;        // Has iterated all the possible address but none is connected.&#xA;        break&#xA;      }&#xA;    }&#xA;  }&#xA;  /* ... */&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&#xA;&lt;li&gt;外部负载均衡&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;// 对于外部负载均衡，Start负责解析负载均衡器的地址列表&#xA;func (b *balancer) Start(target string, config grpc.BalancerConfig) error {&#xA;  /* ... */&#xA;  // 解析，返回watcher&#xA;  w, err := b.r.Resolve(target)&#xA;  b.w = w&#xA;  b.mu.Unlock()&#xA;  balancerAddrsCh := make(chan []remoteBalancerInfo, 1)&#xA;  go func() {&#xA;    for {&#xA;      // 一直循环解析load balancer的地址，一旦有更新则通知&#xA;      if err := b.watchAddrUpdates(w, balancerAddrsCh); err != nil {&#xA;        /* ... */&#xA;      }&#xA;    }&#xA;  }()&#xA;  go func() {&#xA;    var (&#xA;      cc *grpc.ClientConn&#xA;      // ccError is closed when there is an error in the current cc.&#xA;      // A new rb should be picked from rbs and connected.&#xA;      ccError chan struct{}&#xA;      rb      *remoteBalancerInfo&#xA;      rbs     []remoteBalancerInfo&#xA;      rbIdx   int&#xA;    )&#xA;    /* ... */&#xA;    for {&#xA;      var ok bool&#xA;      select {&#xA;      // 从channel中读取load balancer的列表&#xA;      case rbs, ok = &amp;lt;-balancerAddrsCh:&#xA;        /* ... */&#xA;      }&#xA;      /* ... */&#xA;      // 连接load balancer&#xA;      if creds == nil {&#xA;        cc, err = grpc.Dial(rb.addr, grpc.WithInsecure())&#xA;      } else {&#xA;        /* ... */&#xA;        cc, err = grpc.Dial(rb.addr, grpc.WithTransportCredentials(creds))&#xA;      }&#xA;      b.mu.Lock()&#xA;      b.seq++ // tick when getting a new balancer address&#xA;      seq := b.seq&#xA;      b.next = 0&#xA;      b.mu.Unlock()&#xA;      // 对于每个load balancer的地址变化，获取新的服务地址列表，并通知lbWatcher更新&#xA;      go func(cc *grpc.ClientConn, ccError chan struct{}) {&#xA;        // load balancer client&#xA;        lbc := lbpb.NewLoadBalancerClient(cc)&#xA;        // 得到server list，并写入addrChan这个Notify channel&#xA;        b.callRemoteBalancer(lbc, seq)&#xA;        cc.Close()&#xA;        select {&#xA;        case &amp;lt;-ccError:&#xA;        default:&#xA;          close(ccError)&#xA;        }&#xA;      }(cc, ccError)&#xA;    }&#xA;  }()&#xA;  return nil&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;2-请求调用发送与响应&#34;&gt;2. 请求调用、发送与响应&lt;/h3&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;// 单次请求，grpc负责invoke对应的服务方法，并直接返回数据&#xA;func (c *greeterClient) SayHello(ctx context.Context, in *HelloRequest, opts ...grpc.CallOption) (*HelloReply, error) {&#xA;  out := new(HelloReply)&#xA;  err := grpc.Invoke(ctx, &amp;#34;/helloworld.Greeter/SayHello&amp;#34;, in, out, c.cc, opts...)&#xA;  if err != nil {&#xA;    return nil, err&#xA;  }&#xA;  return out, nil&#xA;}&#xA;&#xA;// 流式请求，grpc返回对应的流&#xA;func (c *routeGuideClient) ListFeatures(ctx context.Context, in *Rectangle, opts ...grpc.CallOption) (RouteGuide_ListFeaturesClient, error) {&#xA;  stream, err := grpc.NewClientStream(ctx, &amp;amp;_RouteGuide_serviceDesc.Streams[0], c.cc, &amp;#34;/routeguide.RouteGuide/ListFeatures&amp;#34;, opts...)&#xA;  if err != nil {&#xA;    return nil, err&#xA;  }&#xA;  x := &amp;amp;routeGuideListFeaturesClient{stream}&#xA;  if err := x.ClientStream.SendMsg(in); err != nil {&#xA;    return nil, err&#xA;  }&#xA;  if err := x.ClientStream.CloseSend(); err != nil {&#xA;    return nil, err&#xA;  }&#xA;  return x, nil&#xA;}&#xA;&#xA;// 单次请求调用实现，响应返回时客户端会关闭流，而流式请求会直接将流封装后交给上层开发者，由开发者处理&#xA;func invoke(ctx context.Context, method string, args, reply interface{}, cc *ClientConn, opts ...CallOption) (e error) {&#xA;  /* ... */&#xA;  for {&#xA;    var (&#xA;      err    error&#xA;      t      transport.ClientTransport&#xA;      stream *transport.Stream&#xA;      // Record the put handler from Balancer.Get(...). It is called once the&#xA;      // RPC has completed or failed.&#xA;      put func()&#xA;    )&#xA;    /* ... */&#xA;    // 得到一个tcp连接(ClientTransport)&#xA;    t, put, err = cc.getTransport(ctx, gopts)&#xA;    /* ... */&#xA;    // 发送请求，打开新的流，序列化压缩请求数据，写入流&#xA;    stream, err = sendRequest(ctx, cc.dopts, cc.dopts.cp, callHdr, t, args, topts)&#xA;    /* ... */&#xA;    // 接收响应，解压反序列化响应，并写入reply&#xA;    err = recvResponse(ctx, cc.dopts, t, &amp;amp;c, stream, reply)&#xA;    /* ... */&#xA;    // 关闭流&#xA;    t.CloseStream(stream, nil)&#xA;    if put != nil {&#xA;      put()&#xA;      put = nil&#xA;    }&#xA;    return stream.Status().Err()&#xA;  }&#xA;}&#xA;&#xA;// 发送请求，打开一个新的流&#xA;func sendRequest(ctx context.Context, dopts dialOptions, compressor Compressor, callHdr *transport.CallHdr, t transport.ClientTransport, args interface{}, opts *transport.Options) (_ *transport.Stream, err error) {&#xA;  // 在此连接上打开新的流&#xA;  stream, err := t.NewStream(ctx, callHdr)&#xA;  if err != nil {&#xA;    return nil, err&#xA;  }&#xA;  /* ... */&#xA;  // 序列化压缩数据&#xA;  outBuf, err := encode(dopts.codec, args, compressor, cbuf, outPayload)&#xA;  // 写入流&#xA;  err = t.Write(stream, outBuf, opts)&#xA;  /* ... */&#xA;  // Sent successfully.&#xA;  return stream, nil&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;hr&gt;&#xA;&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;&#xA;&lt;p&gt;至此，gRPC-Go的客户端逻辑主体部分分析完了，其中比较重要的是:&lt;/p&gt;</description>
    </item>
    <item>
      <title>gRPC-Go服务端源码分析</title>
      <link>https://feilengcui008.github.io/post/grpc-go%E6%9C%8D%E5%8A%A1%E7%AB%AF%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</link>
      <pubDate>Sun, 23 Apr 2017 15:47:59 +0800</pubDate>
      <guid>https://feilengcui008.github.io/post/grpc-go%E6%9C%8D%E5%8A%A1%E7%AB%AF%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</guid>
      <description>&lt;h2 id=&#34;基本设计&#34;&gt;基本设计&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;服务抽象&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;一个Server可包含多个Service，每个Service包含多个业务逻辑方法，应用开发者需要:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;不使用protobuf&#xA;&lt;ul&gt;&#xA;&lt;li&gt;规定Service需要实现的接口&lt;/li&gt;&#xA;&lt;li&gt;实现此Service对应的ServiceDesc，ServiceDesc描述了服务名、处理此服务的接口类型、单次调用的方法数组、流式方法数组、其他元数据。&lt;/li&gt;&#xA;&lt;li&gt;实现Service接口具体业务逻辑的结构体&lt;/li&gt;&#xA;&lt;li&gt;实例化Server，并讲ServiceDesc和Service具体实现注册到Server&lt;/li&gt;&#xA;&lt;li&gt;监听并启动Server服务&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;使用protobuf&#xA;&lt;ul&gt;&#xA;&lt;li&gt;实现protobuf grpc插件生成的Service接口&lt;/li&gt;&#xA;&lt;li&gt;实例化Server，并注册Service接口的具体实现&lt;/li&gt;&#xA;&lt;li&gt;监听并启动Server&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;可见，protobuf的gRPC-Go插件帮助我们生成了Service的接口和ServiceDesc。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;底层传输协议&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;gRPC-Go使用http2作为应用层的传输协议，http2会复用底层tcp连接，以流和数据帧的形式处理上层协议，gRPC-Go使用http2的主要逻辑有下面几点，关于http2详细的细节可参考&lt;a href=&#34;http://http2.github.io/&#34;&gt;http2的规范&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;http2帧分为几大类，gRPC-Go使用中比较重要的是HEADERS和DATA帧类型。&#xA;&lt;ul&gt;&#xA;&lt;li&gt;HEADERS帧在打开一个新的流时使用，通常是客户端的一个http请求，gRPC-Go通过底层的go的http2实现帧的读写，并解析出客户端的请求头(大多是grpc内部自己定义的)，读取请求体的数据，grpc规定请求体的数据由两部分构成(5 byte + len(msg)), 其中第1字节表明是否压缩，第2-5个字节消息体的长度(最大2^32即4G)，msg为客户端请求序列化后的原始数据。&lt;/li&gt;&#xA;&lt;li&gt;数据帧从属于某个stream，按照stream id查找，并写入对应的stream中。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Server端接收到客户端建立的连接后，使用一个goroutine专门处理此客户端的连接(即一个tcp连接或者说一个http2连接)，所以同一个grpc客户端连接上服务端后，后续的请求都是通过同一个tcp连接。&lt;/li&gt;&#xA;&lt;li&gt;客户端和服务端的连接在应用层由Transport抽象(类似通常多路复用实现中的封装的channel)，在客户端是ClientTransport，在服务端是ServerTransport。Server端接收到一个客户端的http2请求后即打开一个新的流，ClientTransport和ServerTransport之间使用这个新打开的流以http2帧的形式交换数据。&lt;/li&gt;&#xA;&lt;li&gt;客户端的每个http2请求会打开一个新的流。流可以从两边关闭，对于单次请求来说，客户端会主动关闭流，对于流式请求客户端不会主动关闭(即使使用了CloseSend也只是发送了数据发送结束的标识，还是由服务端关闭)。&lt;/li&gt;&#xA;&lt;li&gt;gRPC-Go中的单次方法和流式方法&#xA;&lt;ul&gt;&#xA;&lt;li&gt;无论是单次方法还是流式方法，服务端在调用完用户的处理逻辑函数返回后，都会关闭流(这也是为什么ServerStream不需要实现CloseSend的原因)。区别只是对于服务端的流式方法来说，可循环多次读取这个流中的帧数据并处理，以此&amp;quot;复用&amp;quot;这个流。&lt;/li&gt;&#xA;&lt;li&gt;客户端如果是流式方法，需要显示调用CloseSend，表示数据发送的结束&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;服务端主要流程&#34;&gt;服务端主要流程&lt;/h2&gt;&#xA;&lt;p&gt;由于比较多，所以分以下几个部分解读主要逻辑:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;实例化Server&lt;/li&gt;&#xA;&lt;li&gt;注册Service&lt;/li&gt;&#xA;&lt;li&gt;监听并接收连接请求&lt;/li&gt;&#xA;&lt;li&gt;连接与请求处理&lt;/li&gt;&#xA;&lt;li&gt;连接的处理细节(http2连接的建立)&lt;/li&gt;&#xA;&lt;li&gt;新请求的处理细节(新流的打开和帧数据的处理)&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;实例化Server&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;// 工厂方法&#xA;func NewServer(opt ...ServerOption) *Server {&#xA;  var opts options&#xA;  // 默认最大消息长度: 4M&#xA;  opts.maxMsgSize = defaultMaxMsgSize&#xA;  // 设置定制的参数&#xA;  for _, o := range opt {&#xA;    o(&amp;amp;opts)&#xA;  }&#xA;  // 默认编解码方式为protobuf&#xA;  if opts.codec == nil {&#xA;    // Set the default codec.&#xA;    opts.codec = protoCodec{}&#xA;  }&#xA;  // 实例化Server&#xA;  s := &amp;amp;Server{&#xA;    lis:   make(map[net.Listener]bool),&#xA;    opts:  opts,&#xA;    conns: make(map[io.Closer]bool),&#xA;    m:     make(map[string]*service),&#xA;  }&#xA;  s.cv = sync.NewCond(&amp;amp;s.mu)&#xA;  s.ctx, s.cancel = context.WithCancel(context.Background())&#xA;  if EnableTracing {&#xA;    _, file, line, _ := runtime.Caller(1)&#xA;    s.events = trace.NewEventLog(&amp;#34;grpc.Server&amp;#34;, fmt.Sprintf(&amp;#34;%s:%d&amp;#34;, file, line))&#xA;  }&#xA;  return s&#xA;}&#xA;&#xA;// Server结构体&#xA;// 一个Server结构代表对外服务的单元，每个Server可以注册&#xA;// 多个Service，每个Service可以有多个方法，主程序需要&#xA;// 实例化Server，注册Service，然后调用s.Serve(l)&#xA;type Server struct {&#xA;  opts options&#xA;  mu sync.Mutex // guards following&#xA;  // 监听地址列表&#xA;  lis map[net.Listener]bool&#xA;  // 客户端的连接&#xA;  conns map[io.Closer]bool&#xA;  drain bool&#xA;  // 上下文&#xA;  ctx    context.Context&#xA;  cancel context.CancelFunc&#xA;  // A CondVar to let GracefulStop() blocks until all the pending RPCs are finished&#xA;  // and all the transport goes away.&#xA;  // 优雅退出时，会等待在此信号，直到所有的RPC都处理完了，并且所有&#xA;  // 的传输层断开&#xA;  cv *sync.Cond&#xA;  // 服务名: 服务&#xA;  m map[string]*service // service name -&amp;gt; service info&#xA;  // 事件追踪&#xA;  events trace.EventLog&#xA;}&#xA;&#xA;// Server配置项&#xA;// Server可设置的选项&#xA;type options struct {&#xA;  // 加密信息， 目前实现了TLS&#xA;  creds credentials.TransportCredentials&#xA;  // 数据编解码，目前实现了protobuf，并用缓存池sync.Pool优化&#xA;  codec Codec&#xA;  // 数据压缩，目前实现了gzip&#xA;  cp Compressor&#xA;  // 数据解压，目前实现了gzip&#xA;  dc Decompressor&#xA;  // 最大消息长度&#xA;  maxMsgSize int&#xA;  // 单次请求的拦截器&#xA;  unaryInt UnaryServerInterceptor&#xA;  // 流式请求的拦截器&#xA;  streamInt   StreamServerInterceptor&#xA;  inTapHandle tap.ServerInHandle&#xA;  // 统计&#xA;  statsHandler stats.Handler&#xA;  // 最大并发流数量，http2协议规范&#xA;  maxConcurrentStreams uint32&#xA;  useHandlerImpl       bool // use http.Handler-based server&#xA;  unknownStreamDesc    *StreamDesc&#xA;  // server端的keepalive参数，会由单独的gorotine负责探测客户端连接的活性&#xA;  keepaliveParams keepalive.ServerParameters&#xA;  keepalivePolicy keepalive.EnforcementPolicy&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&#xA;&lt;li&gt;注册Service&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;// 注册service: sd接口，ss实现&#xA;// 如果使用protobuf的gRPC-Go插件，则会生成sd接口&#xA;func (s *Server) RegisterService(sd *ServiceDesc, ss interface{}) {&#xA;  // 检查ss是否实现sd定义的服务方法接口&#xA;  ht := reflect.TypeOf(sd.HandlerType).Elem()&#xA;  st := reflect.TypeOf(ss)&#xA;  if !st.Implements(ht) {&#xA;    grpclog.Fatalf(&amp;#34;grpc: Server.RegisterService found the handler of type %v that does not satisfy %v&amp;#34;, st, ht)&#xA;  }&#xA;  s.register(sd, ss)&#xA;}&#xA;&#xA;func (s *Server) register(sd *ServiceDesc, ss interface{}) {&#xA;  /* ... */&#xA;  // 检查是否已注册&#xA;  if _, ok := s.m[sd.ServiceName]; ok {&#xA;    grpclog.Fatalf(&amp;#34;grpc: Server.RegisterService found duplicate service registration for %q&amp;#34;, sd.ServiceName)&#xA;  }&#xA;  // 实例化一个服务&#xA;  srv := &amp;amp;service{&#xA;    // 具体实现&#xA;    server: ss,&#xA;    // 单次方法信息&#xA;    md:    make(map[string]*MethodDesc),&#xA;    // 流式方法信息&#xA;    sd:    make(map[string]*StreamDesc),&#xA;    mdata: sd.Metadata,&#xA;  }&#xA;  for i := range sd.Methods {&#xA;    d := &amp;amp;sd.Methods[i]&#xA;    srv.md[d.MethodName] = d&#xA;  }&#xA;  for i := range sd.Streams {&#xA;    d := &amp;amp;sd.Streams[i]&#xA;    srv.sd[d.StreamName] = d&#xA;  }&#xA;  // 注册服务到server&#xA;  s.m[sd.ServiceName] = srv&#xA;}&#xA;&#xA;// 一个由protobuf grcp-go插件生成的sd例子&#xA;var _Greeter_serviceDesc = grpc.ServiceDesc{&#xA;  // 服务名&#xA;  ServiceName: &amp;#34;app.Greeter&amp;#34;,&#xA;  // 此服务的处理类型(通常为实现某服务接口的具体实现结构体)&#xA;  HandlerType: (*GreeterServer)(nil),&#xA;  // 单次方法&#xA;  Methods: []grpc.MethodDesc{&#xA;    {&#xA;      // 方法名&#xA;      MethodName: &amp;#34;SayHello&amp;#34;,&#xA;      // 最终调用的对应/service/method的方法&#xA;      Handler:    _Greeter_SayHello_Handler,&#xA;    },&#xA;  },&#xA;  Streams:  []grpc.StreamDesc{},&#xA;  Metadata: &amp;#34;app.proto&amp;#34;,&#xA;}&#xA;&#xA;// 要注意的是protobuf的gRPC-Go插件为我们生成的MethodDesc中的Handler&#xA;// 对于单次方法和流式方法区别较大，单次方法的参数传入和返回的是单一的请求&#xA;// 和返回对象，而流式方法传入的是底层流的封装ClientStream、ServerStream&#xA;// 因此流式方法可多次读写流。&#xA;// 单次方法的一个例子&#xA;func _Greeter_SayHello_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {&#xA;  in := new(HelloRequest)&#xA;  // 注意这个dec方法参数，负责反序列化，解压&#xA;  if err := dec(in); err != nil {&#xA;    return nil, err&#xA;  }&#xA;  if interceptor == nil {&#xA;    return srv.(GreeterServer).SayHello(ctx, in)&#xA;  }&#xA;  /* ... */&#xA;}&#xA;// 流式方法的一个例子(假设是客户端可流式发送)&#xA;func _Greeter_SayHello_Handler(srv interface{}, stream grpc.ServerStream) error {&#xA;  // 这里应该由业务逻辑实现的SayHello处理流式读取处理的逻辑&#xA;  return srv.(GreeterServer).SayHello(&amp;amp;greeterSayHelloServer{stream})&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&#xA;&lt;li&gt;监听并接收连接请求&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;func (s *Server) Serve(lis net.Listener) error {&#xA;  /* ... */&#xA;  var tempDelay time.Duration // how long to sleep on accept failure&#xA;  // 循环处理连接，每个连接使用一个goroutine处理&#xA;  // accept如果失败，则下次accept之前睡眠一段时间&#xA;  for {&#xA;    rawConn, err := lis.Accept()&#xA;    if err != nil {&#xA;      if ne, ok := err.(interface {&#xA;        Temporary() bool&#xA;      }); ok &amp;amp;&amp;amp; ne.Temporary() {&#xA;        if tempDelay == 0 {&#xA;          // 初始5ms&#xA;          tempDelay = 5 * time.Millisecond&#xA;        } else {&#xA;          // 否则翻倍&#xA;          tempDelay *= 2&#xA;        }&#xA;        // 不超过1s&#xA;        if max := 1 * time.Second; tempDelay &amp;gt; max {&#xA;          tempDelay = max&#xA;        }&#xA;  d     /* ... */&#xA;        // 等待超时重试，或者context事件的发生&#xA;        select {&#xA;        case &amp;lt;-time.After(tempDelay):&#xA;        case &amp;lt;-s.ctx.Done():&#xA;        }&#xA;        continue&#xA;      }&#xA;      /* ... */&#xA;    }&#xA;    // 重置延时&#xA;    tempDelay = 0&#xA;    // Start a new goroutine to deal with rawConn&#xA;    // so we don&amp;#39;t stall this Accept loop goroutine.&#xA;    // 每个新的tcp连接使用单独的goroutine处理&#xA;    go s.handleRawConn(rawConn)&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&#xA;&lt;li&gt;连接与请求处理&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;func (s *Server) handleRawConn(rawConn net.Conn) {&#xA;  // 是否加密&#xA;  conn, authInfo, err := s.useTransportAuthenticator(rawConn)&#xA;  /* ... */&#xA;  s.mu.Lock()&#xA;  // 如果此goroutine处于处理连接中时，server被关闭，则直接关闭连接返回&#xA;  if s.conns == nil {&#xA;    s.mu.Unlock()&#xA;    conn.Close()&#xA;    return&#xA;  }&#xA;  s.mu.Unlock()&#xA;&#xA;  if s.opts.useHandlerImpl {&#xA;    // 测试时使用&#xA;    s.serveUsingHandler(conn)&#xA;  } else {&#xA;    // 处理http2连接的建立，http2连接的建立也需要客户端和&#xA;    // 服务端交换，即http2 Connection Preface，所以后面&#xA;    // 的宏观逻辑是，先处理http2连接建立过程中的帧数据信息，&#xA;    // 然后一直循环处理新的流的建立(即新的http2请求的到达)&#xA;    // 和帧的数据收发。&#xA;    s.serveHTTP2Transport(conn, authInfo)&#xA;  }&#xA;}&#xA;&#xA;// 每个http2连接在服务端会生成一个ServerTransport，这里是 htt2server&#xA;func (s *Server) serveHTTP2Transport(c net.Conn, authInfo credentials.AuthInfo) {&#xA;  config := &amp;amp;transport.ServerConfig{&#xA;    MaxStreams:      s.opts.maxConcurrentStreams,&#xA;    AuthInfo:        authInfo,&#xA;    InTapHandle:     s.opts.inTapHandle,&#xA;    StatsHandler:    s.opts.statsHandler,&#xA;    KeepaliveParams: s.opts.keepaliveParams,&#xA;    KeepalivePolicy: s.opts.keepalivePolicy,&#xA;  }&#xA;  // 返回实现了ServerTransport接口的http2server&#xA;  // 接口规定了HandleStream, Write等方法&#xA;  st, err := transport.NewServerTransport(&amp;#34;http2&amp;#34;, c, config)&#xA;  /* ... */&#xA;  // 加入每个连接的ServerTransport&#xA;  if !s.addConn(st) {&#xA;    // 出错关闭Transport，即关闭客户端的net.Conn&#xA;    st.Close()&#xA;    return&#xA;  }&#xA;  // 开始处理连接Transport，处理新的帧数据和流的打开&#xA;  s.serveStreams(st)&#xA;}&#xA;&#xA;// 新建ServerTransport&#xA;func newHTTP2Server(conn net.Conn, config *ServerConfig) (_ ServerTransport, err error) {&#xA;  // 封装帧的读取，底层使用的是http2.frame&#xA;  framer := newFramer(conn)&#xA;  // 初始的配置帧&#xA;  // Send initial settings as connection preface to client.&#xA;  var settings []http2.Setting&#xA;  // TODO(zhaoq): Have a better way to signal &amp;#34;no limit&amp;#34; because 0 is&#xA;  // permitted in the HTTP2 spec.&#xA;  // 流的最大数量&#xA;  maxStreams := config.MaxStreams&#xA;  if maxStreams == 0 {&#xA;    maxStreams = math.MaxUint32&#xA;  } else {&#xA;    settings = append(settings, http2.Setting{&#xA;      ID:  http2.SettingMaxConcurrentStreams,&#xA;      Val: maxStreams,&#xA;    })&#xA;  }&#xA;  // 流窗口大小，默认16K&#xA;  if initialWindowSize != defaultWindowSize {&#xA;    settings = append(settings, http2.Setting{&#xA;      ID:  http2.SettingInitialWindowSize,&#xA;      Val: uint32(initialWindowSize)})&#xA;  }&#xA;  if err := framer.writeSettings(true, settings...); err != nil {&#xA;    return nil, connectionErrorf(true, err, &amp;#34;transport: %v&amp;#34;, err)&#xA;  }&#xA;  // Adjust the connection flow control window if needed.&#xA;  if delta := uint32(initialConnWindowSize - defaultWindowSize); delta &amp;gt; 0 {&#xA;    if err := framer.writeWindowUpdate(true, 0, delta); err != nil {&#xA;      return nil, connectionErrorf(true, err, &amp;#34;transport: %v&amp;#34;, err)&#xA;    }&#xA;  }&#xA;  // tcp连接的KeepAlive相关参数&#xA;  kp := config.KeepaliveParams&#xA;  // 最大idle时间，超过此客户端连接将被关闭，默认无穷&#xA;  if kp.MaxConnectionIdle == 0 {&#xA;    kp.MaxConnectionIdle = defaultMaxConnectionIdle&#xA;  }&#xA;  if kp.MaxConnectionAge == 0 {&#xA;    kp.MaxConnectionAge = defaultMaxConnectionAge&#xA;  }&#xA;  // Add a jitter to MaxConnectionAge.&#xA;  kp.MaxConnectionAge += getJitter(kp.MaxConnectionAge)&#xA;  if kp.MaxConnectionAgeGrace == 0 {&#xA;    kp.MaxConnectionAgeGrace = defaultMaxConnectionAgeGrace&#xA;  }&#xA;  if kp.Time == 0 {&#xA;    kp.Time = defaultServerKeepaliveTime&#xA;  }&#xA;  if kp.Timeout == 0 {&#xA;    kp.Timeout = defaultServerKeepaliveTimeout&#xA;  }&#xA;  kep := config.KeepalivePolicy&#xA;  if kep.MinTime == 0 {&#xA;    kep.MinTime = defaultKeepalivePolicyMinTime&#xA;  }&#xA;  var buf bytes.Buffer&#xA;  t := &amp;amp;http2Server{&#xA;    ctx:             context.Background(),&#xA;    conn:            conn,&#xA;    remoteAddr:      conn.RemoteAddr(),&#xA;    localAddr:       conn.LocalAddr(),&#xA;    authInfo:        config.AuthInfo,&#xA;    framer:          framer,&#xA;    hBuf:            &amp;amp;buf,&#xA;    hEnc:            hpack.NewEncoder(&amp;amp;buf),&#xA;    maxStreams:      maxStreams,&#xA;    inTapHandle:     config.InTapHandle,&#xA;    controlBuf:      newRecvBuffer(),&#xA;    fc:              &amp;amp;inFlow{limit: initialConnWindowSize},&#xA;    sendQuotaPool:   newQuotaPool(defaultWindowSize),&#xA;    state:           reachable,&#xA;    writableChan:    make(chan int, 1),&#xA;    shutdownChan:    make(chan struct{}),&#xA;    activeStreams:   make(map[uint32]*Stream),&#xA;    streamSendQuota: defaultWindowSize,&#xA;    stats:           config.StatsHandler,&#xA;    kp:              kp,&#xA;    idle:            time.Now(),&#xA;    kep:             kep,&#xA;  }&#xA;  /* ... */&#xA;  // 专门处理控制信息&#xA;  go t.controller()&#xA;  // 专门处理tcp连接的保火逻辑&#xA;  go t.keepalive()&#xA;  // 解锁&#xA;  t.writableChan &amp;lt;- 0&#xA;  return t, nil&#xA;}&#xA;&#xA;&#xA;func (s *Server) serveStreams(st transport.ServerTransport) {&#xA;  // 处理完移除&#xA;  defer s.removeConn(st)&#xA;  // 处理完关闭Transport&#xA;  defer st.Close()&#xA;  var wg sync.WaitGroup&#xA;  // ServerTransport定义的HandleStream, 传入handler和trace callback方法&#xA;  // 这里ServerTransport的HandleStream实现会使用包装的http2.frame，循环不断读取帧&#xA;  // 直到客户端的net.Conn返回错误或者关闭为止，handler只用来处理HEADER类型的帧(即新的http&#xA;  // 请求，新的流的打开)，其他帧比如数据帧会分发到对应的stream, 这里的HEADER帧数据包含&#xA;  // 了grpc定义的http请求头等信息。HandleStream会一直循环读取新到达的帧，知道出现错误&#xA;  // 实在需要关闭客户端的连接，流读写相关的错误一般不会导致连接的关闭。&#xA;  st.HandleStreams(func(stream *transport.Stream) {&#xA;    wg.Add(1)&#xA;    go func() {&#xA;      defer wg.Done()&#xA;      // 处理stream，只有HEADER类型的帧才调用这个处理请求头等信息&#xA;      s.handleStream(st, stream, s.traceInfo(st, stream))&#xA;    }()&#xA;  }, func(ctx context.Context, method string) context.Context {&#xA;    if !EnableTracing {&#xA;      return ctx&#xA;    }&#xA;    tr := trace.New(&amp;#34;grpc.Recv.&amp;#34;+methodFamily(method), method)&#xA;    return trace.NewContext(ctx, tr)&#xA;  })&#xA;  // 等待HandleStream结束，除非客户端的连接由于错误发生需要关闭，一般不会到这&#xA;  wg.Wait()&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&#xA;&lt;li&gt;连接的处理细节(http2连接的建立)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;// 实现的ServerTransport的HandleStreams接口&#xA;func (t *http2Server) HandleStreams(handle func(*Stream), traceCtx func(context.Context, string) context.Context) {&#xA;  // Check the validity of client preface.&#xA;  // 检查是否是http2&#xA;  // 建立一个http2连接之后，之后的所有stream复用此连接&#xA;  preface := make([]byte, len(clientPreface))&#xA;  if _, err := io.ReadFull(t.conn, preface); err != nil {&#xA;    grpclog.Printf(&amp;#34;transport: http2Server.HandleStreams failed to receive the preface from client: %v&amp;#34;, err)&#xA;    t.Close()&#xA;    return&#xA;  }&#xA;  if !bytes.Equal(preface, clientPreface) {&#xA;    grpclog.Printf(&amp;#34;transport: http2Server.HandleStreams received bogus greeting from client: %q&amp;#34;, preface)&#xA;    t.Close()&#xA;    return&#xA;  }&#xA;  // 读取一帧配置信息，参考http2的规范&#xA;  frame, err := t.framer.readFrame()&#xA;  /* ... */&#xA;  sf, ok := frame.(*http2.SettingsFrame)&#xA;  /* ... */&#xA;  t.handleSettings(sf)&#xA;&#xA;  // 一直循环读取并处理帧, 注意什么时候底层的tcp连接会关闭，通常大多数情况下不会导致连接的关闭&#xA;  // 从这里开始就是处理流和数据帧的逻辑了，连接复用在这里真正被体现&#xA;  for {&#xA;    frame, err := t.framer.readFrame()&#xA;    atomic.StoreUint32(&amp;amp;t.activity, 1)&#xA;    if err != nil {&#xA;      // StreamError，不退出，&#xA;      if se, ok := err.(http2.StreamError); ok {&#xA;        t.mu.Lock()&#xA;        s := t.activeStreams[se.StreamID]&#xA;        t.mu.Unlock()&#xA;        // 关闭Stream&#xA;        if s != nil {&#xA;          t.closeStream(s)&#xA;        }&#xA;        // 控制输出错误信息&#xA;        t.controlBuf.put(&amp;amp;resetStream{se.StreamID, se.Code})&#xA;        continue&#xA;      }&#xA;      // io.EOF什么时候触发? 客户端关闭连接?&#xA;      if err == io.EOF || err == io.ErrUnexpectedEOF {&#xA;        t.Close()&#xA;        return&#xA;      }&#xA;      grpclog.Printf(&amp;#34;transport: http2Server.HandleStreams failed to read frame: %v&amp;#34;, err)&#xA;      t.Close()&#xA;      return&#xA;    }&#xA;    // HTTP2定义的帧类型&#xA;    switch frame := frame.(type) {&#xA;    // HEADER frame用来打开一个stream，表示一个新请求的到来和一个新的流的建立，这里需要使用Server定义的处理逻辑&#xA;    // 解析请求头，得到服务和方法的名称&#xA;    case *http2.MetaHeadersFrame:&#xA;      // 上层传递过来的handle处理stream&#xA;      if t.operateHeaders(frame, handle, traceCtx) {&#xA;        t.Close()&#xA;        break&#xA;      }&#xA;    // DataFrame, RSTStream, WindowUpdateFrame都属于特定stream id的Stream&#xA;    // 会被分派给对应的Stream&#xA;    case *http2.DataFrame:&#xA;      t.handleData(frame)&#xA;    case *http2.RSTStreamFrame:&#xA;      t.handleRSTStream(frame)&#xA;    case *http2.SettingsFrame:&#xA;      t.handleSettings(frame)&#xA;    case *http2.PingFrame:&#xA;      t.handlePing(frame)&#xA;    case *http2.WindowUpdateFrame:&#xA;      t.handleWindowUpdate(frame)&#xA;    case *http2.GoAwayFrame:&#xA;      // TODO: Handle GoAway from the client appropriately.&#xA;    default:&#xA;      grpclog.Printf(&amp;#34;transport: http2Server.HandleStreams found unhandled frame type %v.&amp;#34;, frame)&#xA;    }&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&#xA;&lt;li&gt;新请求的处理细节(新流的打开和帧数据的处理)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;// 解析流，提取服务名，方法名等信息，handleStream实现的是stream的业务逻辑处理&#xA;func (s *Server) handleStream(t transport.ServerTransport, stream *transport.Stream, trInfo *traceInfo) {&#xA;  sm := stream.Method()&#xA;  if sm != &amp;#34;&amp;#34; &amp;amp;&amp;amp; sm[0] == &amp;#39;/&amp;#39; {&#xA;    sm = sm[1:]&#xA;  }&#xA;  pos := strings.LastIndex(sm, &amp;#34;/&amp;#34;)&#xA;  /* ... */&#xA;  // 服务名&#xA;  service := sm[:pos]&#xA;  // 方法名&#xA;  method := sm[pos+1:]&#xA;  // 服务&#xA;  srv, ok := s.m[service]&#xA;  // 未注册的服务&#xA;  if !ok {&#xA;    if unknownDesc := s.opts.unknownStreamDesc; unknownDesc != nil {&#xA;      s.processStreamingRPC(t, stream, nil, unknownDesc, trInfo)&#xA;      return&#xA;    }&#xA;    /* ... */&#xA;    return&#xA;  }&#xA;  // Unary RPC or Streaming RPC?&#xA;  // 处理单次请求&#xA;  if md, ok := srv.md[method]; ok {&#xA;    s.processUnaryRPC(t, stream, srv, md, trInfo)&#xA;    return&#xA;  }&#xA;  // 处理流式请求&#xA;  if sd, ok := srv.sd[method]; ok {&#xA;    s.processStreamingRPC(t, stream, srv, sd, trInfo)&#xA;    return&#xA;  }&#xA;  &#xA;  // 没找到对应方法&#xA;  if unknownDesc := s.opts.unknownStreamDesc; unknownDesc != nil {&#xA;    s.processStreamingRPC(t, stream, nil, unknownDesc, trInfo)&#xA;    return&#xA;  }&#xA;  /* ... */&#xA;}&#xA;&#xA;// 处理单次请求&#xA;func (s *Server) processUnaryRPC(t transport.ServerTransport, stream *transport.Stream, srv *service, md *MethodDesc, trInfo *traceInfo) (err error) {&#xA;  /* ... */&#xA;  // 发送数据的压缩格式&#xA;  if s.opts.cp != nil {&#xA;    // NOTE: this needs to be ahead of all handling, https://github.com/grpc/gRPC-Go/issues/686.&#xA;    stream.SetSendCompress(s.opts.cp.Type())&#xA;  }&#xA;  // 解析消息&#xA;  p := &amp;amp;parser{r: stream}&#xA;  for { // TODO: delete&#xA;    // 第一个HEADER帧过后，后面的数据帧包含消息数据&#xA;    // 头5个字节：第一个字节代表是否压缩，2-5个字节消息体的长度，后面的数据全部读取给req&#xA;    pf, req, err := p.recvMsg(s.opts.maxMsgSize)&#xA;    /* ... */&#xA;    // 检查压缩类型是否正确&#xA;    if err := checkRecvPayload(pf, stream.RecvCompress(), s.opts.dc); err != nil {&#xA;      /* ... */&#xA;    }&#xA;    // 解压解码等操作，最终数据放到v中，而这个v则指向服务接口实现对应方法的请求参数req&#xA;    df := func(v interface{}) error {&#xA;      if inPayload != nil {&#xA;        inPayload.WireLength = len(req)&#xA;      }&#xA;      if pf == compressionMade {&#xA;        var err error&#xA;        // 解压&#xA;        req, err = s.opts.dc.Do(bytes.NewReader(req))&#xA;        if err != nil {&#xA;          return Errorf(codes.Internal, err.Error())&#xA;        }&#xA;      }&#xA;      // 解压之后超过最大消息长度&#xA;      if len(req) &amp;gt; s.opts.maxMsgSize {&#xA;        // TODO: Revisit the error code. Currently keep it consistent with&#xA;        // java implementation.&#xA;        return status.Errorf(codes.Internal, &amp;#34;grpc: server received a message of %d bytes exceeding %d limit&amp;#34;, len(req), s.opts.maxMsgSize)&#xA;      }&#xA;      // 解码&#xA;      if err := s.opts.codec.Unmarshal(req, v); err != nil {&#xA;        return status.Errorf(codes.Internal, &amp;#34;grpc: error unmarshalling request: %v&amp;#34;, err)&#xA;      }&#xA;      /* ... */&#xA;    }&#xA;&#xA;    // 处理原始消息数据，调用服务方法，这个Handler即上面protobuf的gRPC-Go插件为我们生成的处理函数&#xA;    reply, appErr := md.Handler(srv.server, stream.Context(), df, s.opts.unaryInt)&#xA;    /* ... */&#xA;    // 发送响应，输出会在Transport和Stream两层做流控&#xA;    if err := s.sendResponse(t, stream, reply, s.opts.cp, opts); err != nil {&#xA;      // 单次请求处理完毕，直接返回&#xA;      if err == io.EOF {&#xA;        // The entire stream is done (for unary RPC only).&#xA;        return err&#xA;      }&#xA;      /* ... */&#xA;    }&#xA;    &#xA;    // TODO: Should we be logging if writing status failed here, like above?&#xA;    // Should the logging be in WriteStatus?  Should we ignore the WriteStatus&#xA;    // error or allow the stats handler to see it?&#xA;    // 发送http响应头，关闭stream&#xA;    return t.WriteStatus(stream, status.New(codes.OK, &amp;#34;&amp;#34;))&#xA;  }&#xA;}&#xA;&#xA;// 处理流式方法&#xA;func (s *Server) processStreamingRPC(t transport.ServerTransport, stream *transport.Stream, srv *service, sd *StreamDesc, trInfo *traceInfo) (err error) {&#xA;  /* ... */&#xA;  ss := &amp;amp;serverStream{&#xA;    t:            t,&#xA;    s:            stream,&#xA;    p:            &amp;amp;parser{r: stream},&#xA;    codec:        s.opts.codec,&#xA;    cp:           s.opts.cp,&#xA;    dc:           s.opts.dc,&#xA;    maxMsgSize:   s.opts.maxMsgSize,&#xA;    trInfo:       trInfo,&#xA;    statsHandler: sh,&#xA;  }&#xA;  if ss.cp != nil {&#xA;    ss.cbuf = new(bytes.Buffer)&#xA;  }&#xA;  /* ... */&#xA;  var appErr error&#xA;  var server interface{}&#xA;  if srv != nil {&#xA;    server = srv.server&#xA;  }&#xA;  if s.opts.streamInt == nil {&#xA;    // 调用protobuf gRPC-Go插件生成的ServiceDesc中的Handler&#xA;    appErr = sd.Handler(server, ss)&#xA;  } else {&#xA;    info := &amp;amp;StreamServerInfo{&#xA;      FullMethod:     stream.Method(),&#xA;      IsClientStream: sd.ClientStreams,&#xA;      IsServerStream: sd.ServerStreams,&#xA;    }&#xA;    appErr = s.opts.streamInt(server, ss, info, sd.Handler)&#xA;  }&#xA;  /* ... */&#xA;  // 注意，业务逻辑的实现函数返回后，最终还是会由服务端关闭流&#xA;  return t.WriteStatus(ss.s, status.New(codes.OK, &amp;#34;&amp;#34;))&#xA;}&#xA;&#xA;// 发送响应数据，输出写数据时做了流量的控制&#xA;func (s *Server) sendResponse(t transport.ServerTransport, stream *transport.Stream, msg interface{}, cp Compressor, opts *transport.Options) error {&#xA;  // 编码并压缩&#xA;  p, err := encode(s.opts.codec, msg, cp, cbuf, outPayload)&#xA;  // ok, 写响应，加了出带宽的流控&#xA;  err = t.Write(stream, p, opts)&#xA;  /* ... */&#xA;  return err&#xA;}&#xA;func (t *http2Server) Write(s *Stream, data []byte, opts *Options) (err error) {&#xA;  // TODO(zhaoq): Support multi-writers for a single stream.&#xA;  var writeHeaderFrame bool&#xA;  s.mu.Lock()&#xA;  // stream已经关闭了&#xA;  if s.state == streamDone {&#xA;    s.mu.Unlock()&#xA;    return streamErrorf(codes.Unknown, &amp;#34;the stream has been done&amp;#34;)&#xA;  }&#xA;  // 需要写header&#xA;  if !s.headerOk {&#xA;    writeHeaderFrame = true&#xA;  }&#xA;  s.mu.Unlock()&#xA;  // 写响应头&#xA;  if writeHeaderFrame {&#xA;    t.WriteHeader(s, nil)&#xA;  }&#xA;&#xA;  // 缓冲&#xA;  r := bytes.NewBuffer(data)&#xA;  for {&#xA;    if r.Len() == 0 {&#xA;      return nil&#xA;    }&#xA;    // 每个frame最多16k&#xA;    size := http2MaxFrameLen&#xA;    // ServerTransport的quota默认等于Stream的quota，为默认窗口大小65535字节&#xA;    // 流层限流&#xA;    sq, err := wait(s.ctx, nil, nil, t.shutdownChan, s.sendQuotaPool.acquire())&#xA;    // 传输层限流&#xA;    tq, err := wait(s.ctx, nil, nil, t.shutdownChan, t.sendQuotaPool.acquire())&#xA;    if sq &amp;lt; size {&#xA;      size = sq&#xA;    }&#xA;    if tq &amp;lt; size {&#xA;      size = tq&#xA;    }&#xA;    // 实际需要发送的数据, 返回buf的size长度的slice&#xA;    p := r.Next(size)&#xA;    ps := len(p)&#xA;    // 小于本次的quota，则归还多的部分&#xA;    if ps &amp;lt; sq {&#xA;      // Overbooked stream quota. Return it back.&#xA;      // add会重置channel中的可用quota&#xA;      s.sendQuotaPool.add(sq - ps)&#xA;    }&#xA;    if ps &amp;lt; tq {&#xA;      // Overbooked transport quota. Return it back.&#xA;      t.sendQuotaPool.add(tq - ps)&#xA;    }&#xA;    t.framer.adjustNumWriters(1)&#xA;    // 等待拿到此transport的锁，通过t.writableChan实现，由于可能有多个stream等待写transport，所以需要&#xA;    // 用chan序列化&#xA;    if _, err := wait(s.ctx, nil, nil, t.shutdownChan, t.writableChan); err != nil {&#xA;      /* ... */&#xA;    }&#xA;    select {&#xA;    case &amp;lt;-s.ctx.Done():&#xA;      t.sendQuotaPool.add(ps)&#xA;      if t.framer.adjustNumWriters(-1) == 0 {&#xA;        t.controlBuf.put(&amp;amp;flushIO{})&#xA;      }&#xA;      // 需要释放锁&#xA;      t.writableChan &amp;lt;- 0&#xA;      return ContextErr(s.ctx.Err())&#xA;    default:&#xA;    }&#xA;    var forceFlush bool&#xA;    // 没有剩下的数据可写了，直接flush，注意http2.frame写的时候是写到framer的Buffer writer&#xA;    // 中，需要flush buffer writer，让数据完全写到客户端的net.Conn里去&#xA;    // 注意这里的opts.Last，客户端发送完数据后需要显示调用CloseSend标识opts.Last为true&#xA;    // 只有在不是显示由客户端发送结束标识，并且是最后一个使用这个stream，且没有可再读取&#xA;    // 的数据时才强制flush&#xA;    if r.Len() == 0 &amp;amp;&amp;amp; t.framer.adjustNumWriters(0) == 1 &amp;amp;&amp;amp; !opts.Last {&#xA;      forceFlush = true&#xA;    }&#xA;    // 写到buffer reader中&#xA;    if err := t.framer.writeData(forceFlush, s.id, false, p); err != nil {&#xA;      t.Close()&#xA;      return connectionErrorf(true, err, &amp;#34;transport: %v&amp;#34;, err)&#xA;    }&#xA;    // flush&#xA;    if t.framer.adjustNumWriters(-1) == 0 {&#xA;      t.framer.flushWrite()&#xA;    }&#xA;    // 需要释放锁，让其他stream写&#xA;    t.writableChan &amp;lt;- 0&#xA;  }&#xA;}&#xA;&#xA;// Data帧的处理，直接写到对应流的buf&#xA;func (t *http2Server) handleData(f *http2.DataFrame) {&#xA;  // 根据stream id找到stream&#xA;  s, ok := t.getStream(f)&#xA;&#xA;  if size &amp;gt; 0 {&#xA;    if f.Header().Flags.Has(http2.FlagDataPadded) {&#xA;      if w := t.fc.onRead(uint32(size) - uint32(len(f.Data()))); w &amp;gt; 0 {&#xA;        t.controlBuf.put(&amp;amp;windowUpdate{0, w})&#xA;      }&#xA;    }&#xA;    /* ... */&#xA;    s.mu.Unlock()&#xA;    // TODO(bradfitz, zhaoq): A copy is required here because there is no&#xA;    // guarantee f.Data() is consumed before the arrival of next frame.&#xA;    // Can this copy be eliminated?&#xA;    if len(f.Data()) &amp;gt; 0 {&#xA;      data := make([]byte, len(f.Data()))&#xA;      copy(data, f.Data())&#xA;      // 写入stream的buf&#xA;      s.write(recvMsg{data: data})&#xA;    }&#xA;  }&#xA;  if f.Header().Flags.Has(http2.FlagDataEndStream) {&#xA;    // Received the end of stream from the client.&#xA;    s.mu.Lock()&#xA;    if s.state != streamDone {&#xA;      s.state = streamReadDone&#xA;    }&#xA;    s.mu.Unlock()&#xA;    // 写入stream的buf&#xA;    s.write(recvMsg{err: io.EOF})&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;hr&gt;&#xA;&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;&#xA;&lt;p&gt;至此，服务端的主要流程就基本走完了，整个处理流程还有很多加密、授权、http2连接的控制信息(比如窗口大小的设置等)、KeepAlive逻辑以及穿插在各个地方的统计、追踪、日志处理等细节，这些细节对理解gRPC-Go的实现影响不大，所以不再细说。整个流程下来，多少可以看到Go的很多特性极大地方便了grpc的实现，用goroutine代替多路复用的回调，io的抽象与缓冲。同时，http2整个的模型其实和基于多路复用实现的grpc框架底层数据传输协议有些类似，http2的一个帧类似于某个格式化和序列化后的请求数据或响应数据，但是传统的rpc协议并没有流对应的概念，要实现&amp;quot;流的复用&amp;quot;也不是太容易，请求的下层直接是tcp连接，另外http2是通用的标准化协议，而且复用连接之后其性能也不差。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Raft实现小结</title>
      <link>https://feilengcui008.github.io/post/raft%E5%AE%9E%E7%8E%B0%E5%B0%8F%E7%BB%93/</link>
      <pubDate>Mon, 20 Mar 2017 19:02:13 +0800</pubDate>
      <guid>https://feilengcui008.github.io/post/raft%E5%AE%9E%E7%8E%B0%E5%B0%8F%E7%BB%93/</guid>
      <description>&lt;p&gt;上一周花了大部分时间重新拾起了之前落下的MIT6.824 2016的分布式课程，实现和调试了下Raft协议，虽然Raft协议相对其他容错分布式一致性协议如Paxos/Multi-Paxos/VR/Zab等来说更容易理解，但是在实现和调试过程中也遇到不少细节问题。虽然论文中有伪代码似的协议描述，但是要把每一小部分逻辑组合起来放到正确的位置还是需要不少思考和踩坑的，这篇文章对此做一个小结。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;raft实现&#34;&gt;Raft实现&lt;/h3&gt;&#xA;&lt;p&gt;这里实现的主要是Raft基本的Leader Election和Log Replication部分，没有考虑Snapshot和Membership Reconfiguration的部分，因为前两者是后两者的实现基础，也是Raft协议的核心。MIT6.824 2016使用的是Go语言实现，一大好处是并发和异步处理非常直观简洁，不用自己去管理异步线程。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;宏观&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;合理规划同步和异步执行的代码块，比如Heartbeat routine/向多个节点异步发送请求的routine&lt;/li&gt;&#xA;&lt;li&gt;注意加锁解锁，每个节点的heartbeat routine/请求返回/接收请求都可能改变Raft结构的状态数据，尤其注意不要带锁发请求，很容易和另一个同时带锁发请求的节点死锁&lt;/li&gt;&#xA;&lt;li&gt;理清以下几块的大体逻辑&#xA;&lt;ul&gt;&#xA;&lt;li&gt;公共部分的逻辑&#xA;&lt;ul&gt;&#xA;&lt;li&gt;发现小的term丢弃&lt;/li&gt;&#xA;&lt;li&gt;发现大的term，跟新自身term，转换为Follower，重置votedFor&lt;/li&gt;&#xA;&lt;li&gt;修改term/votedFor/log之后需要持久化&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Leader/Follower/Candidate的Heartbeat routine逻辑&lt;/li&gt;&#xA;&lt;li&gt;Leader Election&#xA;&lt;ul&gt;&#xA;&lt;li&gt;发送RequestVote并处理返回，成为leader后的逻辑(nop log replication)&lt;/li&gt;&#xA;&lt;li&gt;接收到RequestVote的逻辑，如何投票(Leader Election Restriction)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Log Replication&#xA;&lt;ul&gt;&#xA;&lt;li&gt;发送AppendEntries并处理返回(consistency check and repair)，达成一致后的逻辑(更新commitIndex/nextIndex/matchIndex， apply log)&lt;/li&gt;&#xA;&lt;li&gt;接收到AppendEntries的逻辑(consistency check and repair, 更新commitIndex，apply log)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;细节&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Leader Election&#xA;&lt;ul&gt;&#xA;&lt;li&gt;timeout的随机性&lt;/li&gt;&#xA;&lt;li&gt;timeout的范围，必须远大于rpc请求的平均时间，不然可能很久都选不出主，通常rpc请求在ms级别，所以可设置150~300ms&lt;/li&gt;&#xA;&lt;li&gt;选主请求发送结束后，由于有可能在选主请求(RequestVote)的返回或者别的节点的选主请求中发现较大的term，而被重置为Follower，这时即使投票数超过半数也应该放弃成为Leader，因为当前选主请求的term已经过时，成为Leader可能导致在新的term中出现两个Leader.(注意这点是由于发送请求是异步的，同步请求发现较大的term后可直接修改状态返回)&lt;/li&gt;&#xA;&lt;li&gt;每次发现较大的term时，自身重置为Follower，更新term的同时，需要重置votedFor，以便在新的term中可以参与投票&lt;/li&gt;&#xA;&lt;li&gt;每次选主成功后，发送一条nop的日志复制请求，让Leader提交所有之前应该提交的日志，从而让Leader的状态机为最新，这样为读请求提供linearializability，不会返回stale data&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Log Replication&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Leader更新commitIndex时，需要严格按照论文上的限制条件(使用matchIndex)，不能提交以前term的日志&lt;/li&gt;&#xA;&lt;li&gt;对于同一term同一log index的日志复制，如果失败，应该无限重试，直到成功或者自身不再是Leader，因为我们需要保证在同一term同一log index下有唯一的一条日志cmd，如果不无限重试，有可能会导致以下的问题&#xA;&lt;ul&gt;&#xA;&lt;li&gt;五个节点(0, 1, 2, 3, 4), node 0为leader，复制一条Term n, LogIndex m, Cmd cmd1的日志&lt;/li&gt;&#xA;&lt;li&gt;node 1收到cmd1的日志请求，node 2, 3, 4未收到&lt;/li&gt;&#xA;&lt;li&gt;如果node 0不无限重试而返回，此时另一个cmd2的日志复制请求到达，leader 0使用同一个Term和LogIndex发送请求&lt;/li&gt;&#xA;&lt;li&gt;node 2, 3, 4收到cmd2的请求，node 1未收到&lt;/li&gt;&#xA;&lt;li&gt;node 1通过election成为新的leader(RequestVote的检查会通过，因为具有相同的Term和LogIndex)&lt;/li&gt;&#xA;&lt;li&gt;node 1发送nop提交之前的日志，cmd1被applied(consistency check会通过，因为PrevLogTerm和PrevLogIndex相同)&lt;/li&gt;&#xA;&lt;li&gt;cmd2则被node 2, 3, 4 applied&lt;/li&gt;&#xA;&lt;li&gt;cmd1和cmd2发生了不一致&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;测试和其他一些问题&lt;/p&gt;</description>
    </item>
    <item>
      <title>Runc容器生命周期</title>
      <link>https://feilengcui008.github.io/post/runc%E5%AE%B9%E5%99%A8%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/</link>
      <pubDate>Wed, 30 Nov 2016 17:48:20 +0800</pubDate>
      <guid>https://feilengcui008.github.io/post/runc%E5%AE%B9%E5%99%A8%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/</guid>
      <description>&lt;p&gt;容器的生命周期涉及到内部的程序实现和面向用户的命令行界面，runc内部容器状态转换操作、runc命令的参数定义的操作、docker client定义的容器操作是不同的，比如对于docker client的create来说，&#xA;语义和runc就完全不同，这一篇文章分析runc的容器生命周期的抽象、内部实现以及状态转换图。理解了runc的容器状态转换再对比理解docker client提供的容器操作命令的语义会更容易些。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h4 id=&#34;容器生命周期相关接口&#34;&gt;容器生命周期相关接口&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;最基本的required的接口&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Start: 初始化容器环境并启动一个init进程，或者加入已有容器的namespace并启动一个setns进程；执行postStart hook; 阻塞在init管道的写端，用户发信号替换执行真正的命令&lt;/li&gt;&#xA;&lt;li&gt;Exec: 读init管道，通知init进程或者setns进程继续往下执行&lt;/li&gt;&#xA;&lt;li&gt;Run: Start + Exec的组合&lt;/li&gt;&#xA;&lt;li&gt;Signal: 向容器内init进程发信号&lt;/li&gt;&#xA;&lt;li&gt;Destroy: 杀掉cgroups中的进程，删除cgroups对应的path，运行postStop的hook&lt;/li&gt;&#xA;&lt;li&gt;其他&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Set: 更新容器的配置信息，比如修改cgroups resize等&lt;/li&gt;&#xA;&lt;li&gt;Config: 获取容器的配置信息&lt;/li&gt;&#xA;&lt;li&gt;State: 获取容器的状态信息&lt;/li&gt;&#xA;&lt;li&gt;Status: 获取容器的当前运行状态: created、running、pausing、paused、stopped&lt;/li&gt;&#xA;&lt;li&gt;Processes: 返回容器内所有进程的列表&lt;/li&gt;&#xA;&lt;li&gt;Stats: 容器内的cgroups统计信息&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;对于linux容器定义并实现了特有的功能接口&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Pause: free容器中的所有进程&lt;/li&gt;&#xA;&lt;li&gt;Resume: thaw容器内的所有进程&lt;/li&gt;&#xA;&lt;li&gt;Checkpoint: criu checkpoint&lt;/li&gt;&#xA;&lt;li&gt;Restore: criu restore&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h4 id=&#34;接口在内部的实现&#34;&gt;接口在内部的实现&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;对于Start/Run/Exec的接口是作为不同os环境下的标准接口对开发者暴露，接口在内部的实现有很多重复的部分可以统一，因此内部的接口实际上更简洁，这里以linux容器为例说明&#xA;&lt;ul&gt;&#xA;&lt;li&gt;对于Start/Run/Exec在内部实现实际上只用到下面两个函数，通过传入flag(容器是否处于stopped状态)区分是创建容器的init进程还是创建进程的init进程&#xA;&lt;ul&gt;&#xA;&lt;li&gt;start: 创建init进程，如果status == stopped，则创建并执行newInitProcess，否则创建并执行newSetnsProcess，等待用户发送执行信号(等在管道写端上)，用用户的命令替换掉&lt;/li&gt;&#xA;&lt;li&gt;exec: 读管道，发送执行信号&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Start直接使用start&lt;/li&gt;&#xA;&lt;li&gt;Run实际先使用start(doInit = true)，然后exec&lt;/li&gt;&#xA;&lt;li&gt;Exec实际先使用start(doInit = false), 然后exec&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h4 id=&#34;对用户暴露的命令行参数与容器接口的对应关系以linux容器为例&#34;&gt;对用户暴露的命令行参数与容器接口的对应关系，以linux容器为例&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;create -&amp;gt; Start(doInit = true)&lt;/li&gt;&#xA;&lt;li&gt;start -&amp;gt; Exec&lt;/li&gt;&#xA;&lt;li&gt;run -&amp;gt; Run(doInit = true)&lt;/li&gt;&#xA;&lt;li&gt;exec -&amp;gt; Run(doInit = false)&lt;/li&gt;&#xA;&lt;li&gt;kill -&amp;gt; Signal&lt;/li&gt;&#xA;&lt;li&gt;delete -&amp;gt; Signal and Destroy&lt;/li&gt;&#xA;&lt;li&gt;update -&amp;gt; Set&lt;/li&gt;&#xA;&lt;li&gt;state -&amp;gt; State&lt;/li&gt;&#xA;&lt;li&gt;events -&amp;gt; Stats&lt;/li&gt;&#xA;&lt;li&gt;ps -&amp;gt; Processes&lt;/li&gt;&#xA;&lt;li&gt;list&lt;/li&gt;&#xA;&lt;li&gt;linux specific&#xA;&lt;ul&gt;&#xA;&lt;li&gt;pause -&amp;gt; Pause&lt;/li&gt;&#xA;&lt;li&gt;resume -&amp;gt; Resume&lt;/li&gt;&#xA;&lt;li&gt;checkpoint -&amp;gt; Checkpoint&lt;/li&gt;&#xA;&lt;li&gt;restore -&amp;gt; Restore&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h4 id=&#34;runc命令行的动作序列对容器状态机的影响&#34;&gt;runc命令行的动作序列对容器状态机的影响&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;对于一个容器的生命周期来说，稳定状态有4个: stopped、created、running、paused&lt;/li&gt;&#xA;&lt;li&gt;注意下面状态转换图中的动作是runc命令行参数动作，不是容器的接口动作，这里没考虑checkpoint相关的restore状态&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;img src=&#34;https://feilengcui008.github.io/images/runc.png&#34; alt=&#34;Runc容器状态机&#34;&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Python退出时hang住的问题</title>
      <link>https://feilengcui008.github.io/post/python%E9%80%80%E5%87%BA%E6%97%B6hang%E4%BD%8F%E7%9A%84%E9%97%AE%E9%A2%98/</link>
      <pubDate>Sun, 16 Oct 2016 17:46:43 +0800</pubDate>
      <guid>https://feilengcui008.github.io/post/python%E9%80%80%E5%87%BA%E6%97%B6hang%E4%BD%8F%E7%9A%84%E9%97%AE%E9%A2%98/</guid>
      <description>&lt;p&gt;最近使用Python遇到两个非常不好定位的问题，表现都是Python主线程退出时hang住。最终定位出一个是subprocess模块使用不当的问题，另一个是threading.Timer线程的问题。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h4 id=&#34;subprocess模块使用不当的问题&#34;&gt;subprocess模块使用不当的问题&lt;/h4&gt;&#xA;&lt;p&gt;Python的subprocess比较强大，基本上能替换os.system、os.popen、commands.getstatusoutput的功能，但是在使用的过程中需要注意参数stdin/stdout/stderr使用subprocess.PIPE的情况，因为管道通常会有默认大小的缓冲区(Linux x86_64下实测是64K)，父进程如果不使用communicate消耗掉子进程管道写端(stdout/stderr)中的数据，直接进入wait等待子进程退出，此时子进程可能阻塞在了pipe的写上，从而导致父子进程都hang住，下面是测试代码。&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# main.py&#xA;#!/usr/bin/env python&#xA;# encoding: utf-8&#xA;&#xA;import subprocess&#xA;import os&#xA;import tempfile&#xA;import sys&#xA;import traceback&#xA;import commands&#xA;&#xA;&#xA;# both parent and child process will hang &#xA;# if run.py stdout/stderr exceed 64K, since&#xA;# parent process is waiting child process exit&#xA;# but child process is blocked by writing pipe&#xA;def testSubprocessCallPipe():&#xA;    # call: just Popen().wait()&#xA;    p = subprocess.Popen([&amp;#34;python&amp;#34;, &amp;#34;run.py&amp;#34;], &#xA;        stdin=subprocess.PIPE, &#xA;        stdout=subprocess.PIPE, &#xA;        stderr=subprocess.PIPE)&#xA;    ret = p.wait()&#xA;    print ret&#xA;&#xA;&#xA;# will not hang since the parent process which&#xA;# call communicate will poll or thread to comsume&#xA;# the pipe buffer, so the child process can write&#xA;# all it&amp;#39;s data to stdout or stderr pipe and it will&#xA;# not be blocked.&#xA;def testSubprocessCommunicate():&#xA;    p = subprocess.Popen([&amp;#34;python&amp;#34;, &amp;#34;run.py&amp;#34;], &#xA;        stdin=subprocess.PIPE, &#xA;        stdout=subprocess.PIPE, &#xA;        stderr=subprocess.PIPE)&#xA;    print p.communicate()[0]&#xA;&#xA;&#xA;# will not hang since sys.stdout and sys.stderr &#xA;# don&amp;#39;t have 64K default buffer limitation, child&#xA;# process can write all it&amp;#39;s data to stdout or &#xA;# stderr fd and exit&#xA;def testSubprocessCallStdout():&#xA;    # call: just Popen().wait()&#xA;    p = subprocess.Popen([&amp;#34;python&amp;#34;, &amp;#34;run.py&amp;#34;], &#xA;        stdin=sys.stdin, &#xA;        stdout=sys.stdout, &#xA;        stderr=sys.stderr)&#xA;    ret = p.wait()&#xA;    print ret&#xA;&#xA;&#xA;# will not hang since file has no limitation of 64K&#xA;def testSubprocessCallFile():&#xA;    stdout = tempfile.mktemp()&#xA;    stderr = tempfile.mktemp()&#xA;    print &amp;#34;stdout file %s&amp;#34; % (stdout,), &amp;#34;stderr file %s&amp;#34; % (stderr,)&#xA;    stdout = open(stdout, &amp;#34;w&amp;#34;)&#xA;    stderr = open(stderr, &amp;#34;w&amp;#34;)&#xA;    p = subprocess.Popen([&amp;#34;python&amp;#34;, &amp;#34;run.py&amp;#34;], &#xA;        stdin=None, &#xA;        stdout=stdout, &#xA;        stderr=stderr)&#xA;    ret = p.wait()&#xA;    print ret&#xA;&#xA;&#xA;print os.getpid()&#xA;# not hang&#xA;print &amp;#34;use file&amp;#34;&#xA;testSubprocessCallFile()&#xA;# not hang&#xA;print &amp;#34;use sys.stdout and sys.stderr&amp;#34;&#xA;testSubprocessCallStdout()&#xA;# not hang&#xA;print &amp;#34;use pipe and communicate&amp;#34;&#xA;testSubprocessCommunicate()&#xA;# hang&#xA;print &amp;#34;use pipe and call directly&amp;#34;&#xA;testSubprocessCallPipe()&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# run.py&#xA;import os&#xA;&#xA;print os.getpid()&#xA;&#xA;string = &amp;#34;&amp;#34;&#xA;# &amp;gt; 64k will hang&#xA;for i in range(1024 * 64 - 4):&#xA;    string = string + &amp;#34;c&amp;#34;&#xA;# flush to my stdout which might &#xA;# be sys.stdout/pipe/fd...&#xA;print string&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;另外，在subprocess模块源码中还注释说明了另外一种由于fork -&amp;gt; 子进程gc -&amp;gt; exec导致的进程hang住，详细信息可以阅读subprocess模块源码。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Docker简介</title>
      <link>https://feilengcui008.github.io/post/docker%E7%AE%80%E4%BB%8B/</link>
      <pubDate>Sat, 08 Oct 2016 17:44:02 +0800</pubDate>
      <guid>https://feilengcui008.github.io/post/docker%E7%AE%80%E4%BB%8B/</guid>
      <description>&lt;p&gt;本文主要介绍Docker的一些基本概念、Docker的源码分析、Docker相关的一些issue、Docker周边生态等等。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;基本概念&#34;&gt;基本概念&lt;/h3&gt;&#xA;&lt;h4 id=&#34;basics&#34;&gt;Basics&lt;/h4&gt;&#xA;&lt;p&gt;docker大体包括三大部分，runtime(container)、image(graphdriver)、registry，runtime提供环境的隔离与资源的隔离和限制，image提供layer、image、rootfs的管理、registry负责镜像存储与分发。当然，还有其他一些比如data volume, network等等，总体来说还是分为计算、存储与网络。&lt;/p&gt;&#xA;&lt;h4 id=&#34;computing&#34;&gt;computing&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;接口规范&lt;/li&gt;&#xA;&lt;li&gt;命名空间隔离、资源隔离与限制的实现&lt;/li&gt;&#xA;&lt;li&gt;造坑与入坑&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;network&#34;&gt;network&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;接口规范与实现&#xA;&lt;ul&gt;&#xA;&lt;li&gt;bridge&#xA;&lt;ul&gt;&#xA;&lt;li&gt;veth pair for two namespace communication&lt;/li&gt;&#xA;&lt;li&gt;bridge and veth pair for multi-namespace communication&lt;/li&gt;&#xA;&lt;li&gt;do not support multi-host&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;overlay&#xA;&lt;ul&gt;&#xA;&lt;li&gt;docker overlay network: with swarm mode or with kv etcd/zookeeper/consul -&amp;gt; vxlan&lt;/li&gt;&#xA;&lt;li&gt;coreos flannel -&amp;gt; 多种backend，udp/vxlan&amp;hellip;&lt;/li&gt;&#xA;&lt;li&gt;ovs&lt;/li&gt;&#xA;&lt;li&gt;weave -&amp;gt; udp and vxlan，与flannel udp不同的是会将多container的packet一块打包&lt;/li&gt;&#xA;&lt;li&gt;calico&#xA;&lt;ul&gt;&#xA;&lt;li&gt;pure layer 3&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://xelatex.github.io/2015/11/15/Battlefield-Calico-Flannel-Weave-and-Docker-Overlay-Network/&#34;&gt;一篇对比&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;null&#xA;&lt;ul&gt;&#xA;&lt;li&gt;与世隔绝&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;host&#xA;&lt;ul&gt;&#xA;&lt;li&gt;共享主机net namespace&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;storage&#34;&gt;storage&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;graphdriver(layers,image and rootfs)&#xA;&lt;ul&gt;&#xA;&lt;li&gt;graph:独立于各个driver，记录image的各层依赖关系(DAG)，注意是image不包括运行中的container的layer，当container commit生成image后，会将新layer的依赖关系写入&lt;/li&gt;&#xA;&lt;li&gt;device mapper&#xA;&lt;ul&gt;&#xA;&lt;li&gt;snapshot基于block，allocation-on-demand&lt;/li&gt;&#xA;&lt;li&gt;默认基于空洞文件(data and metadata)挂载到回环设备&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;aufs&#xA;&lt;ul&gt;&#xA;&lt;li&gt;diff:实际存储各个layer的变更数据&lt;/li&gt;&#xA;&lt;li&gt;layers:每个layer依赖的layers，包括正在运行中的container&lt;/li&gt;&#xA;&lt;li&gt;mnt:container的实际挂载根目录&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;overlayfs&lt;/li&gt;&#xA;&lt;li&gt;vfs&lt;/li&gt;&#xA;&lt;li&gt;btrfs&lt;/li&gt;&#xA;&lt;li&gt;&amp;hellip;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;volume&#xA;&lt;ul&gt;&#xA;&lt;li&gt;driver接口&#xA;&lt;ul&gt;&#xA;&lt;li&gt;local driver&lt;/li&gt;&#xA;&lt;li&gt;flocker: container和volume管理与迁移&lt;/li&gt;&#xA;&lt;li&gt;rancher的convoy:多重volume存储后端的支持device mapper, NFS, EBS&amp;hellip;,提供快照、备份、恢复等功能&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;数据卷容器&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;registry:与docker registry交互&#xA;&lt;ul&gt;&#xA;&lt;li&gt;支持basic/token等认证方式&lt;/li&gt;&#xA;&lt;li&gt;token可以基于basic/oauth等方式从第三方auth server获取bearer token&lt;/li&gt;&#xA;&lt;li&gt;tls通信的支持&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;libkv&#xA;&lt;ul&gt;&#xA;&lt;li&gt;支持consul/etcd/zookeeper&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;分布式存储的支持&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;security&#34;&gt;security&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;docker&#xA;&lt;ul&gt;&#xA;&lt;li&gt;libseccomp限制系统调用(内部使用bpf)&lt;/li&gt;&#xA;&lt;li&gt;linux capabilities限制root用户权限范围scope&lt;/li&gt;&#xA;&lt;li&gt;user namespace用户和组的映射&lt;/li&gt;&#xA;&lt;li&gt;selinux&lt;/li&gt;&#xA;&lt;li&gt;apparmor&lt;/li&gt;&#xA;&lt;li&gt;&amp;hellip;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;image and registry&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;other-stuffs&#34;&gt;Other Stuffs&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;迁移&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linux内核抢占</title>
      <link>https://feilengcui008.github.io/post/linux%E5%86%85%E6%A0%B8%E6%8A%A2%E5%8D%A0/</link>
      <pubDate>Sat, 18 Jun 2016 11:02:55 +0800</pubDate>
      <guid>https://feilengcui008.github.io/post/linux%E5%86%85%E6%A0%B8%E6%8A%A2%E5%8D%A0/</guid>
      <description>&lt;p&gt;本文主要介绍内核抢占的相关概念和具体实现，以及抢占对内核调度、内核竞态和同步的一些影响。(所用内核版本3.19.3)&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h4 id=&#34;1-基本概念&#34;&gt;1. 基本概念&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;用户抢占和内核抢占&#xA;&lt;ul&gt;&#xA;&lt;li&gt;用户抢占发生点&#xA;&lt;ul&gt;&#xA;&lt;li&gt;当从系统调用或者中断上下文返回用户态的时候，会检查need_resched标志，如果被设置则会重新选择用户态task执行&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;内核抢占发生点&#xA;&lt;ul&gt;&#xA;&lt;li&gt;当从中断上下文返回内核态的时候，检查need_resched标识以及__preemp_count计数，如果标识被设置，并且可抢占，则会触发调度程序preempt_schedule_irq()&lt;/li&gt;&#xA;&lt;li&gt;内核代码由于阻塞等原因直接或间接显示调用schedule，比如preemp_disable时可能会触发preempt_schedule()&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;本质上内核态中的task是共享一个内核地址空间，在同一个core上，从中断返回的task很可能执行和被抢占的task相同的代码，并且两者同时等待各自的资源释放，也可能两者修改同一共享变量，所以会造成死锁或者竞态等；而对于用户态抢占来说，由于每个用户态进程都有独立的地址空间，所以在从内核代码(系统调用或者中断)返回用户态时，由于是不同地址空间的锁或者共享变量，所以不会出现不同地址空间之间的死锁或者竞态，也就没必要检查__preempt_count，是安全的。__preempt_count主要负责内核抢占计数。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h4 id=&#34;2-内核抢占的实现&#34;&gt;2. 内核抢占的实现&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;percpu变量__preempt_count&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;抢占计数8位, PREEMPT_MASK                     =&amp;gt; 0x000000ff&#xA;软中断计数8位, SOFTIRQ_MASK                   =&amp;gt; 0x0000ff00&#xA;硬中断计数4位, HARDIRQ_MASK                   =&amp;gt; 0x000f0000&#xA;不可屏蔽中断1位, NMI_MASK                     =&amp;gt; 0x00100000&#xA;PREEMPTIVE_ACTIVE(标识内核抢占触发的schedule)  =&amp;gt; 0x00200000&#xA;调度标识1位, PREEMPT_NEED_RESCHED             =&amp;gt; 0x80000000&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;__preempt_count的作用&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;抢占计数&lt;/li&gt;&#xA;&lt;li&gt;判断当前所在上下文&lt;/li&gt;&#xA;&lt;li&gt;重新调度标识&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;thread_info的flags&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;thread_info的flags中有一个是TIF_NEED_RESCHED，在系统调用返回，中断返回，以及preempt_disable的时候会检查是否设置，如果设置并且抢占计数为0(可抢占)，则会触发重新调度schedule()或者preempt_schedule()或者preempt_schedule_irq()。通常在scheduler_tick中会检查是否设置此标识(每个HZ触发一次)，然后在下一次中断返回时检查，如果设置将触发重新调度，而在schedule()中会清除此标识。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;// kernel/sched/core.c&#xA;// 设置thread_info flags和__preempt_count的need_resched标识&#xA;void resched_curr(struct rq *rq)&#xA;{&#xA;  /*省略*/&#xA;    if (cpu == smp_processor_id()) {&#xA;    // 设置thread_info的need_resched标识 &#xA;    set_tsk_need_resched(curr);&#xA;    // 设置抢占计数__preempt_count里的need_resched标识&#xA;    set_preempt_need_resched();&#xA;    return;&#xA;  }&#xA;  /*省略*/&#xA;}&#xA;  &#xA;//在schedule()中清除thread_info和__preempt_count中的need_resched标识&#xA;static void __sched __schedule(void)&#xA;{&#xA;  /*省略*/&#xA;need_resched:&#xA;  // 关抢占读取percpu变量中当前cpu id，运行队列&#xA;  preempt_disable();&#xA;  cpu = smp_processor_id(); &#xA;  rq = cpu_rq(cpu);&#xA;  rcu_note_context_switch();&#xA;  prev = rq-&amp;gt;curr;&#xA;  /*省略*/&#xA;    //关闭本地中断，关闭抢占，获取rq自旋锁&#xA;  raw_spin_lock_irq(&amp;amp;rq-&amp;gt;lock);&#xA;  switch_count = &amp;amp;prev-&amp;gt;nivcsw;&#xA;  // PREEMPT_ACTIVE 0x00200000&#xA;  // preempt_count = __preempt_count &amp;amp; (~(0x80000000))&#xA;  // 如果进程没有处于running的状态或者设置了PREEMPT_ACTIVE标识&#xA;  //(即本次schedule是由于内核抢占导致)，则不会将当前进程移出队列&#xA;  // 此处PREEMPT_ACTIVE的标识是由中断返回内核空间时调用&#xA;  // preempt_schdule_irq或者内核空间调用preempt_schedule&#xA;  // 而设置的，表明是由于内核抢占导致的schedule，此时不会将当前&#xA;  // 进程从运行队列取出，因为有可能其再也无法重新运行。&#xA;  if (prev-&amp;gt;state &amp;amp;&amp;amp; !(preempt_count() &amp;amp; PREEMPT_ACTIVE)) {&#xA;    // 如果有信号不移出run_queue&#xA;    if (unlikely(signal_pending_state(prev-&amp;gt;state, prev))) {&#xA;      prev-&amp;gt;state = TASK_RUNNING;&#xA;    } else { // 否则移除队列让其睡眠&#xA;      deactivate_task(rq, prev, DEQUEUE_SLEEP);&#xA;      prev-&amp;gt;on_rq = 0;&#xA;      // 是否唤醒一个工作队列内核线程&#xA;      if (prev-&amp;gt;flags &amp;amp; PF_WQ_WORKER) {&#xA;        struct task_struct *to_wakeup;&#xA;&#xA;        to_wakeup = wq_worker_sleeping(prev, cpu);&#xA;        if (to_wakeup)&#xA;          try_to_wake_up_local(to_wakeup);&#xA;      }&#xA;    }&#xA;    switch_count = &amp;amp;prev-&amp;gt;nvcsw;&#xA;  }&#xA;    /*省略*/&#xA;  next = pick_next_task(rq, prev);&#xA;  // 清除之前task的need_resched标识&#xA;  clear_tsk_need_resched(prev);&#xA;    // 清除抢占计数的need_resched标识&#xA;  clear_preempt_need_resched();&#xA;  rq-&amp;gt;skip_clock_update = 0;&#xA;  // 不是当前进程，切换上下文&#xA;  if (likely(prev != next)) {&#xA;    rq-&amp;gt;nr_switches++;&#xA;    rq-&amp;gt;curr = next;&#xA;    ++*switch_count;&#xA;    rq = context_switch(rq, prev, next);&#xA;    cpu = cpu_of(rq);&#xA;  } else&#xA;    raw_spin_unlock_irq(&amp;amp;rq-&amp;gt;lock);&#xA;  post_schedule(rq);&#xA;  // 重新开抢占&#xA;  sched_preempt_enable_no_resched();&#xA;  // 再次检查need_resched&#xA;  if (need_resched())&#xA;    goto need_resched;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&#xA;&lt;li&gt;__preempt_count的相关操作&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&#xA;/////// need_resched标识相关 ///////&#xA;&#xA;// PREEMPT_NEED_RESCHED位如果是0表示需要调度&#xA;#define PREEMPT_NEED_RESCHED 0x80000000 &#xA;&#xA;static __always_inline void set_preempt_need_resched(void)&#xA;{&#xA;  // __preempt_count最高位清零表示need_resched&#xA;  raw_cpu_and_4(__preempt_count, ~PREEMPT_NEED_RESCHED);&#xA;}&#xA;&#xA;static __always_inline void clear_preempt_need_resched(void)&#xA;{&#xA;  // __preempt_count最高位置位&#xA;  raw_cpu_or_4(__preempt_count, PREEMPT_NEED_RESCHED);&#xA;}&#xA;&#xA;static __always_inline bool test_preempt_need_resched(void)&#xA;{&#xA;  return !(raw_cpu_read_4(__preempt_count) &amp;amp; PREEMPT_NEED_RESCHED);&#xA;}&#xA;&#xA;// 是否需要重新调度，两个条件：1. 抢占计数为0；2. 最高位清零&#xA;static __always_inline bool should_resched(void)&#xA;{&#xA;  return unlikely(!raw_cpu_read_4(__preempt_count));&#xA;}&#xA;&#xA;////////// 抢占计数相关 ////////&#xA;&#xA;#define PREEMPT_ENABLED (0 + PREEMPT_NEED_RESCHED)&#xA;#define PREEMPT_DISABLE (1 + PREEMPT_ENABLED)&#xA;// 读取__preempt_count，忽略need_resched标识位&#xA;static __always_inline int preempt_count(void)&#xA;{&#xA;  return raw_cpu_read_4(__preempt_count) &amp;amp; ~PREEMPT_NEED_RESCHED;&#xA;}&#xA;static __always_inline void __preempt_count_add(int val)&#xA;{&#xA;  raw_cpu_add_4(__preempt_count, val);&#xA;}&#xA;static __always_inline void __preempt_count_sub(int val)&#xA;{&#xA;  raw_cpu_add_4(__preempt_count, -val);&#xA;}&#xA;// 抢占计数加1关闭抢占&#xA;#define preempt_disable() \&#xA;do { \&#xA;  preempt_count_inc(); \&#xA;  barrier(); \&#xA;} while (0)&#xA;// 重新开启抢占，并测试是否需要重新调度&#xA;#define preempt_enable() \&#xA;do { \&#xA;  barrier(); \&#xA;  if (unlikely(preempt_count_dec_and_test())) \&#xA;    __preempt_schedule(); \&#xA;} while (0)&#xA;&#xA;// 抢占并重新调度&#xA;// 这里设置PREEMPT_ACTIVE会对schdule()中的行为有影响&#xA;asmlinkage __visible void __sched notrace preempt_schedule(void)&#xA;{&#xA;  // 如果抢占计数不为0或者没有开中断，则不调度&#xA;  if (likely(!preemptible()))&#xA;    return;&#xA;  do {&#xA;    __preempt_count_add(PREEMPT_ACTIVE);&#xA;    __schedule();&#xA;    __preempt_count_sub(PREEMPT_ACTIVE);&#xA;    barrier();&#xA;  } while (need_resched());&#xA;}&#xA;// 检查thread_info flags&#xA;static __always_inline bool need_resched(void)&#xA;{&#xA;  return unlikely(tif_need_resched());&#xA;}&#xA;&#xA;////// 中断相关 ////////&#xA;&#xA;// 硬件中断计数&#xA;#define hardirq_count() (preempt_count() &amp;amp; HARDIRQ_MASK)&#xA;// 软中断计数&#xA;#define softirq_count() (preempt_count() &amp;amp; SOFTIRQ_MASK)&#xA;// 中断计数&#xA;#define irq_count() (preempt_count() &amp;amp; (HARDIRQ_MASK | SOFTIRQ_MASK \&#xA;         | NMI_MASK))&#xA;// 是否处于外部中断上下文&#xA;#define in_irq()    (hardirq_count())&#xA;// 是否处于软中断上下文&#xA;#define in_softirq()    (softirq_count())&#xA;// 是否处于中断上下文&#xA;#define in_interrupt()    (irq_count())&#xA;#define in_serving_softirq()  (softirq_count() &amp;amp; SOFTIRQ_OFFSET)&#xA;&#xA;// 是否处于不可屏蔽中断环境&#xA;#define in_nmi()  (preempt_count() &amp;amp; NMI_MASK)&#xA;&#xA;// 是否可抢占 : 抢占计数为0并且没有处在关闭抢占的环境中&#xA;# define preemptible()  (preempt_count() == 0 &amp;amp;&amp;amp; !irqs_disabled())&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;hr&gt;&#xA;&lt;h4 id=&#34;3-系统调用和中断处理流程的实现以及抢占的影响&#34;&gt;3. 系统调用和中断处理流程的实现以及抢占的影响&lt;/h4&gt;&#xA;&lt;p&gt;(arch/x86/kernel/entry_64.S)&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linux内核namespace</title>
      <link>https://feilengcui008.github.io/post/linux%E5%86%85%E6%A0%B8namespace/</link>
      <pubDate>Fri, 10 Jun 2016 19:27:52 +0800</pubDate>
      <guid>https://feilengcui008.github.io/post/linux%E5%86%85%E6%A0%B8namespace/</guid>
      <description>&lt;h3 id=&#34;1-介绍&#34;&gt;1. 介绍&lt;/h3&gt;&#xA;&lt;p&gt;Namespace是Linux内核为容器技术提供的基础设施之一(另一个是cgroups)，包括uts/user/pid/mnt/ipc/net六个(3.13.0的内核)，主要用来做资源的隔离，本质上是全局资源的映射，映射之间独立了自然隔离了。主要涉及到的接口是:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;clone&lt;/li&gt;&#xA;&lt;li&gt;setns&lt;/li&gt;&#xA;&lt;li&gt;unshare&lt;/li&gt;&#xA;&lt;li&gt;/proc/pid/ns, /proc/pid/uid_map, /proc/pid/gid_map等&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;后面会简单分析一下内核源码里面是怎么实现这几个namespace的，并以几个简单系统调用为例，看看namespace是怎么产生影响的，最后简单分析下setns和unshare的实现。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;2-测试流程及代码&#34;&gt;2. 测试流程及代码&lt;/h3&gt;&#xA;&lt;p&gt;下面是一些简单的例子，主要测试uts/pid/user/mnt四个namespace的效果，测试代码主要用到三个进程，一个是clone系统调用执行/bin/bash后的进程，也是生成新的子namespace的初始进程，然后是打开/proc/pid/ns下的namespace链接文件，用setns将第二个可执行文件的进程加入/bin/bash的进程的namespace(容器)，并让其fork出一个子进程，测试pid namespace的差异。值得注意的几个点:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;不同版本的内核setns和unshare对namespace的支持不一样，较老的内核可能只支持ipc/net/uts三个namespace&lt;/li&gt;&#xA;&lt;li&gt;某个进程创建后其pid namespace就固定了，使用setns和unshare改变后，其本身的pid namespace不会改变，只有fork出的子进程的pid namespace改变(改变的是每个进程的nsproxy-&amp;gt;pid_namespace_for_children)&lt;/li&gt;&#xA;&lt;li&gt;用setns添加mnt namespace应该放在其他namespace之后，否则可能出现无法打开/proc/pid/ns/&amp;hellip;的错误&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;// 代码1: 开一些新的namespace(启动新容器)&#xA;#define _GNU_SOURCE&#xA;#include &amp;lt;sys/wait.h&amp;gt;&#xA;#include &amp;lt;sched.h&amp;gt;&#xA;#include &amp;lt;string.h&amp;gt;&#xA;#include &amp;lt;stdio.h&amp;gt;&#xA;#include &amp;lt;stdlib.h&amp;gt;&#xA;#include &amp;lt;unistd.h&amp;gt;&#xA;&#xA;#define errExit(msg)  do { perror(msg); exit(EXIT_FAILURE); \&#xA;} while (0)&#xA;&#xA;/* Start function for cloned child */&#xA;static int childFunc(void *arg)&#xA;{&#xA;  const char *binary = &amp;#34;/bin/bash&amp;#34;;&#xA;  char *const argv[] = {&#xA;    &amp;#34;/bin/bash&amp;#34;,&#xA;    NULL&#xA;  };&#xA;  char *const envp[] = { NULL };&#xA;&#xA;  /* wrappers for execve */&#xA;  // has const char * as argument list&#xA;  // execl &#xA;  // execle  =&amp;gt; has envp&#xA;  // execlp  =&amp;gt; need search PATH &#xA;  &#xA;  // has char *const arr[] as argument list &#xA;  // execv &#xA;  // execvpe =&amp;gt; need search PATH and has envp&#xA;  // execvp  =&amp;gt; need search PATH &#xA;  &#xA;  //int ret = execve(binary, argv, envp);&#xA;  int ret = execv(binary, argv);&#xA;  if (ret &amp;lt; 0) {&#xA;    errExit(&amp;#34;execve error&amp;#34;);&#xA;  }&#xA;  return ret;&#xA;}&#xA;&#xA;#define STACK_SIZE (1024 * 1024)    /* Stack size for cloned child */&#xA;&#xA;int main(int argc, char *argv[])&#xA;{&#xA;  char *stack; &#xA;  char *stackTop;                 &#xA;  pid_t pid;&#xA;  stack = malloc(STACK_SIZE);&#xA;  if (stack == NULL)&#xA;    errExit(&amp;#34;malloc&amp;#34;);&#xA;  stackTop = stack + STACK_SIZE;  /* Assume stack grows downward */&#xA;&#xA;  //pid = clone(childFunc, stackTop, CLONE_NEWUTS | CLONE_NEWNS | CLONE_NEWPID | CLONE_NEWUSER | SIGCHLD, NULL);&#xA;  pid = clone(childFunc, stackTop, CLONE_NEWUTS | CLONE_NEWNS | CLONE_NEWPID | CLONE_NEWUSER | CLONE_NEWIPC | SIGCHLD, NULL);&#xA;//pid = clone(childFunc, stackTop, CLONE_NEWUTS | //CLONE_NEWNS | CLONE_NEWPID | CLONE_NEWUSER | CLONE_NEWIPC //| CLONE_NEWNET | SIGCHLD, NULL);&#xA;  if (pid == -1)&#xA;    errExit(&amp;#34;clone&amp;#34;);&#xA;  printf(&amp;#34;clone() returned %ld\n&amp;#34;, (long) pid);&#xA;&#xA;  if (waitpid(pid, NULL, 0) == -1)  &#xA;    errExit(&amp;#34;waitpid&amp;#34;);&#xA;  printf(&amp;#34;child has terminated\n&amp;#34;);&#xA;&#xA;  exit(EXIT_SUCCESS);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;// 代码2: 使用setns加入新进程&#xA;#define _GNU_SOURCE  // ?&#xA;#include &amp;lt;stdio.h&amp;gt;&#xA;#include &amp;lt;string.h&amp;gt;&#xA;#include &amp;lt;stdlib.h&amp;gt;&#xA;#include &amp;lt;errno.h&amp;gt;&#xA;#include &amp;lt;sys/utsname.h&amp;gt;&#xA;#include &amp;lt;unistd.h&amp;gt;&#xA;#include &amp;lt;sys/types.h&amp;gt;&#xA;#include &amp;lt;sched.h&amp;gt;&#xA;#include &amp;lt;fcntl.h&amp;gt;&#xA;#include &amp;lt;wait.h&amp;gt;&#xA;&#xA;// mainly setns and unshare system calls&#xA;&#xA;/* int setns(int fd, int nstype); */&#xA;&#xA;// 不同版本内核/proc/pid/ns下namespace文件情况&#xA;/*&#xA;   CLONE_NEWCGROUP (since Linux 4.6)&#xA;   fd must refer to a cgroup namespace.&#xA;&#xA;   CLONE_NEWIPC (since Linux 3.0)&#xA;   fd must refer to an IPC namespace.&#xA;&#xA;   CLONE_NEWNET (since Linux 3.0)&#xA;   fd must refer to a network namespace.&#xA;&#xA;   CLONE_NEWNS (since Linux 3.8)&#xA;   fd must refer to a mount namespace.&#xA;&#xA;   CLONE_NEWPID (since Linux 3.8)&#xA;   fd must refer to a descendant PID namespace.&#xA;&#xA;   CLONE_NEWUSER (since Linux 3.8)&#xA;   fd must refer to a user namespace.&#xA;&#xA;   CLONE_NEWUTS (since Linux 3.0)&#xA;   fd must refer to a UTS namespace.&#xA;   */&#xA;&#xA;/* // 特殊的pid namespace &#xA;   CLONE_NEWPID behaves somewhat differently from the other nstype&#xA;values: reassociating the calling thread with a PID namespace changes&#xA;only the PID namespace that child processes of the caller will be&#xA;created in; it does not change the PID namespace of the caller&#xA;itself.  Reassociating with a PID namespace is allowed only if the&#xA;PID namespace specified by fd is a descendant (child, grandchild,&#xA;etc.)  of the PID namespace of the caller.  For further details on&#xA;PID namespaces, see pid_namespaces(7).&#xA;*/&#xA;&#xA;&#xA;/*&#xA;int unshare(int flags);&#xA;CLONE_FILES | CLONE_FS | CLONE_NEWCGROUP | CLONE_NEWIPC | CLONE_NEWNET &#xA;| CLONE_NEWNS | CLONE_NEWPID | CLONE_NEWUSER | CLONE_NEWUTS | CLONE_SYSVSEM&#xA;*/&#xA;&#xA;&#xA;&#xA;#define MAX_PROCPATH_LEN 1024&#xA;&#xA;#define errorExit(msg) \&#xA;  do { fprintf(stderr, &amp;#34;%s in file %s in line %d\n&amp;#34;, msg, __FILE__, __LINE__);\&#xA;    exit(EXIT_FAILURE); } while (0)&#xA;&#xA;void printInfo();&#xA;int openAndSetns(const char *path);&#xA;&#xA;int main(int argc, char *argv[])&#xA;{&#xA;  if (argc &amp;lt; 2) {&#xA;    fprintf(stdout, &amp;#34;usage : execname pid(find namespaces of this process)\n&amp;#34;);&#xA;    return 0;&#xA;  }&#xA;  printInfo();&#xA;&#xA;  fprintf(stdout, &amp;#34;---- setns for uts ----\n&amp;#34;);&#xA;  char uts[MAX_PROCPATH_LEN];&#xA;  snprintf(uts, MAX_PROCPATH_LEN, &amp;#34;/proc/%s/ns/uts&amp;#34;, argv[1]);&#xA;  openAndSetns(uts);&#xA;  printInfo();&#xA;&#xA;  fprintf(stdout, &amp;#34;---- setns for user ----\n&amp;#34;);&#xA;  char user[MAX_PROCPATH_LEN];&#xA;  snprintf(user, MAX_PROCPATH_LEN, &amp;#34;/proc/%s/ns/user&amp;#34;, argv[1]);&#xA;  openAndSetns(user);&#xA;  printInfo();&#xA;&#xA;  // 注意pid namespace的不同行为，只有后续创建的子进程进入setns设置&#xA;  // 的新的pid namespace，本进程不会改变&#xA;  fprintf(stdout, &amp;#34;---- setns for pid ----\n&amp;#34;);&#xA;  char pidpath[MAX_PROCPATH_LEN];&#xA;  snprintf(pidpath, MAX_PROCPATH_LEN, &amp;#34;/proc/%s/ns/pid&amp;#34;, argv[1]);&#xA;  openAndSetns(pidpath);&#xA;  printInfo();&#xA;&#xA;&#xA;  fprintf(stdout, &amp;#34;---- setns for ipc ----\n&amp;#34;);&#xA;  char ipc[MAX_PROCPATH_LEN];&#xA;  snprintf(ipc, MAX_PROCPATH_LEN, &amp;#34;/proc/%s/ns/ipc&amp;#34;, argv[1]);&#xA;  openAndSetns(ipc);&#xA;  printInfo();&#xA;&#xA;  fprintf(stdout, &amp;#34;---- setns for net ----\n&amp;#34;);&#xA;  char net[MAX_PROCPATH_LEN];&#xA;  snprintf(net, MAX_PROCPATH_LEN, &amp;#34;/proc/%s/ns/net&amp;#34;, argv[1]);&#xA;  openAndSetns(net);&#xA;  printInfo();&#xA;&#xA;  // 注意mnt namespace需要放在其他后面，避免mnt namespace改变后&#xA;  // 找不到/proc/pid/ns下的文件&#xA;  fprintf(stdout, &amp;#34;---- setns for mount ----\n&amp;#34;);&#xA;  char mount[MAX_PROCPATH_LEN];&#xA;  snprintf(mount, MAX_PROCPATH_LEN, &amp;#34;/proc/%s/ns/mnt&amp;#34;, argv[1]);&#xA;  openAndSetns(mount);&#xA;  printInfo();&#xA;&#xA;  // 测试子进程的pid namespace&#xA;  int ret = fork();&#xA;  if (-1 == ret) {&#xA;    errorExit(&amp;#34;failed to fork&amp;#34;);&#xA;  } else if (ret == 0) {&#xA;    fprintf(stdout, &amp;#34;********\n&amp;#34;);&#xA;    fprintf(stdout, &amp;#34;in child process\n&amp;#34;);&#xA;    printInfo();&#xA;    fprintf(stdout, &amp;#34;********\n&amp;#34;);&#xA;    for (;;) {&#xA;      sleep(5);&#xA;    }&#xA;  } else {&#xA;    fprintf(stdout, &amp;#34;child pid : %d\n&amp;#34;, ret);&#xA;  }&#xA;  for (;;) {&#xA;    sleep(5);&#xA;  }&#xA;  waitpid(ret, NULL, 0);&#xA;  return 0;&#xA;}&#xA;&#xA;void printInfo()&#xA;{&#xA;  pid_t pid;&#xA;  struct utsname uts;&#xA;  uid_t uid;&#xA;  gid_t gid;&#xA;  // pid namespace &#xA;  pid = getpid();&#xA;  // user namespace &#xA;  uid = getuid();&#xA;  gid = getgid();&#xA;  // uts namespace &#xA;  uname(&amp;amp;uts);&#xA;  fprintf(stdout, &amp;#34;pid : %d\n&amp;#34;, pid);&#xA;  fprintf(stdout, &amp;#34;uid : %d\n&amp;#34;, uid);&#xA;  fprintf(stdout, &amp;#34;gid : %d\n&amp;#34;, gid);&#xA;  fprintf(stdout, &amp;#34;hostname : %s\n&amp;#34;, uts.nodename);&#xA;}&#xA;&#xA;int openAndSetns(const char *path)&#xA;{&#xA;  int ret = open(path, O_RDONLY, 0);&#xA;  if (-1 == ret) {&#xA;    fprintf(stderr, &amp;#34;%s\n&amp;#34;, strerror(errno));&#xA;    errorExit(&amp;#34;failed to open fd&amp;#34;);&#xA;  }&#xA;  if (-1 == (ret = setns(ret, 0))) {&#xA;    fprintf(stderr, &amp;#34;%s\n&amp;#34;, strerror(errno));&#xA;    errorExit(&amp;#34;failed to setns&amp;#34;);&#xA;  }&#xA;  return ret;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;hr&gt;&#xA;&lt;h3 id=&#34;3-测试效果&#34;&gt;3. 测试效果&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;user的效果 : 通过/proc/pid/uid_map和/proc/pid/gid_map设置container外用户id和容器内用户id的映射关系(把这放前面是因为后面hostname和mount需要权限&amp;hellip;)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://img.blog.csdn.net/20160610195657440&#34; alt=&#34;这里写图片描述&#34;&gt;&#xA;&lt;img src=&#34;http://img.blog.csdn.net/20160610195625033&#34; alt=&#34;这里写图片描述&#34;&gt;&#xA;&lt;img src=&#34;http://img.blog.csdn.net/20160610195759722&#34; alt=&#34;这里写图片描述&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linux下的时间</title>
      <link>https://feilengcui008.github.io/post/linux%E4%B8%8B%E7%9A%84%E6%97%B6%E9%97%B4/</link>
      <pubDate>Mon, 16 May 2016 17:03:34 +0800</pubDate>
      <guid>https://feilengcui008.github.io/post/linux%E4%B8%8B%E7%9A%84%E6%97%B6%E9%97%B4/</guid>
      <description>&lt;h3 id=&#34;时钟&#34;&gt;时钟&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;硬件时钟&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;RTC(real time clock)，记录wall clock time，硬件对应到/dev/rtc设备文件，读取设备文件可得到硬件时间&lt;/li&gt;&#xA;&lt;li&gt;读取方式&#xA;&lt;ul&gt;&#xA;&lt;li&gt;通过ioctl&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;#include &amp;lt;linux/rtc.h&amp;gt;&#xA;int ioctl(fd, RTC_request, param);&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;hwclock命令&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;通常内核在boot以及从低电量中恢复时，会读取RTC更新system time&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;软件时钟&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;HZ and jiffies, 由内核维护，对于PC通常HZ配置为 1s / 10ms = 100&lt;/li&gt;&#xA;&lt;li&gt;精度影响select等依赖timeout的系统调用&lt;/li&gt;&#xA;&lt;li&gt;HRT(high-resolution timers). Linux 2.6.21开始，内核支持高精度定时器，不受内核jiffy限制，可以达到硬件时钟的精度。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;外部时钟&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;从网络ntp，原子钟等同步&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;时间&#34;&gt;时间&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;时间类别&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;wall clock time =&amp;gt; 硬件时间&lt;/li&gt;&#xA;&lt;li&gt;real time =&amp;gt; 从某个时间点(比如Epoch)开始的系统时间&lt;/li&gt;&#xA;&lt;li&gt;sys and user time =&amp;gt; 通常指程序在内核态和用户态花的时间&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;时间的表示&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;time_t 从Epoch开始的秒数&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;calendar time 字符串&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;拆分时间 struct tm&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C++&#34; data-lang=&#34;C++&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;struct&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tm&lt;/span&gt; {&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; tm_sec;         &lt;span style=&#34;color:#75715e&#34;&gt;/* seconds */&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; tm_min;         &lt;span style=&#34;color:#75715e&#34;&gt;/* minutes */&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; tm_hour;        &lt;span style=&#34;color:#75715e&#34;&gt;/* hours */&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; tm_mday;        &lt;span style=&#34;color:#75715e&#34;&gt;/* day of the month */&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; tm_mon;         &lt;span style=&#34;color:#75715e&#34;&gt;/* month */&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; tm_year;        &lt;span style=&#34;color:#75715e&#34;&gt;/* year */&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; tm_wday;        &lt;span style=&#34;color:#75715e&#34;&gt;/* day of the week */&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; tm_yday;        &lt;span style=&#34;color:#75715e&#34;&gt;/* day in the year */&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; tm_isdst;       &lt;span style=&#34;color:#75715e&#34;&gt;/* daylight saving time */&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;};&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;struct timeval/struct timespec&lt;/p&gt;</description>
    </item>
    <item>
      <title>分布式一致性协议(三)</title>
      <link>https://feilengcui008.github.io/post/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE-%E4%B8%89/</link>
      <pubDate>Thu, 10 Mar 2016 21:05:15 +0800</pubDate>
      <guid>https://feilengcui008.github.io/post/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE-%E4%B8%89/</guid>
      <description>&lt;p&gt;在上一篇文章中讨论了leader选举对于基本Paxos算法在实际工程应用中的必要性，这一篇文章首先结合raft的选举算法谈谈leader选举的实质和常用方法，然后结合raft算法选举后的日志恢复以及《Paxos Made Simple》里lamport勾勒的multi-paxos的日志恢复来详细分析一下选主后要做的两件重要事情以及俩算法在这块的差异。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;1raft的选主算法以及选主算法的实质&#34;&gt;1.raft的选主算法以及选主算法的实质&lt;/h3&gt;&#xA;&lt;p&gt;前面一篇文章中提到，选主本质上就是分布式共识问题，可以用基本Paxos解决，下面就raft选主算法与基本Paxos的对应关系来说明。&lt;/p&gt;&#xA;&lt;p&gt;关于raft选主的详细描述可以参考原论文&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;raft选主时的term实际上对应基本Paxos中的proposal id&lt;/li&gt;&#xA;&lt;li&gt;raft选主时的要求即每个term期间只能最多有一个leader实际上对应于基本Paxos的每个proposal要么达成决议要么没达成决议&lt;/li&gt;&#xA;&lt;li&gt;raft选主时的随机timeout实际上是为了防止基本Paxos livelock的问题，这也是FLP定理所决定的&lt;/li&gt;&#xA;&lt;li&gt;raft选举时与基本Paxos的区别在于，raft选举不要求在某个term(proposal id)选出一个leader(达成决议后)不需要后续的某个term(proposal id)选出同一台机器作为leader(使用同一个值达成决议)，而是可以每次重新选一个机器(proposal选不同值)，当然我们可以使用一定方法，增大选某台机器的概率，比如为每台机器设置rank值。&lt;/li&gt;&#xA;&lt;li&gt;raft选举时，当candidate和leader接受到更大的term时立即更新term转为follower，在下一次超时前自然不能再提proposal，实际上对应于基本Paxos第一阶段acceptor接收到proposal id更大的proposal时更新proposal id放弃当前的proposal(在选主中实际上就对应放弃我candidate和leader的身份，本质上就是proposer的身份)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;所以选主本质上是可以通过基本Paxos算法来保证的，选主没有完全使用Paxos算法，可以看作使用了Paxos算法的某个子算法解决了比容错分布式一致问题限制稍微小的问题。当然，我们可以在选主时加上额外的限制条件，只要能保证可能选出一个主。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;2选主后日志的同步&#34;&gt;2.选主后日志的同步&lt;/h3&gt;&#xA;&lt;p&gt;选出新的leader后，它至少要负责做两件事情，一件是确定下一次客户请求应该用哪个日志槽位或者说项，另一件是确定整个集群的机器过去已经提交过的最近的项(或者说日志)，确定这两个值的过程实际上就是日志恢复的过程，下面对两种算法具体分析。这里补充一点之前文章漏掉的东西，基本Paxos算法实际上有三个阶段，最后一个阶段是提交阶段，只是通常leader-based算法为了优化网络开销，将第三阶段和第二阶段合并了，而在每次执行第二阶段是带上leader已经提交过的日志号，所以新leader还需要确定最近被提交过的日志，而这种优化也引入了另外的复杂性。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;对于raft来说&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;由于选主时额外的限制条件以及log replication时的consistency check保证(关于这两者是什么东西，不细说，基本上这就是raft简化了multi-paxos最核心的东西吧)，所以每个新leader一定有最新的日志，所以对于下一条日志槽位的选取，只需要读取最后一条日志来判断就行了。关于raft的log replication，后面有机会再说。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;而对于已提交日志的判断，由于存在可能已经形成多数派，也就是在内存中形成了多数派，但是还没有机器commited到磁盘，这时，新的leader无法判断这条日志是已经提交还是没有提交(参见原论文5.4.2节)，raft的做法是不管这条可能被新leader覆盖掉的日志，只需要保证在新的term期间，提交一条日志，那么由于consistency check，自然会提交之前的日志。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;对于multi-paxos来说&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;由于在log replication说，不像raft那样保证一个顺序应答(不能保证线性一致性，能保证顺序一致性)，也就是保证一个日志槽位达成多数派后才接受下一个请求，multi-paxos可以在一个日志槽位还没有达成多数派时并发处理另外一个日志槽位，所以新leader在恢复确认下一个可用日志槽位以及已提交日志时更麻烦。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;lamport原论文描述的方法是，对于明确知道已提交的日志(这一点我们可以通过给每一条已提交日志加一个标示，这样可以减少日志恢复的时间)，不用再次进行基本Paxos的决议，而对于未明确知道已提交的日志，则进行基本Paxos的二个阶段来确认已达成多数派的值，对于中间空洞且之前没有达成过多数派的，直接写一条空操作的日志，至于为什么会产生这种情况，可以参考原论文。一旦所有日志都经过这种方法恢复后，下一个可用日志槽位和最近已提交日志号也就能确定了。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;对比上面两者恢复的过程，我们可以看到raft是怎么简化multi-paxos的。一旦新的leader确定了上面那两件事情，就可以进入正常的log replication阶段了，也就仅仅是多数派的事情了。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;3log-replication客户端交互membership管理leader-lease等&#34;&gt;3.log replication，客户端交互，membership管理，leader lease等&lt;/h3&gt;&#xA;&lt;p&gt;这一节为后面的文章做个铺垫，对于log replication实际上不会涉及太多状态的reason，所以也就比较容易理解，基本上是类似简化的两阶段提交，后面会介绍下raft的log replication。对于客户端交互，leader什么时候返回结果，客户端怎么超时重试，以及怎么保证请求的幂等，membership management，以及leader lease等一些优化手段。&lt;/p&gt;</description>
    </item>
    <item>
      <title>分布式一致性协议(二)</title>
      <link>https://feilengcui008.github.io/post/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE-%E4%BA%8C/</link>
      <pubDate>Wed, 09 Mar 2016 14:19:04 +0800</pubDate>
      <guid>https://feilengcui008.github.io/post/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE-%E4%BA%8C/</guid>
      <description>&lt;p&gt;上一篇文章推导了基本Paxos算法，并引出了在实际使用中其存在的问题，然后说明了leader-based分布式一致性协议的优势。这篇文章分析一下选主的本质，选出一个主对整个算法的影响，采用选主会存在的问题以及基本Paxos协议是怎么样保证这些问题不会影响一致性的。&lt;/p&gt;&#xA;&lt;h3 id=&#34;1为什么选主&#34;&gt;1.为什么选主&lt;/h3&gt;&#xA;&lt;p&gt;至于为什么选主？个人认为有如下原因：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;避免并发决议导致的livelock和新丢失的问题&lt;/li&gt;&#xA;&lt;li&gt;可以采用一定方法在选主时(raft)，选主中或者选主后保证leader上有最新的达成多数派(达成多数派应该用多数派已经将值写入持久化日志来判定)，这样可以优化针对同一个项的读请求，不然每次客户端读请求也需要走一遍基本Paxos&lt;/li&gt;&#xA;&lt;li&gt;选出leader可以保证在一个leader的统治期间内只有这一个leader可以接收客户端请求，发起决议(至于脑裂的问题，后面会分析)，&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;2不同的选主算法其本质是什么&#34;&gt;2.不同的选主算法，其本质是什么？&lt;/h3&gt;&#xA;&lt;p&gt;前面说了在一个leader统治期间内，不可能存在多个leader同时对一个项达成多数派(如果一个leader也没有自然满足，包括脑裂后面会分析到也是满足的)，但是对于选主本身来说，实际上其本质上就是一个分布式一致性问题，并且可能有多个proposer并发提出选主决议，所以可以使用基本Paxos来解决，这就回到了基本的Paxos算法了！所以我们需要为每次选主决议编号，比如raft算法的term，这个实际上就对应基本Paxos算法的proposal id。&lt;/p&gt;&#xA;&lt;h3 id=&#34;3选主后对整个算法造成什么影响&#34;&gt;3.选主后对整个算法造成什么影响？&lt;/h3&gt;&#xA;&lt;p&gt;前面提到了&amp;quot;选出leader可以保证在一个leader的统治期间内只有这一个leader可以接收客户端请求，发起决议&amp;quot;。这样实际上基本Paxos的第一阶段prepare就没有必要了，因为对于下一个项来说，在这个leader统治期内，在达成多数派之前，不可能有其他人提出决议并达成多数派，所以可以直接使用客户端的值进入第二阶段accept。&lt;/p&gt;&#xA;&lt;h3 id=&#34;4选主可能会导致的问题&#34;&gt;4.选主可能会导致的问题？&lt;/h3&gt;&#xA;&lt;p&gt;最大的问题应该是脑裂了，也就是说可能存在多个分区多个leader接收客户端响应，但是由于多数派的限制，只能最多有一个分区能达成多数派。我们假设最简单的情况，A/B/C/D/E五台机器，两个分区P1有三台A/B/C和P2有两台D/E，那么可能的情况是：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;(1).P1有leader；P2没有leader&lt;/li&gt;&#xA;&lt;li&gt;(2).P1有leader；P2也有leader&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;显然由于多数派的限制，只有P1可能达成决议&lt;/p&gt;&#xA;&lt;h3 id=&#34;5新的leader选出来后的操作&#34;&gt;5.新的leader选出来后的操作&lt;/h3&gt;&#xA;&lt;p&gt;一般来说，新的leader选出来后，我们需要对leader进行日志恢复，以便leader决定下一次客户端请求的时候该用哪个日志槽位或者说哪个项吧，这里也是不同的算法差异较大的地方，比如raft，viewstamped replication，zab以及lamport 《Paxos Made Simple》里面第三节描述的方法。在lamport论文的描述中，还是采用基本的Paxos，对未明确知道达成多数派的项重新走一遍基本Paxos算法，具体可以参照原论文，细节还是挺多。对于raft来说，由于其保证日志是连续的，且保证在选主的时候只选择具有最新的日志的机器，所以选主之后，新的leader上的日志本身就是最新的。&lt;/p&gt;&#xA;&lt;p&gt;下一篇会着重分析在新的leader选举后，新leader怎么恢复日志记录以及怎么确定已提交的日志，这一点还是通过对比lamport在《Paxos Made Simple》第三节提到的方法以及raft中的实现来说明。&lt;/p&gt;</description>
    </item>
    <item>
      <title>分布式一致性协议(一)</title>
      <link>https://feilengcui008.github.io/post/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE-%E4%B8%80/</link>
      <pubDate>Tue, 08 Mar 2016 19:05:46 +0800</pubDate>
      <guid>https://feilengcui008.github.io/post/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE-%E4%B8%80/</guid>
      <description>&lt;p&gt;这一篇文章主要介绍一下分布式共识、分布式容错一致性协议的背景以及Paxos算法。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;1-分布式系统基本概念&#34;&gt;1. 分布式系统基本概念&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;分布式系统的基本特点&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;部分故障&#xA;&lt;ul&gt;&#xA;&lt;li&gt;容错&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;没有全局时钟&#xA;&lt;ul&gt;&#xA;&lt;li&gt;事件定序 : 原子时钟，Lamport Clock，Vector Clock等&lt;/li&gt;&#xA;&lt;li&gt;副本一致性问题 : 通常为了保证容错，需要使用多个副本，副本之间的复制需要保证强一致&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;通信延时影响性能和扩展性&#xA;&lt;ul&gt;&#xA;&lt;li&gt;保证系统正确性下较少消息传递，减少共享状态，使用缓存等等&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;系统模型&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;同步和异步&#xA;&lt;ul&gt;&#xA;&lt;li&gt;同步&lt;/li&gt;&#xA;&lt;li&gt;异步(执行时间和消息传递时间没有上限)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;网络模型&#xA;&lt;ul&gt;&#xA;&lt;li&gt;可靠&lt;/li&gt;&#xA;&lt;li&gt;消息丢失，重复传递，消息乱序&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;故障模型&#xA;&lt;ul&gt;&#xA;&lt;li&gt;crash-failure fault&lt;/li&gt;&#xA;&lt;li&gt;byzantine fault&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;一致性&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;data-central&#xA;&lt;ul&gt;&#xA;&lt;li&gt;严格一致性(strict consistency)&lt;/li&gt;&#xA;&lt;li&gt;线性一致性(linear consistency)&lt;/li&gt;&#xA;&lt;li&gt;顺序一致性(sequential consistency)&lt;/li&gt;&#xA;&lt;li&gt;因果一致性(casual consistency)&lt;/li&gt;&#xA;&lt;li&gt;弱一致性(weak consistency)&lt;/li&gt;&#xA;&lt;li&gt;最终一致性(eventual consistency)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;client-central&#xA;&lt;ul&gt;&#xA;&lt;li&gt;单调读一致性(Monotonic Reads Consistency)&lt;/li&gt;&#xA;&lt;li&gt;单调写一致性(Monotonic Writes Consistency)&lt;/li&gt;&#xA;&lt;li&gt;读写一致性(Read Your Writes Consistency)&lt;/li&gt;&#xA;&lt;li&gt;写读一致性(Write Follows Read Consistency)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;其他&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;2分布式共识问题及容错分布式一致性协议&#34;&gt;2.分布式共识问题及容错分布式一致性协议&lt;/h3&gt;&#xA;&lt;p&gt;导致对Paxos理解困难的一个原因是对分布式共识问题本身没有较好的理解。先举个简单例子，然后再说明其需要满足的safety和liveness条件。&lt;/p&gt;&#xA;&lt;p&gt;例子：多个人在食堂决定吃什么菜，不能事先商量好，每个人都可以同时提出一样菜，中间可能有些人临时去上厕所了，待会重新回来，要保证的是最终只有一种菜被接受，而且重新回来的人在需要的时候能够知道这次吃饭到底吃的是什么菜。这里需要注意的是：“同时”说明并发的，有些提议的值可能被覆盖的；“有人临时上厕所”说明需要容错，即在机器挂掉下的分布式一致；“重新回来”说明机器recover后能知道之前决议的结果；&lt;/p&gt;&#xA;&lt;p&gt;分布式共识问题通常需要满足Safety和Liveness的要求，具体来说就是：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Safety&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;只有被提出的值才有可能通过决议&lt;/li&gt;&#xA;&lt;li&gt;最终只有一个值被接受&lt;/li&gt;&#xA;&lt;li&gt;一个参与者只有在决议达成之后才可能知道决议的值&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Liveness&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;最终能对某个值达成决议&lt;/li&gt;&#xA;&lt;li&gt;如果有一个值达成了决议，那么这个值能最终被参与者学习到&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;对于Liveness的问题想多说点，在FLP定理中讨论的模型是完全异步，crash-failure fault但网络可靠这种假设比较严格的模型，并证明了在此系统模型下不存在完整的分布式一致性算法能解决分布式共识问题(注意是完整，如果我们放弃一些Safety或者Liveness的要求，比如保证严格的Safety而使用随机化等方法保证一定概率的Liveness，这样的算法是能实现的，而这也是Paxos一类算法的取舍，毕竟放弃了Safety没太大意义了），而通常像Paxos和类Paxos算法讨论的模型比FLP中的模型更松：完全异步，网络不可靠，crash-failure fault甚至byzantine fault，所以Paxos类算法本质上也没办法完美解决Liveness的问题，Lamport的原始论文中只提到选主(选出distinguished proposer)来解决这个问题，但是至于选主本身的Liveness和容错问题并没有详细讨论，这在后面选主相关部分还会涉及到。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linux内核协议栈socket接口层</title>
      <link>https://feilengcui008.github.io/post/linux%E5%86%85%E6%A0%B8%E5%8D%8F%E8%AE%AE%E6%A0%88socket%E6%8E%A5%E5%8F%A3%E5%B1%82/</link>
      <pubDate>Sat, 31 Oct 2015 12:57:00 +0800</pubDate>
      <guid>https://feilengcui008.github.io/post/linux%E5%86%85%E6%A0%B8%E5%8D%8F%E8%AE%AE%E6%A0%88socket%E6%8E%A5%E5%8F%A3%E5%B1%82/</guid>
      <description>&lt;p&gt;本文接上一篇&lt;a href=&#34;http://blog.csdn.net/feilengcui008/article/details/49509993&#34;&gt;Linux内核协议栈-初始化流程分析&lt;/a&gt;，在上一篇中主要分析了了Linux内核协议栈涉及到的关键初始化函数，在这一篇文章中将分析协议栈的BSD socket和到传输层的流程。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;1.准备&#xA;协议的基本分层：&#xA;(A代表socket的某个系统调用)&#xA;BSD socket system calls A =&amp;gt; proto_ops-&amp;gt;A  =&amp;gt; sock-&amp;gt;A =&amp;gt; tcp_prot =&amp;gt; A&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;BSD socket层和具体协议族某个类型的联系是通过struct proto_ops，在include/linux/net.h中定义了不同协议族如af_inet，af_unix等的通用操作函数指针的结构体struct proto_ops，具体的定义有各个协议族的某个类型的子模块自己完成。比如ipv4/af_inet.c中定义的af_inet family的tcp/udp等相应的struct proto_ops。&lt;/li&gt;&#xA;&lt;li&gt;由于对于每个family的不同类型，其针对socket的某些需求可能不同，所以抽了一层struct sock出来，sock-&amp;gt;sk_prot挂接到具体tcp/udp等传输层的struct proto上(具体定义在ipv4/tcp_ipv4.c,ipv4/udp.c)&lt;/li&gt;&#xA;&lt;li&gt;另外，由于内容比较多，这一篇主要分析socket，bind，listen，accept几个系统调用，下一篇会涉及connect，send，recv等的分析&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;//不同协议族的通用函数hooks&#xA;//比如af_inet相关的定义在ipv4/af_inet.c中&#xA;//除了创建socket为系统调用外，基本针对socket层的操作函数都在这里面&#xA;struct proto_ops {&#xA;  int   family;&#xA;  struct module *owner;&#xA;  int   (*release)   (struct socket *sock);&#xA;  int   (*bind)      (struct socket *sock,&#xA;              struct sockaddr *myaddr,&#xA;              int sockaddr_len);&#xA;  int   (*connect)   (struct socket *sock,&#xA;              struct sockaddr *vaddr,&#xA;              int sockaddr_len, int flags);&#xA;  int   (*socketpair)(struct socket *sock1,&#xA;              struct socket *sock2);&#xA;  int   (*accept)    (struct socket *sock,&#xA;              struct socket *newsock, int flags);&#xA;  int   (*getname)   (struct socket *sock,&#xA;              struct sockaddr *addr,&#xA;              int *sockaddr_len, int peer);&#xA;  unsigned int  (*poll)      (struct file *file, struct socket *sock,&#xA;              struct poll_table_struct *wait);&#xA;  int   (*ioctl)     (struct socket *sock, unsigned int cmd,&#xA;              unsigned long arg);&#xA;#ifdef CONFIG_COMPAT&#xA;  int   (*compat_ioctl) (struct socket *sock, unsigned int cmd,&#xA;              unsigned long arg);&#xA;#endif&#xA;  int   (*listen)    (struct socket *sock, int len);&#xA;  int   (*shutdown)  (struct socket *sock, int flags);&#xA;  int   (*setsockopt)(struct socket *sock, int level,&#xA;              int optname, char __user *optval, unsigned int optlen);&#xA;/*省略部分*/&#xA;};&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;//传输层的proto &#xA;//作为sock-&amp;gt;sk_prot与具体传输层的hooks&#xA;struct proto {&#xA;  void      (*close)(struct sock *sk,&#xA;          long timeout);&#xA;  int     (*connect)(struct sock *sk,&#xA;          struct sockaddr *uaddr,&#xA;          int addr_len);&#xA;  int     (*disconnect)(struct sock *sk, int flags);&#xA;&#xA;  struct sock *   (*accept)(struct sock *sk, int flags, int *err);&#xA;&#xA;  int     (*ioctl)(struct sock *sk, int cmd,&#xA;           unsigned long arg);&#xA;  int     (*init)(struct sock *sk);&#xA;  void      (*destroy)(struct sock *sk);&#xA;  void      (*shutdown)(struct sock *sk, int how);&#xA;  int     (*setsockopt)(struct sock *sk, int level,&#xA;          int optname, char __user *optval,&#xA;          unsigned int optlen);&#xA;  int     (*getsockopt)(struct sock *sk, int level,&#xA;          int optname, char __user *optval,&#xA;          int __user *option);&#xA;#ifdef CONFIG_COMPAT&#xA;  int     (*compat_setsockopt)(struct sock *sk,&#xA;          int level,&#xA;          int optname, char __user *optval,&#xA;          unsigned int optlen);&#xA;  int     (*compat_getsockopt)(struct sock *sk,&#xA;          int level,&#xA;          int optname, char __user *optval,&#xA;          int __user *option);&#xA;  int     (*compat_ioctl)(struct sock *sk,&#xA;          unsigned int cmd, unsigned long arg);&#xA;#endif&#xA;  int     (*sendmsg)(struct kiocb *iocb, struct sock *sk,&#xA;             struct msghdr *msg, size_t len);&#xA;  int     (*recvmsg)(struct kiocb *iocb, struct sock *sk,&#xA;             struct msghdr *msg,&#xA;             size_t len, int noblock, int flags,&#xA;             int *addr_len);&#xA;  int     (*sendpage)(struct sock *sk, struct page *page,&#xA;          int offset, size_t size, int flags);&#xA;  int     (*bind)(struct sock *sk,&#xA;          struct sockaddr *uaddr, int addr_len);&#xA;&#xA;  /*省略部分*/&#xA;};&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;同时附上其他几个关键结构体：&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linux内核协议栈初始化流程</title>
      <link>https://feilengcui008.github.io/post/linux%E5%86%85%E6%A0%B8%E5%8D%8F%E8%AE%AE%E6%A0%88%E5%88%9D%E5%A7%8B%E5%8C%96%E6%B5%81%E7%A8%8B/</link>
      <pubDate>Sat, 31 Oct 2015 10:35:43 +0800</pubDate>
      <guid>https://feilengcui008.github.io/post/linux%E5%86%85%E6%A0%B8%E5%8D%8F%E8%AE%AE%E6%A0%88%E5%88%9D%E5%A7%8B%E5%8C%96%E6%B5%81%E7%A8%8B/</guid>
      <description>&lt;p&gt;本文主要针对Linux-3.19.3版本的内核简单分析内核协议栈初始化涉及到的关键步骤和主要函数。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;1.准备&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Linux内核协议栈本身构建在虚拟文件系统之上，所以对Linux VFS不太了解的可以参考内核源码根目录下Documentation/filesystems/vfs.txt，另外，socket接口层，协议层，设备层的许多数据结构涉及到内存管理，所以对基本虚拟内存管理，slab缓存，页高速缓存不太了解的也可以查阅相关文档。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;源码涉及的主要文件位于net/socket.c，net/core，include/linux/net*&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;2.开始&lt;/p&gt;&#xA;&lt;p&gt;开始分析前，这里有些小技巧可以快速定位到主要的初始化函数，在分析其他子系统源码时也可以采用这个技巧&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;grep _initcall socket.c&#xA;find ./core/ -name &amp;#34;*.c&amp;#34; |xargs cat | grep _initcall&#xA;grep net_inuse_init tags&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;http://img.blog.csdn.net/20151030132956270&#34; alt=&#34;这里写图片描述&#34;&gt;&#xA;&lt;img src=&#34;http://img.blog.csdn.net/20151030140301037&#34; alt=&#34;这里写图片描述&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;这里*__initcall宏是设置初始化函数位于内核代码段.initcall#id.init的位置其中id代表优先级level，小的一般初始化靠前，定义在include/linux/init.h，使用gcc的attribute扩展。而各个level的初始化函数的调用流程基本如下：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;start_kernel -&amp;gt; rest_init -&amp;gt; kernel_init内核线程 -&amp;gt; kernel_init_freeable -&amp;gt; do_basic_setup -&amp;gt; do_initcalls -&amp;gt; do_initcall_level -&amp;gt; do_one_initcall -&amp;gt; *(initcall_t)&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;http://img.blog.csdn.net/20151030133735173&#34; alt=&#34;这里写图片描述&#34;&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;3.详细分析&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;可以看到pure_initcall(net_ns_init)位于0的初始化level，基本不依赖其他的初始化子系统，所以从这个开始&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;//core/net_namespace.c&#xA;//基本上这个函数主要的作用是初始化net结构init_net的一些数据，比如namespace相关，并且调用注册的pernet operations的init钩子针对net进行各自需求的初始化&#xA;pure_initcall(net_ns_init);&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;static int __init net_ns_init(void)&#xA;{&#xA;  struct net_generic *ng;&#xA;  //net namespace相关&#xA;#ifdef CONFIG_NET_NS&#xA;  //分配slab缓存&#xA;  net_cachep = kmem_cache_create(&amp;#34;net_namespace&amp;#34;, sizeof(struct net),SMP_CACHE_BYTES,SLAB_PANIC, NULL);&#xA;&#xA;  /* Create workqueue for cleanup */&#xA;  netns_wq = create_singlethread_workqueue(&amp;#34;netns&amp;#34;);&#xA;  if (!netns_wq)&#xA;    panic(&amp;#34;Could not create netns workq&amp;#34;);&#xA;#endif&#xA;  ng = net_alloc_generic();&#xA;  if (!ng)&#xA;    panic(&amp;#34;Could not allocate generic netns&amp;#34;);&#xA;&#xA;  rcu_assign_pointer(init_net.gen, ng);&#xA;  mutex_lock(&amp;amp;net_mutex);&#xA;    //初始化net namespace相关的对象, 传入初始的namespace init_user_ns&#xA;    //设置net结构的初始namespace&#xA;    //对每个pernet_list中注册的pernet operation，调用其初始化net中的对应数据对象&#xA;  if (setup_net(&amp;amp;init_net, &amp;amp;init_user_ns))&#xA;    panic(&amp;#34;Could not setup the initial network namespace&amp;#34;);&#xA;&#xA;  rtnl_lock();&#xA;    //加入初始net结构的list中&#xA;  list_add_tail_rcu(&amp;amp;init_net.list, &amp;amp;net_namespace_list);&#xA;  rtnl_unlock();&#xA;  mutex_unlock(&amp;amp;net_mutex);&#xA;    //加入pernet_list链表，并且调用pernet operation的init函数初始化net &#xA;  register_pernet_subsys(&amp;amp;net_ns_ops);&#xA;  return 0;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&#xA;&lt;li&gt;下面分析core_init(sock_init)：&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;//socket.c&#xA;//在.initcall1.init代码段注册，以便内核启动时do_initcalls中调用&#xA;//从而注册socket filesystem &#xA;core_initcall(sock_init); /* early initcall */&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;进入core_init(sock_init):&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linux内核页高速缓存</title>
      <link>https://feilengcui008.github.io/post/linux%E5%86%85%E6%A0%B8%E9%A1%B5%E9%AB%98%E9%80%9F%E7%BC%93%E5%AD%98/</link>
      <pubDate>Tue, 20 Oct 2015 18:51:24 +0800</pubDate>
      <guid>https://feilengcui008.github.io/post/linux%E5%86%85%E6%A0%B8%E9%A1%B5%E9%AB%98%E9%80%9F%E7%BC%93%E5%AD%98/</guid>
      <description>&lt;p&gt;Linux内核的VFS是非常经典的抽象，不仅抽象出了flesystem，super_block，inode，dentry，file等结构，而且还提供了像页高速缓存层的通用接口，当然，你可以自己选择是否使用或者自己定制使用方式。本文主要根据自己阅读Linux Kernel 3.19.3系统调用read相关的源码来追踪页高速缓存在整个流程中的痕迹，以常规文件的页高速缓存为例，了解页高速缓存的实现过程，不过于追究具体bio请求的底层细节。另外，在写操作的过程中，页高速缓存的处理流程有所不同(回写)，涉及的东西更多，本文主要关注读操作。Linux VFS相关的重要数据结构及概念可以参考Document目录下的&lt;a href=&#34;https://www.kernel.org/doc/Documentation/filesystems/vfs.txt&#34;&gt;vfs.txt&lt;/a&gt;。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h4 id=&#34;1与页高速缓存相关的重要数据结构&#34;&gt;1.与页高速缓存相关的重要数据结构&lt;/h4&gt;&#xA;&lt;p&gt;除了前述基本数据结构以外，struct address_space 和 struct address_space_operations也在页高速缓存中起着极其重要的作用。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;address_space结构通常被struct page的一个字段指向，主要存放已缓存页面的相关信息，便于快速查找对应文件的缓存页面，具体查找过程是通过radix tree结构的相关操作实现的。&lt;/li&gt;&#xA;&lt;li&gt;address_space_operations结构定义了具体读写页面等操作的钩子，比如生成并发送bio请求，我们可以定制相应的函数实现自己的读写逻辑。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;//include/linux/fs.h&#xA;struct address_space {&#xA;    //指向文件的inode，可能为NULL&#xA;  struct inode    *host;  &#xA;  //存放装有缓存数据的页面&#xA;  struct radix_tree_root  page_tree;  &#xA;  spinlock_t    tree_lock;  &#xA;  atomic_t    i_mmap_writable;&#xA;  struct rb_root    i_mmap; &#xA;  struct list_head  i_mmap_nonlinear;&#xA;  struct rw_semaphore i_mmap_rwsem;&#xA;  //已缓存页的数量&#xA;  unsigned long   nrpages;  &#xA;  unsigned long   nrshadows;  &#xA;  pgoff_t     writeback_index;&#xA;  //address_space相关操作，定义了具体读写页面的钩子&#xA;  const struct address_space_operations *a_ops; &#xA;  unsigned long   flags;  &#xA;  struct backing_dev_info *backing_dev_info; &#xA;  spinlock_t    private_lock; &#xA;  struct list_head  private_list; &#xA;  void      *private_data;&#xA;} __attribute__((aligned(sizeof(long))));&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;//include/linux/fs.h &#xA;struct address_space_operations {&#xA;    //具体写页面的操作&#xA;  int (*writepage)(struct page *page, struct writeback_control *wbc);&#xA;  //具体读页面的操作&#xA;  int (*readpage)(struct file *, struct page *);&#xA;  int (*writepages)(struct address_space *, struct writeback_control *);&#xA;    //标记页面脏&#xA;  int (*set_page_dirty)(struct page *page);&#xA;  int (*readpages)(struct file *filp, struct address_space *mapping, struct list_head *pages, unsigned nr_pages);&#xA;  int (*write_begin)(struct file *, struct address_space  *mapping, loff_t pos, unsigned len, unsigned flags, struct page **pagep, void **fsdata);&#xA;  int (*write_end)(struct file *, struct address_space *mapping, loff_t pos, unsigned len, unsigned copied, struct page *page, void *fsdata);&#xA;  sector_t (*bmap)(struct address_space *, sector_t);&#xA;  void (*invalidatepage) (struct page *, unsigned int, unsigned int);&#xA;  int (*releasepage) (struct page *, gfp_t);&#xA;  void (*freepage)(struct page *);&#xA;  ssize_t (*direct_IO)(int, struct kiocb *, struct iov_iter *iter, loff_t offset);&#xA;  int (*get_xip_mem)(struct address_space *, pgoff_t, int, void **, unsigned long *);&#xA;&#xA;  int (*migratepage) (struct address_space *, struct page *, struct page *, enum migrate_mode);&#xA;  int (*launder_page) (struct page *);&#xA;  int (*is_partially_uptodate) (struct page *, unsigned long, unsigned long);&#xA;  void (*is_dirty_writeback) (struct page *, bool *, bool *);&#xA;  int (*error_remove_page)(struct address_space *, struct page *);&#xA;  /* swapfile support */&#xA;  int (*swap_activate)(struct swap_info_struct *sis, struct file *file, sector_t *span);&#xA;  void (*swap_deactivate)(struct file *file);&#xA;};&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;hr&gt;&#xA;&lt;h4 id=&#34;2系统调用read流程与页高速缓存相关代码分析&#34;&gt;2.系统调用read流程与页高速缓存相关代码分析&lt;/h4&gt;&#xA;&lt;p&gt;关于挂载和打开文件的操作，不赘述(涉及的细节也很多&amp;hellip;)，(极其)简陋地理解，挂载返回挂载点的root dentry，并且读取磁盘数据生成了super_block链接到全局超级块链表中，这样，当前进程就可以通过root dentry找到其inode，从而找到并生成其子树的dentry和inode信息，从而实现查找路径的逻辑。打开文件简单理解就是分配fd，通过dentry将file结构与对应inode挂接，最后安装到进程的打开文件数组中，这里假设已经成功打开文件，返回了fd，我们从系统调用read开始。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linux内核内存访问与缺页中断</title>
      <link>https://feilengcui008.github.io/post/linux%E5%86%85%E6%A0%B8%E5%86%85%E5%AD%98%E8%AE%BF%E9%97%AE%E4%B8%8E%E7%BC%BA%E9%A1%B5%E4%B8%AD%E6%96%AD/</link>
      <pubDate>Fri, 16 Oct 2015 18:12:15 +0800</pubDate>
      <guid>https://feilengcui008.github.io/post/linux%E5%86%85%E6%A0%B8%E5%86%85%E5%AD%98%E8%AE%BF%E9%97%AE%E4%B8%8E%E7%BC%BA%E9%A1%B5%E4%B8%AD%E6%96%AD/</guid>
      <description>&lt;p&gt;简单描述了x86 32位体系结构下Linux内核的用户进程和内核线程的线性地址空间和物理内存的联系，分析了高端内存的引入与缺页中断的具体处理流程。先介绍了用户态进程的执行流程，然后对比了内核线程，引入高端内存的概念，最后分析了缺页中断的流程。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;用户进程&#xA;fork之后的用户态进程已经建立好了所需的数据结构，比如task struct，thread info，mm struct等，将编译链接好的可执行程序的地址区域与进程结构中内存区域做好映射，等开始执行的时候，访问并未经过映射的用户地址空间，会发生缺页中断，然后内核态的对应中断处理程序负责分配page，并将用户进程空间导致缺页的地址与page关联，然后检查是否有相同程序文件的buffer，因为可能其他进程执行同一个程序文件，已经将程序读到buffer里边了，如果没有，则将磁盘上的程序部分读到buffer，而buffer head通常是与分配的页面相关联的，所以实际上会读到对应页面代表的物理内存之中，返回到用户态导致缺页的地址继续执行，此时经过mmu的翻译，用户态地址成功映射到对应页面和物理地址，然后读取指令执行。在上述过程中，如果由于内存耗尽或者权限的问题，可能会返回-NOMEM或segment fault错误给用户态进程。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;内核线程&#xA;没有独立的mm结构，所有内核线程共享一个内核地址空间与内核页表，由于为了方便系统调用等，在用户态进程规定了内核的地址空间是高1G的线性地址，而低3G线性地址空间供用户态使用。注意这部分是和用户态进程的线性地址是重合的，经过mmu的翻译，会转换到相同的物理地址，即前1G的物理地址（准确来讲后128M某些部分的物理地址可能会变化），内核线程访问内存也是要经过mmu的，所以借助用户态进程的页表，虽然内核有自己的内核页表，但不直接使用（为了减少用户态和内核态页表切换的消耗？），用户进程页表的高1G部分实际上是共享内核页表的映射的，访问高1G的线性地址时能访问到低1G的物理地址。而且，由于从用户进程角度看，内核地址空间只有3G－4G这一段（内核是无法直接访问0－3G的线性地址空间的，因为这一段是用户进程所有，一方面如果内核直接读写0－3G的线性地址可能会毁坏进程数据结构，另一方面，不同用户态进程线性地址空间实际映射到不同的物理内存地址，所以可能此刻内核线程借助这个用户态进程的页表成功映射到某个物理地址，但是到下一刻，借助下一个用户态进程的页表，相同的线性地址就可能映射到不同的物理内存地址了）。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;高端内存&#xA;那么，如何让内核访问到大于1G的物理内存？由此引入高端内存的概念，基本思路就是将3G－4G这1G的内核线性地址空间（从用户进程的角度看，从内核线程的角度看是0－1G）取出一部分挪作他用，而不是固定映射，即重用部分内核线性地址空间，映射到1G之上的物理内存。所以，对于x86 32位体系上的Linux内核将3G－4G的线性地址空间分为0－896m和896m－1G的部分，前面部分使用固定映射，当内核使用进程页表访问3G－3G＋896m的线性地址时，不会发生缺页中断，但是当访问3G＋896m以上的线性地址时，可能由于内核页表被更新，而进程页表还未和内核页表同步，此时会发生内核地址空间的缺页中断，从而将内核页表同步到当前进程页表。注意，使用vmalloc分配内存的时候，可能已经设置好了内核页表，等到下一次借助进程页表访问内核空间地址发生缺页时才会触发内核页表和当前页表的同步。&#xA;Linux x86 32位下的线性地址空间与物理地址空间&#xA;(图片出自《understanding the linux virtual memory manager》)&#xA;&lt;img src=&#34;http://img.blog.csdn.net/20151016181439699&#34; alt=&#34;这里写图片描述&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;缺页&#xA;page fault的处理过程如下：在用户空间上下文和内核上下文下都可能访问缺页的线性地址导致缺页中断，但有些情况没有实际意义。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;如果缺页地址位于内核线性地址空间&#xA;&lt;ul&gt;&#xA;&lt;li&gt;如果在vmalloc区，则同步内核页表和用户进程页表，否则挂掉。注意此处未分具体上下文&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;如果发生在中断上下文或者!mm，则检查exception table，如果没有则挂掉。&lt;/li&gt;&#xA;&lt;li&gt;如果缺页地址发生在用户进程线性地址空间&#xA;&lt;ul&gt;&#xA;&lt;li&gt;如果在内核上下文，则查exception table，如果没有，则挂掉。这种情况没多大实际意义&lt;/li&gt;&#xA;&lt;li&gt;如果在用户进程上下文&#xA;&lt;ul&gt;&#xA;&lt;li&gt;查找vma，找到，先判断是否需要栈扩张，否则进入通常的处理流程&lt;/li&gt;&#xA;&lt;li&gt;查找vma，未找到，bad area，通常返回segment fault&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;具体的缺页中断流程图及代码如下：&#xA;(图片出自《understanding the linux virtual memory manager》)&#xA;&lt;img src=&#34;http://img.blog.csdn.net/20151016181719231&#34; alt=&#34;这里写图片描述&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;（Linux 3.19.3 arch/x86/mm/fault.c 1044）&#xA;/*&#xA; * This routine handles page faults.  It determines the address,&#xA; * and the problem, and then passes it off to one of the appropriate&#xA; * routines.&#xA; *&#xA; * This function must have noinline because both callers&#xA; * {,trace_}do_page_fault() have notrace on. Having this an actual function&#xA; * guarantees there&amp;#39;s a function trace entry.&#xA; */&#xA;&#xA;//处理缺页中断&#xA;//参数：寄存器值，错误码，缺页地址&#xA;static noinline void&#xA;__do_page_fault(struct pt_regs *regs, unsigned long error_code,&#xA;    unsigned long address)&#xA;{&#xA;  struct vm_area_struct *vma;&#xA;  struct task_struct *tsk;&#xA;  struct mm_struct *mm;&#xA;  int fault, major = 0;&#xA;  unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;&#xA;&#xA;  tsk = current;&#xA;  mm = tsk-&amp;gt;mm;&#xA;&#xA;  /*&#xA;   * Detect and handle instructions that would cause a page fault for&#xA;   * both a tracked kernel page and a userspace page.&#xA;   */&#xA;  if (kmemcheck_active(regs))&#xA;    kmemcheck_hide(regs);&#xA;  prefetchw(&amp;amp;mm-&amp;gt;mmap_sem);&#xA;&#xA;  if (unlikely(kmmio_fault(regs, address)))&#xA;    return;&#xA;&#xA;  /*&#xA;   * We fault-in kernel-space virtual memory on-demand. The&#xA;   * &amp;#39;reference&amp;#39; page table is init_mm.pgd.&#xA;   *&#xA;   * NOTE! We MUST NOT take any locks for this case. We may&#xA;   * be in an interrupt or a critical region, and should&#xA;   * only copy the information from the master page table,&#xA;   * nothing more.&#xA;   *&#xA;   * This verifies that the fault happens in kernel space&#xA;   * (error_code &amp;amp; 4) == 0, and that the fault was not a&#xA;   * protection error (error_code &amp;amp; 9) == 0.&#xA;   */&#xA;&#xA;    //如果缺页地址位于内核空间&#xA;  if (unlikely(fault_in_kernel_space(address))) {&#xA;    if (!(error_code &amp;amp; (PF_RSVD | PF_USER | PF_PROT))) { //位于内核上下文&#xA;      if (vmalloc_fault(address) &amp;gt;= 0) //如果位于vmalloc区域 vmalloc_sync_one同步内核页表进程页表 &#xA;        return;&#xA;&#xA;      if (kmemcheck_fault(regs, address, error_code))&#xA;        return;&#xA;    }&#xA;&#xA;    /* Can handle a stale RO-&amp;gt;RW TLB: */&#xA;    if (spurious_fault(error_code, address))&#xA;      return;&#xA;&#xA;    /* kprobes don&amp;#39;t want to hook the spurious faults: */&#xA;    if (kprobes_fault(regs))&#xA;      return;&#xA;    /*&#xA;     * Don&amp;#39;t take the mm semaphore here. If we fixup a prefetch&#xA;     * fault we could otherwise deadlock:&#xA;     */&#xA;    bad_area_nosemaphore(regs, error_code, address);&#xA;&#xA;    return;&#xA;  }&#xA;&#xA;&#xA;&#xA;  /* kprobes don&amp;#39;t want to hook the spurious faults: */&#xA;  if (unlikely(kprobes_fault(regs)))&#xA;    return;&#xA;&#xA;  if (unlikely(error_code &amp;amp; PF_RSVD))&#xA;    pgtable_bad(regs, error_code, address);&#xA;&#xA;  if (unlikely(smap_violation(error_code, regs))) {&#xA;    bad_area_nosemaphore(regs, error_code, address);&#xA;    return;&#xA;  }&#xA;&#xA;  /*&#xA;   * If we&amp;#39;re in an interrupt, have no user context or are running&#xA;   * in an atomic region then we must not take the fault:&#xA;   */&#xA;&#xA;    //如果位于中断上下文或者!mm, 出错&#xA;  if (unlikely(in_atomic() || !mm)) {&#xA;    bad_area_nosemaphore(regs, error_code, address);&#xA;    return;&#xA;  }&#xA;&#xA;  /*&#xA;   * It&amp;#39;s safe to allow irq&amp;#39;s after cr2 has been saved and the&#xA;   * vmalloc fault has been handled.&#xA;   *&#xA;   * User-mode registers count as a user access even for any&#xA;   * potential system fault or CPU buglet:&#xA;   */&#xA;  if (user_mode_vm(regs)) {&#xA;    local_irq_enable();&#xA;    error_code |= PF_USER;&#xA;    flags |= FAULT_FLAG_USER;&#xA;  } else {&#xA;    if (regs-&amp;gt;flags &amp;amp; X86_EFLAGS_IF)&#xA;      local_irq_enable();&#xA;  }&#xA;&#xA;  perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, address);&#xA;&#xA;  if (error_code &amp;amp; PF_WRITE)&#xA;    flags |= FAULT_FLAG_WRITE;&#xA;&#xA;  /*&#xA;   * When running in the kernel we expect faults to occur only to&#xA;   * addresses in user space.  All other faults represent errors in&#xA;   * the kernel and should generate an OOPS.  Unfortunately, in the&#xA;   * case of an erroneous fault occurring in a code path which already&#xA;   * holds mmap_sem we will deadlock attempting to validate the fault&#xA;   * against the address space.  Luckily the kernel only validly&#xA;   * references user space from well defined areas of code, which are&#xA;   * listed in the exceptions table.&#xA;   *&#xA;   * As the vast majority of faults will be valid we will only perform&#xA;   * the source reference check when there is a possibility of a&#xA;   * deadlock. Attempt to lock the address space, if we cannot we then&#xA;   * validate the source. If this is invalid we can skip the address&#xA;   * space check, thus avoiding the deadlock:&#xA;   */&#xA;  if (unlikely(!down_read_trylock(&amp;amp;mm-&amp;gt;mmap_sem))) {&#xA;    if ((error_code &amp;amp; PF_USER) == 0 &amp;amp;&amp;amp;&#xA;        !search_exception_tables(regs-&amp;gt;ip)) {&#xA;      bad_area_nosemaphore(regs, error_code, address);&#xA;      return;&#xA;    }&#xA;retry:&#xA;    down_read(&amp;amp;mm-&amp;gt;mmap_sem);&#xA;  } else {&#xA;    /*&#xA;     * The above down_read_trylock() might have succeeded in&#xA;     * which case we&amp;#39;ll have missed the might_sleep() from&#xA;     * down_read():&#xA;     */&#xA;    might_sleep();&#xA;  }&#xA;&#xA;&#xA;    //缺页中断地址位于用户空间 &#xA;    //查找vma &#xA;  vma = find_vma(mm, address);&#xA;&#xA;    //没找到，出错&#xA;  if (unlikely(!vma)) {&#xA;    bad_area(regs, error_code, address);&#xA;    return;&#xA;  }&#xA;&#xA;    //检查在vma的地址的合法性&#xA;  if (likely(vma-&amp;gt;vm_start &amp;lt;= address))&#xA;    goto good_area;&#xA;&#xA;  if (unlikely(!(vma-&amp;gt;vm_flags &amp;amp; VM_GROWSDOWN))) {&#xA;    bad_area(regs, error_code, address);&#xA;    return;&#xA;  }&#xA;&#xA;    //如果在用户上下文&#xA;  if (error_code &amp;amp; PF_USER) {&#xA;    /*&#xA;     * Accessing the stack below %sp is always a bug.&#xA;     * The large cushion allows instructions like enter&#xA;     * and pusha to work. (&amp;#34;enter $65535, $31&amp;#34; pushes&#xA;     * 32 pointers and then decrements %sp by 65535.)&#xA;     */&#xA;    if (unlikely(address + 65536 + 32 * sizeof(unsigned long) &amp;lt; regs-&amp;gt;sp)) {&#xA;      bad_area(regs, error_code, address);&#xA;      return;&#xA;    }&#xA;  }&#xA;&#xA;    //栈扩张&#xA;  if (unlikely(expand_stack(vma, address))) {&#xA;    bad_area(regs, error_code, address);&#xA;    return;&#xA;  }&#xA;&#xA;  /*&#xA;   * Ok, we have a good vm_area for this memory access, so&#xA;   * we can handle it..&#xA;   */&#xA;&#xA;    //vma合法 &#xA;good_area:&#xA;  if (unlikely(access_error(error_code, vma))) {&#xA;    bad_area_access_error(regs, error_code, address);&#xA;    return;&#xA;  }&#xA;&#xA;  /*&#xA;   * If for any reason at all we couldn&amp;#39;t handle the fault,&#xA;   * make sure we exit gracefully rather than endlessly redo&#xA;   * the fault.  Since we never set FAULT_FLAG_RETRY_NOWAIT, if&#xA;   * we get VM_FAULT_RETRY back, the mmap_sem has been unlocked.&#xA;   */&#xA;&#xA;    //调用通用的缺页处理&#xA;  fault = handle_mm_fault(mm, vma, address, flags);&#xA;  major |= fault &amp;amp; VM_FAULT_MAJOR;&#xA;&#xA;  /*&#xA;   * If we need to retry the mmap_sem has already been released,&#xA;   * and if there is a fatal signal pending there is no guarantee&#xA;   * that we made any progress. Handle this case first.&#xA;   */&#xA;  if (unlikely(fault &amp;amp; VM_FAULT_RETRY)) {&#xA;    /* Retry at most once */&#xA;    if (flags &amp;amp; FAULT_FLAG_ALLOW_RETRY) {&#xA;      flags &amp;amp;= ~FAULT_FLAG_ALLOW_RETRY;&#xA;      flags |= FAULT_FLAG_TRIED;&#xA;      if (!fatal_signal_pending(tsk))&#xA;        goto retry;&#xA;    }&#xA;&#xA;    /* User mode? Just return to handle the fatal exception */&#xA;    if (flags &amp;amp; FAULT_FLAG_USER)&#xA;      return;&#xA;&#xA;    /* Not returning to user mode? Handle exceptions or die: */&#xA;    no_context(regs, error_code, address, SIGBUS, BUS_ADRERR);&#xA;    return;&#xA;  }&#xA;&#xA;  up_read(&amp;amp;mm-&amp;gt;mmap_sem);&#xA;  if (unlikely(fault &amp;amp; VM_FAULT_ERROR)) {&#xA;    mm_fault_error(regs, error_code, address, fault);&#xA;    return;&#xA;  }&#xA;&#xA;  /*&#xA;   * Major/minor page fault accounting. If any of the events&#xA;   * returned VM_FAULT_MAJOR, we account it as a major fault.&#xA;   */&#xA;  if (major) {&#xA;    tsk-&amp;gt;maj_flt++;&#xA;    perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, regs, address);&#xA;  } else {&#xA;    tsk-&amp;gt;min_flt++;&#xA;    perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, regs, address);&#xA;  }&#xA;&#xA;  check_v8086_mode(regs, address, tsk);&#xA;}&#xA;NOKPROBE_SYMBOL(__do_page_fault);&#xA;&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    <item>
      <title>Linux内核编译与启动流程</title>
      <link>https://feilengcui008.github.io/post/linux%E5%86%85%E6%A0%B8%E7%BC%96%E8%AF%91%E4%B8%8E%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/</link>
      <pubDate>Sun, 20 Sep 2015 23:27:03 +0800</pubDate>
      <guid>https://feilengcui008.github.io/post/linux%E5%86%85%E6%A0%B8%E7%BC%96%E8%AF%91%E4%B8%8E%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/</guid>
      <description>&lt;h3 id=&#34;编译流程&#34;&gt;编译流程&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;1.编译除arch/x86/boot目录外的其他目录，生成各模块的built_in.o，将静态编译进内核的模块链接成ELF格式的文件vmlinux大约100M，置于源码根目录之下&lt;/li&gt;&#xA;&lt;li&gt;2.通过objcopy将源码根目录下的vmlinux去掉符号等信息置于arch/x86/boot/compressed/vmlinux.bin，大约15M，将其压缩为boot/vmlinux.bin.gz(假设配置的压缩工具是gzip)。&lt;/li&gt;&#xA;&lt;li&gt;3.使用生成的compressed/mkpiggy为compressed/vmlinux.bin.gz添加解压缩程序头，生成compressed/piggy.S，进而生成compressed/piggy.o。&lt;/li&gt;&#xA;&lt;li&gt;4.将compressed/head_64.o，compressed/misc.o，compressed/piggy.o链接为compressed/vmlinux。&lt;/li&gt;&#xA;&lt;li&gt;5.回到boot目录，用objcopy为compressed/vmlinux去掉符号等信息生成boot/vmlinux.bin。&lt;/li&gt;&#xA;&lt;li&gt;6.将boot/setup.bin与boot/vmlinux.bin链接，生成bzImage。&lt;/li&gt;&#xA;&lt;li&gt;7.将各个设置为动态编译的模块链接为内核模块kmo。&lt;/li&gt;&#xA;&lt;li&gt;8.over，maybe copy bzImage to /boot and kmods to /lib.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;下面是内核镜像的组成:&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://img.blog.csdn.net/20150920225335155&#34; alt=&#34;这里写图片描述&#34;&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;启动流程&#34;&gt;启动流程&lt;/h3&gt;&#xA;&lt;p&gt;早期版本的linux内核，如0.1，是通过自带的bootsect.S/setup.S引导，现在需要通过bootloader如grub/lilo来引导。grub的作用大致如下:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;1.grub安装时将stage1 512字节和所在分区文件系统类型对应的stage1.5文件分别写入mbr和之后的扇区。&lt;/li&gt;&#xA;&lt;li&gt;2.bios通过中断加载mbr的512个字节的扇区到0x7c00地址，跳转到0x07c0:0x0000执行。&lt;/li&gt;&#xA;&lt;li&gt;3.通过bios中断加载/boot/grub下的stage2，读取/boot/grub/menu.lst配置文件生成启动引导菜单。&lt;/li&gt;&#xA;&lt;li&gt;4.加载/boot/vmlinuz-xxx-xx与/boot/inird-xxx，将控制权交给内核。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;下面是较为详细的步骤:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;1.BIOS加载硬盘第一个扇区(MBR 512字节)到0000:07C00处，MBR包含引导代码(446字节，比如grub第一阶段的引导代码)，分区表(64字节)信息，结束标志0xAA55(2字节)&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;2.MBR开始执行加载活跃分区，grub第一阶段代码加载1.5阶段的文件系统相关的代码(通过bios中断读活跃分区的扇区)&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;3.有了grub1.5阶段的文件系统相关的模块，接下来读取位于文件系统的grub第2阶段的代码，并执行&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;4.grub第2阶段的代码读取/boot/grub.cfg文件，生成引导菜单&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;5.加载对应的压缩内核vmlinuz和initrd（到哪个地址？）&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;6.实模式下执行vmlinuz头setup部分(bootsect和setup)[head.S[calll main],main.c[go_to_protected_mode]]  ==&amp;gt; 准备进入32位保护模式&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;7.跳转到过渡的32位保护模式执行compressed/head_64.S[startup_32,startup_64]  ==&amp;gt; 进入临时的32位保护模式&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;8.解压缩剩余的vmlinuz，设置页表等，设置64位环境，跳转到解压地址执行  ==&amp;gt; 进入64位&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;9.arch/x86/kernel/head_64.S[startup_64]&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;10.arch/x86/kernel/head64.c[x86_64_start_up]&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;11.init/main.c[start_kernel]&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;12.然后后面的事情就比较好知道了:)&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;ref: Linux source code 3.19.3&lt;/p&gt;&#xA;&lt;/blockquote&gt;</description>
    </item>
    <item>
      <title>Y Combinator</title>
      <link>https://feilengcui008.github.io/post/y-combinator/</link>
      <pubDate>Thu, 14 May 2015 19:26:59 +0800</pubDate>
      <guid>https://feilengcui008.github.io/post/y-combinator/</guid>
      <description>&lt;p&gt;由于匿名函数(通常成为lambda函数但是跟lambda calculus不同)在递归时无法获得函数名，从而导致一些问题，而Y Combinator能很好地解决这个问题。利用不动点的原理，可以利用一般的函数来辅助得到匿名函数的递归形式，从而间接调用无法表达的真正的匿名函数。下面以一个阶乘的递归来说明。&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;#Python版本，后面会加上C++版本&#xA;#F(f) = f&#xA;def F(f,n):&#xA;    return 1 if n==0 else n*f(n-1)&#xA;#或者用lambda&#xA;#F = lambda f,n: 1 if n==0 else n*f(n-1)&#xA;#Y不能用lambda，因为Y会调用自己&#xA;&#xA;#Y(F) = f = F(f) = F(Y(F))&#xA;def Y(F):&#xA;    return lambda n: F(Y(F),n)&#xA;a = Y(F)&#xA;# 6&#xA;print a(3)&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;一些解释：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;F是伪递归函数，将真正的我们假设的匿名函数作为参数，有性质&#xA;F(f)=f.&lt;/li&gt;&#xA;&lt;li&gt;好了以上是我们的已知条件，为了得到f的间接表达式，我们引入Y函数&#xA;使得Y(F) = f&lt;/li&gt;&#xA;&lt;li&gt;所以有Y(F) = f = F(f) = F(Y(F)) （最终的目标是要用YF的组合表示f），所以很容易就得到了Y(F)的函数表达式为F(Y(F))，而Y不是匿名函数，所以能自身调用(其实感觉这东西没想象中那么玄乎～)，上面的代码也就比较好理解了。我们假设的函数只有一个额外参数n，这完全可以自己添加其他参数，只需稍微修改Y中F的调用。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;最后附上一段C++的实现代码：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;//需要C++11支持&#xA;#include &amp;lt;iostream&amp;gt;&#xA;#include &amp;lt;functional&amp;gt;&#xA;//F(f) = f&#xA;int &#xA;F(std::function&amp;lt;int(int)&amp;gt; f, int n)&#xA;{&#xA;    return n==0 ? 1 : n*f(n-1);&#xA;}&#xA;//或者&#xA;//auto F1 = [](std::function&amp;lt;int(int)&amp;gt; f, int n) {&#xA;//    return n==0 ? 1 : n*f(n-1);&#xA;//};&#xA;&#xA;&#xA;//Y(F) = f = F(f) = F(Y(F))&#xA;std::function&amp;lt;int(int)&amp;gt;&#xA;Y(std::function&amp;lt;int(std::function&amp;lt;int(int)&amp;gt;,int)&amp;gt; F)&#xA;{&#xA;    return std::bind(F, std::bind(Y,F), std::placeholders::_1);&#xA;}&#xA;&#xA;int main(int argc, char *argv[])&#xA;{&#xA;    auto f = Y(F);&#xA;    std::cout &amp;lt;&amp;lt; f(3) &amp;lt;&amp;lt; std::endl; //6&#xA;    return 0;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    <item>
      <title>Linux下x86_64进程地址空间布局</title>
      <link>https://feilengcui008.github.io/post/linux%E4%B8%8Bx86_64%E8%BF%9B%E7%A8%8B%E5%9C%B0%E5%9D%80%E7%A9%BA%E9%97%B4%E5%B8%83%E5%B1%80/</link>
      <pubDate>Sun, 08 Mar 2015 23:33:03 +0800</pubDate>
      <guid>https://feilengcui008.github.io/post/linux%E4%B8%8Bx86_64%E8%BF%9B%E7%A8%8B%E5%9C%B0%E5%9D%80%E7%A9%BA%E9%97%B4%E5%B8%83%E5%B1%80/</guid>
      <description>&lt;p&gt;关于Linux 32位内存下的内存空间布局，可以参考这篇博文&lt;a href=&#34;http://blog.csdn.net/embedded_hunter/article/details/6897027&#34;&gt;Linux下C程序进程地址空间局&lt;/a&gt;关于源代码中各种数据类型/代码在elf格式文件以及进程空间中所处的段，在x86_64下和i386下是类似的，本文主要关注vm.legacy_va_layout以及kernel.randomize_va_space参数影响下的进程空间内存宏观布局，以及vDSO和多线程下的堆和栈分布。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;情形一&#34;&gt;情形一：&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;vm_legacy_va_layout=1&lt;/li&gt;&#xA;&lt;li&gt;kernel.randomize_va_space=0&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;此种情况下采用传统内存布局方式，不开启随机化，程序的内存布局&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://img.blog.csdn.net/20150308225850362&#34; alt=&#34;&#34;&gt;&#xA;可以看出:&#xA;代码段：0x400000&amp;ndash;&amp;gt;&#xA;数据段&#xA;堆：向上增长 2aaaaaaab000&amp;ndash;&amp;gt;&#xA;栈：7ffffffde000&amp;lt;&amp;ndash;7ffffffff000&#xA;系统调用：ffffffffff600000-ffffffffff601000&#xA;你可以试一下其他程序，在kernel.randomize_va_space=0时堆起点是不变的&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;情形二&#34;&gt;情形二：&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;vm_legacy_va_layout=0&lt;/li&gt;&#xA;&lt;li&gt;kernel.randomize_va_space=0&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;现在默认内存布局，不随机化&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://img.blog.csdn.net/20150308231829505&#34; alt=&#34;&#34;&gt;&#xA;可以看出:&#xA;代码段：0x400000&amp;ndash;&amp;gt;&#xA;数据段&#xA;堆：向下增长 &amp;lt;&amp;ndash;7ffff7fff000&#xA;栈：7ffffffde000&amp;lt;&amp;ndash;7ffffffff000&#xA;系统调用：ffffffffff600000-ffffffffff601000&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;情形三&#34;&gt;情形三：&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;vm_legacy_va_layout=0&lt;/li&gt;&#xA;&lt;li&gt;kernel.randomize_va_space=2 //ubuntu 14.04默认值&#xA;使用现在默认布局，随机化&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://img.blog.csdn.net/20150308232612405&#34; alt=&#34;&#34;&gt;&#xA;&lt;img src=&#34;http://img.blog.csdn.net/20150308232738454&#34; alt=&#34;&#34;&gt;&#xA;对比两次启动的cat程序，其内存布局堆的起点是变化的，这从一定程度上防止了缓冲区溢出攻击。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;情形四&#34;&gt;情形四：&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;vm_legacy_va_layout=1&lt;/li&gt;&#xA;&lt;li&gt;kernel.randomize_va_space=2 //ubuntu 14.04默认值&#xA;与情形三类似，不再赘述&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;vdso&#34;&gt;vDSO&lt;/h3&gt;&#xA;&lt;p&gt;在前面谈了两个不同参数下的进程运行时内存空间宏观的分布。也许你会注意到这样一个细节，在每个进程的stack以上的地址中，有一段动态变化的映射地址段，比如下面这个进程，映射到vdso。&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://img.blog.csdn.net/20150314205520905&#34; alt=&#34;cat&#34;&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;如果我们用ldd看相应的程序，会发现vdso在磁盘上没有对应的so文件。&#xA;不记得曾经在哪里看到大概这样一个问题：&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;getpid，gettimeofday是不是系统调用？&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;其实这个问题的答案就和vDSO有关，杂x86_64和i386上，getpid是系统调用，而gettimeofday不是。&lt;/p&gt;&#xA;&lt;p&gt;vDSO全称是virtual dynamic shared object，是一种内核将一些本身应该是系统调用的直接映射到用户空间，这样对于一些使用比较频繁的系统调用，直接在用户空间调用可以节省开销。如果想详细了解，可以参考&lt;a href=&#34;http://man7.org/linux/man-pages/man7/vdso.7.html&#34;&gt;这篇文档&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;下面我们用一段程序验证下：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;#include &amp;lt;stdio.h&amp;gt;&#xA;#include &amp;lt;sys/time.h&amp;gt;&#xA;#include &amp;lt;sys/syscall.h&amp;gt;&#xA;#include &amp;lt;unistd.h&amp;gt;&#xA;&#xA;int main(int argc, char **argv)&#xA;{&#xA;    struct timeval tv;&#xA;    int ret;&#xA;    if ((ret=gettimeofday(&amp;amp;tv, NULL))&amp;lt;0) {&#xA;        fprintf(stderr, &amp;#34;gettimeofday call failed\n&amp;#34;);&#xA;    }else{&#xA;        fprintf(stdout, &amp;#34;seconds:%ld\n&amp;#34;, (long int)tv.tv_sec);&#xA;    }&#xA;&#xA;    fprintf(stdout, &amp;#34;pid:%d\n&amp;#34;, (int)getpid());&#xA;    fprintf(stdout, &amp;#34;thread id:%d\n&amp;#34;, (int)syscall(SYS_gettid));&#xA;    return 0;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;编译为可执行文件后，我们可以用strace来验证：&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linux网络编程小结</title>
      <link>https://feilengcui008.github.io/post/linux%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%B0%8F%E7%BB%93/</link>
      <pubDate>Wed, 04 Mar 2015 22:37:15 +0800</pubDate>
      <guid>https://feilengcui008.github.io/post/linux%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%B0%8F%E7%BB%93/</guid>
      <description>&lt;p&gt;网络编程是一个很大也很有趣的话题，要写好一个高性能并且bug少的服务端或者客户端程序还是挺不容易的，而且往往涉及到进程线程管理、内存管理、协议栈、并发等许多相关的知识，而不仅仅只是会使用socket那么简单。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;网络编程模型&#34;&gt;网络编程模型&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;阻塞和非阻塞&#xA;阻塞和非阻塞通常是指文件描述符本身的属性。对于默认阻塞的socket来说，当socket读缓冲区中没有数据或者写缓冲区满时，都会造成read/recv或者write/send系统调用阻塞，而非阻塞socket在这种情况下会产生EWOULDBLOCK或者EAGAIN等错误并立即返回，不会等待socket变得可读或者可写。在Linux下我们可以通过accept4/fcntl系统调用设置socket为非阻塞。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;同步/异步&#xA;同步和异步可以分两层理解。一个是底层OS提供的IO基础设施的同步和异步，另一个是编程方式上的同步和异步。同步IO和异步IO更多地是怎么处理读写问题的一种手段。通常这也对应着两种高性能网络编程模式reactor和proactor。同步通常是事件发生时主动读写数据，直到显示地返回读写状态标志；而异步通常是我们交给操作系统帮我们读写，只需要注册读写完成的回调函数，提交读写的请求后，控制权就返回到进程。对于编程方式上的异步，典型的比如事件循环的回调、C++11的std::async/std::future等等，更多的是通过回调或者线程的方式组织异步的代码逻辑。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;IO复用&#xA;IO复用通常是用select/poll/epoll等来统一代理多个socket的事件的发生。select是一种比较通用的多路复用技术，poll是Linux平台下对select做的改进，而epoll是目前Linux下最常用的多路复用技术。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;常见网络库采用的模型只看epoll&#34;&gt;常见网络库采用的模型(只看epoll)：&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;nginx：master进程+多个worker进程，one eventloop per process&lt;/li&gt;&#xA;&lt;li&gt;memcached：主线程+多个worker线程，one eventloop per thread&lt;/li&gt;&#xA;&lt;li&gt;tornado：单线程，one eventloop per thread&lt;/li&gt;&#xA;&lt;li&gt;muduo：网络库，one eventloop per thread&lt;/li&gt;&#xA;&lt;li&gt;libevent、libev、boost.asio：网络库，跨平台eventloop封装&lt;/li&gt;&#xA;&lt;li&gt;&amp;hellip;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;排除掉传统的单线程、多进程、多线程等模型，最常用的高性能网络编程模型是one eventloop per thread与多线程的组合。另外，为了处理耗时的任务再加上线程池，为了更好的内存管理再加上对象池。&lt;/p&gt;&#xA;&lt;h3 id=&#34;应用层之外&#34;&gt;应用层之外&lt;/h3&gt;&#xA;&lt;p&gt;前面的模型多是针对应用层的C10K类问题的解决方案，在更高并发要求的环境下就需要在内核态下做手脚了，比如使用零拷贝等技术，直接越过内核协议栈，实现高速数据包的传递，相应的内核模块也早有实现。主要的技术点在于：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;数据平面与控制平面分离，减少不必要的系统调用&lt;/li&gt;&#xA;&lt;li&gt;用户态驱动uio/vfio等减少内存拷贝&lt;/li&gt;&#xA;&lt;li&gt;使用内存池减少内存分配&lt;/li&gt;&#xA;&lt;li&gt;通过CPU亲和性提高缓存命中率&lt;/li&gt;&#xA;&lt;li&gt;网卡多队列与poll模式充分利用多核&lt;/li&gt;&#xA;&lt;li&gt;batch syscall&lt;/li&gt;&#xA;&lt;li&gt;用户态协议栈&lt;/li&gt;&#xA;&lt;li&gt;&amp;hellip;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;相应的技术方案大多数是围绕这些点来做优化结合的。比如OSDI &amp;lsquo;14上的Arrakis、IX，再早的有pfring、netmap、intel DPDK、mTCP等等。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
