<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>基础技术 on 靠谱</title>
    <link>https://feilengcui008.github.io/categories/%E5%9F%BA%E7%A1%80%E6%8A%80%E6%9C%AF/</link>
    <description>Recent content in 基础技术 on 靠谱</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 19 Jul 2017 20:06:51 +0800</lastBuildDate>
    
	<atom:link href="https://feilengcui008.github.io/categories/%E5%9F%BA%E7%A1%80%E6%8A%80%E6%9C%AF/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Tcplayer介绍</title>
      <link>https://feilengcui008.github.io/post/tcplayer%E4%BB%8B%E7%BB%8D/</link>
      <pubDate>Wed, 19 Jul 2017 20:06:51 +0800</pubDate>
      
      <guid>https://feilengcui008.github.io/post/tcplayer%E4%BB%8B%E7%BB%8D/</guid>
      <description>Tcplayer是最近写的一个流量抓取、解析、放大、重放工具。
背景 通常真实流量相比手工构造的请求来说，更有利于测试。真实流量的回放大体上有以下三种方式：
 应用层  这种方式通常是在服务中耦合拷贝请求的代码。由于直接工作在应用层，截下来的流量是一个个完整的请求，所以其支持场景最多，实现也相对简单，但是需要耦合其他代码，也会给服务程序带来资源消耗。
 网络层  这种方式通常是抓取原始网络包，解析出IP报文，修改报文的目的IP和端口，伪造与测试机的TCP会话，回放到测试机。其优点是不需要处理传输层的TCP包排序，也不需要理解上层应用层的请求格式，但其缺点是配置相对复杂，且难以支持长连接。如果在长连接已经建立的情况下抓包回放，由于测试机并未经过任何SYN-SYN/ACK-ACK的请求建立过程，所以所有请求的TCP PUSH包都会被测试机RST丢掉。通常后端RPC服务都是长连接，所以这是一个比较大的问题。这里有一个工作在这一层的开源工具tcpcopy
 传输层  为了解决外部代码依赖以及长连接的问题，tcplayer基于TCP传输层，按照应用层请求格式解析出请求并重放。这种方式可以抓取到已经建立的长连接的请求，并且服务不需要耦合其他代码，但同时引入了解析和匹配应用层协议的复杂性。
Tcplayer Tcplayer主要包含以下几个步骤:
 libpcap抓取实时流量 tcp包重排序 对于每一个tcp会话(flow)，解析tcp包，尽量匹配应用层协议 放大应用层请求，并与测试服务建立连接回放  目前支持的协议:
 HTTP 1.x 短连接 Thrift strict mode binary protocol GRPC部分支持  这里由于GRPC基于HTTP2，而HTTP2的基本单元Frame没有固定的协议字段，所以无法匹配，导致无法解析到正确的HTTP2请求&amp;hellip;   要给tcplayer添加其他自己定义的应用层协议很容易，可以参考代码factory/thrift</description>
    </item>
    
    <item>
      <title>gRPC-Go客户端源码分析</title>
      <link>https://feilengcui008.github.io/post/grpc-go%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</link>
      <pubDate>Mon, 24 Apr 2017 15:33:49 +0800</pubDate>
      
      <guid>https://feilengcui008.github.io/post/grpc-go%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</guid>
      <description>基本设计 gRPC-Go客户端的逻辑相对比较简单，从前面服务端的逻辑我们知道，客户端会通过http2复用tcp连接，每一次请求的调用基本上就是在已经建立好的tcp连接(并用ClientTransport抽象)上发送http请求，通过帧和流与服务端交互数据。
另外，一个服务对应的具体地址可能有多个，grpc在这里抽象了负载均衡的接口和部分实现。grpc提供两种负载均衡方式，一种是客户端内部自带的策略实现(目前只实现了轮询方式)，另一种方式是外部的load balancer。
 内部自带的策略实现: 这种方式主要针对一些简单的负载均衡策略比如轮询。轮询的实现逻辑是建立连接时通过定义的服务地址解析接口Resolver得到服务的地址列表，并单独用goroutine负责更新保持可用的连接，Watcher定义了具体更新实现的接口(比如多长时间解析更新一次)，最终在请求调用时会从可用连接列表中轮询选择其中一个连接发送请求。所以，grpc的负载均衡策略是请求级别的而不是连接级别的。 外部load balancer：这种方式主要针对 较复杂的负载均衡策略。grpclb实现了grpc这边的逻辑，并用protobuf定义了与load balancer交互的接口。gRPC-Go客户端建立连接时，会先与load balancer建立连接，并使用和轮询方式类似的Resolver、Watcher接口来更新load balancer的可用连接列表，不同的是每次load balancer连接变化时，会像load balancer地址发送rpc请求得到服务的地址列表。  客户端主要流程 客户端的逻辑主要可分为下面两部分:
 建立连接 请求调用、发送与响应  1. 建立连接  典型的步骤
func main() { // 建立连接 conn, err := grpc.Dial(address, grpc.WithInsecure()) c := pb.NewGreeterClient(conn) // 请求调用 r, err := c.SayHello(context.Background(), &amp;amp;pb.HelloRequest{Name: name}) // 处理返回r // 对于单次请求，grpc直接负责返回响应数据 // 对于流式请求，grpc会返回一个流的封装，由开发者负责流中数据的读写 }  建立tcp(http2)连接
func Dial(target string, opts ...DialOption) (*ClientConn, error) { return DialContext(context.Background(), target, opts...) } func DialContext(ctx context.</description>
    </item>
    
    <item>
      <title>gRPC-Go服务端源码分析</title>
      <link>https://feilengcui008.github.io/post/grpc-go%E6%9C%8D%E5%8A%A1%E7%AB%AF%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</link>
      <pubDate>Sun, 23 Apr 2017 15:47:59 +0800</pubDate>
      
      <guid>https://feilengcui008.github.io/post/grpc-go%E6%9C%8D%E5%8A%A1%E7%AB%AF%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</guid>
      <description>基本设计  服务抽象
 一个Server可包含多个Service，每个Service包含多个业务逻辑方法，应用开发者需要:  不使用protobuf  规定Service需要实现的接口 实现此Service对应的ServiceDesc，ServiceDesc描述了服务名、处理此服务的接口类型、单次调用的方法数组、流式方法数组、其他元数据。 实现Service接口具体业务逻辑的结构体 实例化Server，并讲ServiceDesc和Service具体实现注册到Server 监听并启动Server服务  使用protobuf  实现protobuf grpc插件生成的Service接口 实例化Server，并注册Service接口的具体实现 监听并启动Server  可见，protobuf的gRPC-Go插件帮助我们生成了Service的接口和ServiceDesc。   底层传输协议
 gRPC-Go使用http2作为应用层的传输协议，http2会复用底层tcp连接，以流和数据帧的形式处理上层协议，gRPC-Go使用http2的主要逻辑有下面几点，关于http2详细的细节可参考http2的规范  http2帧分为几大类，gRPC-Go使用中比较重要的是HEADERS和DATA帧类型。  HEADERS帧在打开一个新的流时使用，通常是客户端的一个http请求，gRPC-Go通过底层的go的http2实现帧的读写，并解析出客户端的请求头(大多是grpc内部自己定义的)，读取请求体的数据，grpc规定请求体的数据由两部分构成(5 byte + len(msg)), 其中第1字节表明是否压缩，第2-5个字节消息体的长度(最大2^32即4G)，msg为客户端请求序列化后的原始数据。 数据帧从属于某个stream，按照stream id查找，并写入对应的stream中。  Server端接收到客户端建立的连接后，使用一个goroutine专门处理此客户端的连接(即一个tcp连接或者说一个http2连接)，所以同一个grpc客户端连接上服务端后，后续的请求都是通过同一个tcp连接。 客户端和服务端的连接在应用层由Transport抽象(类似通常多路复用实现中的封装的channel)，在客户端是ClientTransport，在服务端是ServerTransport。Server端接收到一个客户端的http2请求后即打开一个新的流，ClientTransport和ServerTransport之间使用这个新打开的流以http2帧的形式交换数据。 客户端的每个http2请求会打开一个新的流。流可以从两边关闭，对于单次请求来说，客户端会主动关闭流，对于流式请求客户端不会主动关闭(即使使用了CloseSend也只是发送了数据发送结束的标识，还是由服务端关闭)。 gRPC-Go中的单次方法和流式方法  无论是单次方法还是流式方法，服务端在调用完用户的处理逻辑函数返回后，都会关闭流(这也是为什么ServerStream不需要实现CloseSend的原因)。区别只是对于服务端的流式方法来说，可循环多次读取这个流中的帧数据并处理，以此&amp;rdquo;复用&amp;rdquo;这个流。 客户端如果是流式方法，需要显示调用CloseSend，表示数据发送的结束     服务端主要流程 由于比较多，所以分以下几个部分解读主要逻辑:
 实例化Server 注册Service 监听并接收连接请求 连接与请求处理 连接的处理细节(http2连接的建立) 新请求的处理细节(新流的打开和帧数据的处理)   实例化Server
// 工厂方法 func NewServer(opt ...ServerOption) *Server { var opts options // 默认最大消息长度: 4M opts.</description>
    </item>
    
    <item>
      <title>Runc容器生命周期</title>
      <link>https://feilengcui008.github.io/post/runc%E5%AE%B9%E5%99%A8%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/</link>
      <pubDate>Wed, 30 Nov 2016 17:48:20 +0800</pubDate>
      
      <guid>https://feilengcui008.github.io/post/runc%E5%AE%B9%E5%99%A8%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/</guid>
      <description> 容器的生命周期涉及到内部的程序实现和面向用户的命令行界面，runc内部容器状态转换操作、runc命令的参数定义的操作、docker client定义的容器操作是不同的，比如对于docker client的create来说， 语义和runc就完全不同，这一篇文章分析runc的容器生命周期的抽象、内部实现以及状态转换图。理解了runc的容器状态转换再对比理解docker client提供的容器操作命令的语义会更容易些。
容器生命周期相关接口  最基本的required的接口  Start: 初始化容器环境并启动一个init进程，或者加入已有容器的namespace并启动一个setns进程；执行postStart hook; 阻塞在init管道的写端，用户发信号替换执行真正的命令 Exec: 读init管道，通知init进程或者setns进程继续往下执行 Run: Start + Exec的组合 Signal: 向容器内init进程发信号 Destroy: 杀掉cgroups中的进程，删除cgroups对应的path，运行postStop的hook 其他  Set: 更新容器的配置信息，比如修改cgroups resize等 Config: 获取容器的配置信息 State: 获取容器的状态信息 Status: 获取容器的当前运行状态: created、running、pausing、paused、stopped Processes: 返回容器内所有进程的列表 Stats: 容器内的cgroups统计信息  对于linux容器定义并实现了特有的功能接口  Pause: free容器中的所有进程 Resume: thaw容器内的所有进程 Checkpoint: criu checkpoint Restore: criu restore    接口在内部的实现  对于Start/Run/Exec的接口是作为不同os环境下的标准接口对开发者暴露，接口在内部的实现有很多重复的部分可以统一，因此内部的接口实际上更简洁，这里以linux容器为例说明  对于Start/Run/Exec在内部实现实际上只用到下面两个函数，通过传入flag(容器是否处于stopped状态)区分是创建容器的init进程还是创建进程的init进程  start: 创建init进程，如果status == stopped，则创建并执行newInitProcess，否则创建并执行newSetnsProcess，等待用户发送执行信号(等在管道写端上)，用用户的命令替换掉 exec: 读管道，发送执行信号  Start直接使用start Run实际先使用start(doInit = true)，然后exec Exec实际先使用start(doInit = false), 然后exec   对用户暴露的命令行参数与容器接口的对应关系，以linux容器为例  create -&amp;gt; Start(doInit = true) start -&amp;gt; Exec run -&amp;gt; Run(doInit = true) exec -&amp;gt; Run(doInit = false) kill -&amp;gt; Signal delete -&amp;gt; Signal and Destroy update -&amp;gt; Set state -&amp;gt; State events -&amp;gt; Stats ps -&amp;gt; Processes list linux specific  pause -&amp;gt; Pause resume -&amp;gt; Resume checkpoint -&amp;gt; Checkpoint restore -&amp;gt; Restore   runc命令行的动作序列对容器状态机的影响  对于一个容器的生命周期来说，稳定状态有4个: stopped、created、running、paused 注意下面状态转换图中的动作是runc命令行参数动作，不是容器的接口动作，这里没考虑checkpoint相关的restore状态     </description>
    </item>
    
    <item>
      <title>Docker简介</title>
      <link>https://feilengcui008.github.io/post/docker%E7%AE%80%E4%BB%8B/</link>
      <pubDate>Sat, 08 Oct 2016 17:44:02 +0800</pubDate>
      
      <guid>https://feilengcui008.github.io/post/docker%E7%AE%80%E4%BB%8B/</guid>
      <description>本文主要介绍Docker的一些基本概念、Docker的源码分析、Docker相关的一些issue、Docker周边生态等等。
基本概念 Basics docker大体包括三大部分，runtime(container)、image(graphdriver)、registry，runtime提供环境的隔离与资源的隔离和限制，image提供layer、image、rootfs的管理、registry负责镜像存储与分发。当然，还有其他一些比如data volume, network等等，总体来说还是分为计算、存储与网络。
computing  接口规范 命名空间隔离、资源隔离与限制的实现 造坑与入坑  network  接口规范与实现  bridge  veth pair for two namespace communication bridge and veth pair for multi-namespace communication do not support multi-host  overlay  docker overlay network: with swarm mode or with kv etcd/zookeeper/consul -&amp;gt; vxlan coreos flannel -&amp;gt; 多种backend，udp/vxlan&amp;hellip; ovs weave -&amp;gt; udp and vxlan，与flannel udp不同的是会将多container的packet一块打包 calico  pure layer 3  一篇对比  null  与世隔绝  host  共享主机net namespace    storage  graphdriver(layers,image and rootfs)  graph:独立于各个driver，记录image的各层依赖关系(DAG)，注意是image不包括运行中的container的layer，当container commit生成image后，会将新layer的依赖关系写入 device mapper  snapshot基于block，allocation-on-demand 默认基于空洞文件(data and metadata)挂载到回环设备  aufs  diff:实际存储各个layer的变更数据 layers:每个layer依赖的layers，包括正在运行中的container mnt:container的实际挂载根目录  overlayfs vfs btrfs &amp;hellip;  volume  driver接口  local driver flocker: container和volume管理与迁移 rancher的convoy:多重volume存储后端的支持device mapper, NFS, EBS&amp;hellip;,提供快照、备份、恢复等功能  数据卷容器  registry:与docker registry交互  支持basic/token等认证方式 token可以基于basic/oauth等方式从第三方auth server获取bearer token tls通信的支持  libkv  支持consul/etcd/zookeeper  分布式存储的支持  security  docker  libseccomp限制系统调用(内部使用bpf) linux capabilities限制root用户权限范围scope user namespace用户和组的映射 selinux apparmor &amp;hellip;  image and registry  Other Stuffs  迁移</description>
    </item>
    
  </channel>
</rss>